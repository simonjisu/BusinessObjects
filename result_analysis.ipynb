{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "proj_path = Path('.').resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sql_gen_hint_top1_1_desc | error samples 0 | empty hints 0: 401it [00:01, 256.68it/s]\n",
      "sql_gen_hint_top1_2_desc | error samples 1 | empty hints 0: 401it [00:01, 265.86it/s]\n",
      "sql_gen_hint_top1_3+_desc | error samples 1 | empty hints 0: 401it [00:01, 264.61it/s]\n",
      "sql_gen_hint_top3_1_desc | error samples 1 | empty hints 0: 401it [00:01, 260.51it/s]\n",
      "sql_gen_hint_top3_2_desc | error samples 3 | empty hints 0: 401it [00:01, 259.03it/s]\n",
      "sql_gen_hint_top3_3+_desc | error samples 3 | empty hints 0: 401it [00:01, 261.92it/s]\n",
      "sql_gen_hint_top1_1_descvt | error samples 3 | empty hints 0: 401it [00:01, 256.23it/s]\n",
      "sql_gen_hint_top1_2_descvt | error samples 4 | empty hints 0: 401it [00:01, 257.04it/s]\n",
      "sql_gen_hint_top1_3+_descvt | error samples 5 | empty hints 0: 401it [00:01, 240.18it/s]\n",
      "sql_gen_hint_top3_1_descvt | error samples 6 | empty hints 0: 401it [00:01, 266.58it/s]\n",
      "sql_gen_hint_top3_2_descvt | error samples 6 | empty hints 0: 401it [00:01, 261.93it/s]\n",
      "sql_gen_hint_top3_3+_descvt | error samples 6 | empty hints 0: 401it [00:01, 251.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing SQL errors: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "from src.database import SqliteDatabase\n",
    "from src.eval import result_eq, check_if_exists_orderby\n",
    "from src.eval_complexity import eval_all\n",
    "from src.process_sql import get_schema, Schema\n",
    "from src.parsing_sql import (\n",
    "    extract_selection, \n",
    "    extract_condition, \n",
    "    extract_aggregation, \n",
    "    extract_nested_setoperation, \n",
    "    extract_others,\n",
    "    extract_aliases,\n",
    ")\n",
    "\n",
    "def error_check(proj_path, all_tasks):\n",
    "    error_infos = {\n",
    "        'pred_exec': [],\n",
    "        'result': [],\n",
    "        'parsing_sql': [],\n",
    "        'error_samples': set(),\n",
    "        'empty_hint': set()\n",
    "    }\n",
    "\n",
    "    # filter parsing errors\n",
    "    for task in all_tasks:\n",
    "        with open(proj_path / 'experiments' / f'{task}.jsonl', 'r') as f:\n",
    "            iterator = tqdm(f, desc=task)\n",
    "            for line in iterator:\n",
    "                x = json.loads(line)\n",
    "                if x['hint'] == '':\n",
    "                    error_infos['empty_hint'].add(x['sample_id'])\n",
    "                has_error = False\n",
    "                schema = get_schema(str(proj_path / 'data' / 'spider' / 'database' / x['db_id'] / f'{x[\"db_id\"]}.sqlite'))\n",
    "                schema = Schema(schema)\n",
    "                \n",
    "                parsed_result = {}\n",
    "                for s in ['gold', 'pred']:\n",
    "                    try:\n",
    "                        sql = x[f'{s}_sql']\n",
    "                        statement = sqlparse.parse(sql.strip())[0]\n",
    "                        aliases = extract_aliases(statement)\n",
    "                        selection = extract_selection(statement, aliases, schema)\n",
    "                        condition = extract_condition(statement)\n",
    "                        aggregation = extract_aggregation(statement, aliases, schema)\n",
    "                        nested = extract_nested_setoperation(statement)\n",
    "                        others = extract_others(statement, aliases, schema)\n",
    "                        \n",
    "                        parsed_result[s + '_selection'] = selection\n",
    "                        parsed_result[s + '_condition'] = condition\n",
    "                        parsed_result[s + '_aggregation'] = aggregation\n",
    "                        parsed_result[s + '_nested'] = nested\n",
    "                        parsed_result[s + '_others'] = {\n",
    "                            'distinct': others['distinct'], \n",
    "                            'order by': others['order by'], \n",
    "                            'limit': others['limit']\n",
    "                        }\n",
    "                    except Exception as e:\n",
    "                        has_error = True\n",
    "                        error_infos['parsing_sql'].append((x['sample_id'], s, str(e)))\n",
    "                        error_infos['error_samples'].add(x['sample_id'])\n",
    "                        break\n",
    "                \n",
    "                if has_error:\n",
    "                    continue\n",
    "\n",
    "                iterator.update()\n",
    "                iterator.set_description_str(f'{task} | error samples {len(error_infos[\"error_samples\"])} | empty hints {len(error_infos[\"empty_hint\"])}')\n",
    "\n",
    "    print(f'Parsing SQL errors: {len(error_infos[\"parsing_sql\"])}')\n",
    "\n",
    "    return error_infos\n",
    "\n",
    "# process single task\n",
    "def process_task(task, error_infos):\n",
    "    task_results = {\n",
    "        'sample_id': [],\n",
    "        'score': [],\n",
    "        's_sel': [], 's_cond': [], 's_agg': [], 's_nest': [], 's_oth': [],\n",
    "    }\n",
    "    with open(proj_path / 'experiments' / f'{task}.jsonl', 'r') as f:\n",
    "        iterator = tqdm(f, desc=task.lstrip('sql_gen_hint_top'))\n",
    "        for line in iterator:\n",
    "            x = json.loads(line)\n",
    "            if x['sample_id'] in error_infos['error_samples']:\n",
    "                continue\n",
    "\n",
    "            task_results['sample_id'].append(x['sample_id'])\n",
    "            # parsing sql\n",
    "            schema = get_schema(str(proj_path / 'data' / 'spider' / 'database' / x['db_id'] / f'{x[\"db_id\"]}.sqlite'))\n",
    "            schema = Schema(schema)\n",
    "            \n",
    "            parsed_result = {}\n",
    "            for s in ['gold', 'pred']:\n",
    "                sql = x[f'{s}_sql']\n",
    "                statement = sqlparse.parse(sql.strip())[0]\n",
    "                aliases = extract_aliases(statement)\n",
    "                selection = extract_selection(statement, aliases, schema)\n",
    "                condition = extract_condition(statement)\n",
    "                aggregation = extract_aggregation(statement, aliases, schema)\n",
    "                nested = extract_nested_setoperation(statement)\n",
    "                others = extract_others(statement, aliases, schema)\n",
    "                \n",
    "                parsed_result[s + '_selection'] = selection\n",
    "                parsed_result[s + '_condition'] = condition\n",
    "                parsed_result[s + '_aggregation'] = aggregation\n",
    "                parsed_result[s + '_nested'] = nested\n",
    "                parsed_result[s + '_others'] = {\n",
    "                    'distinct': others['distinct'], \n",
    "                    'order by': others['order by'], \n",
    "                    'limit': others['limit']\n",
    "                }\n",
    "\n",
    "            # partial & complexity eval\n",
    "            eval_res = eval_all(parsed_result, k=6)\n",
    "            task_results['s_sel'].append(eval_res['score']['selection'])\n",
    "            task_results['s_cond'].append(eval_res['score']['condition'])\n",
    "            task_results['s_agg'].append(eval_res['score']['aggregation'])\n",
    "            task_results['s_nest'].append(eval_res['score']['nested'])\n",
    "            task_results['s_oth'].append(eval_res['score']['others'])\n",
    "            # Execution\n",
    "            database = SqliteDatabase(\n",
    "                str(proj_path / 'data' / 'spider' / 'database' / x['db_id'] / f'{x[\"db_id\"]}.sqlite')\n",
    "            )\n",
    "            error_info = ''\n",
    "            try:\n",
    "                pred_result = database.execute(x['pred_sql'], rt_pandas=False)\n",
    "            except Exception as e:\n",
    "                pred_result = []\n",
    "                error_info = 'Predction Execution Error:' + str(e)\n",
    "                score = 0\n",
    "\n",
    "            try:\n",
    "                gold_result = database.execute(x['gold_sql'], rt_pandas=False)\n",
    "            except Exception as e:\n",
    "                error_info = 'Gold Execution Error:' + str(e)\n",
    "\n",
    "            if 'Gold Execution Error' in error_info:\n",
    "                continue\n",
    "            elif 'Predction Execution Error' in error_info:\n",
    "                task_results['score'].append(score)\n",
    "                continue\n",
    "            else:\n",
    "                exists_orderby = check_if_exists_orderby(x['gold_sql'])\n",
    "                score = int(result_eq(pred_result, gold_result, order_matters=exists_orderby))\n",
    "                task_results['score'].append(score)\n",
    "\n",
    "    return task_results\n",
    "\n",
    "def process_all_exps(proj_path, all_tasks):\n",
    "    error_infos = error_check(proj_path, all_tasks)\n",
    "    for task in all_tasks:\n",
    "        task_results = process_task(task, error_infos)\n",
    "        pd.DataFrame(task_results).to_csv(proj_path / 'experiments' / 'bo_evals' / f'{task}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks = []\n",
    "typ = '_c'  # '_c'\n",
    "iterator = ['low', 'mid', 'high'] if typ == '_c' else ['1', '2', '3+']\n",
    "for typ2 in ['desc', 'descvt']:        \n",
    "    for n_retrieval in [1, 3]:\n",
    "        for level in iterator:\n",
    "            all_tasks.append(f'sql_gen_hint_top{n_retrieval}_{level}_{typ2}')\n",
    "\n",
    "# process_all_exps(proj_path, all_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sql_gen_hint_top1_low_desc\n",
      "sql_gen_hint_top1_mid_desc\n",
      "sql_gen_hint_top1_high_desc\n",
      "sql_gen_hint_top3_low_desc\n",
      "sql_gen_hint_top3_mid_desc\n",
      "sql_gen_hint_top3_high_desc\n",
      "sql_gen_hint_top1_low_descvt\n",
      "sql_gen_hint_top1_mid_descvt\n",
      "sql_gen_hint_top1_high_descvt\n",
      "sql_gen_hint_top3_low_descvt\n",
      "sql_gen_hint_top3_mid_descvt\n",
      "sql_gen_hint_top3_high_descvt\n"
     ]
    }
   ],
   "source": [
    "# real eval\n",
    "typ = '_c' # '_t', '_c'\n",
    "iterator = ['low', 'mid', 'high'] if typ == '_c' else ['1', '2', '3+']\n",
    "all_tasks = []\n",
    "for typ2 in ['desc', 'descvt']:        \n",
    "    for n_retrieval in [1, 3]:\n",
    "        for level in iterator:\n",
    "            all_tasks.append(f'sql_gen_hint_top{n_retrieval}_{level}_{typ2}')\n",
    "\n",
    "col = 'cate_gold_c' if typ == '_c' else 'cate_len_tbls'\n",
    "results = {\n",
    "    'bo_topk': [], 'bo_level': [], 'bo_desc_vt': [], \n",
    "    'count': [], 'ex_acc': [], 'pm_sel': [], 'pm_cond': [], 'pm_agg': [], 'pm_nest': [], 'pm_oth': [],\n",
    "}\n",
    "for l in iterator:\n",
    "    results[f'ex_acc_{l}'] = []\n",
    "    results[f'count_{l}'] = []\n",
    "    for c in ['sel', 'cond', 'agg', 'nest', 'oth']:\n",
    "        results[f'pm_{c}_{l}'] = []\n",
    "\n",
    "df_test = pd.read_csv(proj_path / 'data' / 'spilt_in_domain' / f'bo{typ}_eval.csv')\n",
    "baseline = df_test.groupby(col)[['score', 's_sel', 's_cond', 's_agg', 's_nest', 's_oth']].mean() * 100\n",
    "\n",
    "for task in all_tasks:\n",
    "    print(task)\n",
    "    # results['task'].append(task)\n",
    "    results['bo_topk'].append(int(task.lstrip('sql_gen_hint_top').split('_')[0]))\n",
    "    results['bo_level'].append(task.lstrip('sql_gen_hint_top').split('_')[1])\n",
    "    results['bo_desc_vt'].append(task.lstrip('sql_gen_hint_top').split('_')[2])\n",
    "\n",
    "    df = pd.read_csv(proj_path / 'experiments' / 'bo_evals' / f'{task}.csv')\n",
    "    df = pd.merge(df, df_test.loc[:, ['sample_id', 'cate_len_tbls', 'cate_gold_c']], on='sample_id', how='left')\n",
    "    results['count'].append(df.shape[0])\n",
    "    results['ex_acc'].append(df['score'].mean()*100)\n",
    "    results['pm_sel'].append((df['s_sel'].mean() - df_test['s_sel'].mean())*100)\n",
    "    results['pm_cond'].append((df['s_cond'].mean() - df_test['s_cond'].mean())*100)\n",
    "    results['pm_agg'].append((df['s_agg'].mean() - df_test['s_agg'].mean())*100)\n",
    "    results['pm_nest'].append((df['s_nest'].mean() - df_test['s_nest'].mean())*100)\n",
    "    results['pm_oth'].append((df['s_oth'].mean() - df_test['s_oth'].mean())*100)\n",
    "\n",
    "    g_score = df.groupby(col)[['score', 's_sel', 's_cond', 's_agg', 's_nest', 's_oth']].mean() * 100 - baseline\n",
    "    for l in iterator:\n",
    "        results[f'ex_acc_{l}'].append(g_score.loc[l, 'score'])\n",
    "        results[f'count_{l}'].append(df[df[col] == l].shape[0])\n",
    "        for c in ['s_sel', 's_cond', 's_agg', 's_nest', 's_oth']:\n",
    "            results[f'pm_{c[2:]}_{l}'].append(g_score.loc[l, c])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "# df.set_index(['bo_topk','bo_desc_vt', 'bo_level'], inplace=True)\n",
    "desc_vt = {'desc': 'BA', 'descvt': 'BA + VT'}\n",
    "df['bo_desc_vt'] = df['bo_desc_vt'].map(desc_vt)\n",
    "df['bo_level'] = df['bo_level'].str.capitalize()\n",
    "\n",
    "idx_cols = ['bo_topk','bo_desc_vt', 'bo_level']\n",
    "count_cols = ['count', 'count_low', 'count_mid', 'count_high'] if typ == '_c' else ['count', 'count_1', 'count_2', 'count_3+']\n",
    "r1 = ['ex_acc_low', 'ex_acc_mid', 'ex_acc_high', 'ex_acc'] if typ == '_c' else ['ex_acc_1', 'ex_acc_2', 'ex_acc_3+', 'ex_acc']\n",
    "r2 = ['pm_sel', 'pm_cond', 'pm_agg', 'pm_nest', 'pm_oth'] if typ == '_c' else ['pm_sel', 'pm_cond', 'pm_agg', 'pm_nest', 'pm_oth']\n",
    "r3 = ['pm_sel_low', 'pm_sel_mid', 'pm_sel_high'] if typ == '_c' else ['pm_sel_1', 'pm_sel_2', 'pm_sel_3+']\n",
    "r4 = ['pm_cond_low', 'pm_cond_mid', 'pm_cond_high'] if typ == '_c' else ['pm_cond_1', 'pm_cond_2', 'pm_cond_3+']\n",
    "r5 = ['pm_agg_low', 'pm_agg_mid', 'pm_agg_high'] if typ == '_c' else ['pm_agg_1', 'pm_agg_2', 'pm_agg_3+']\n",
    "r6 = ['pm_nest_low', 'pm_nest_mid', 'pm_nest_high'] if typ == '_c' else ['pm_nest_1', 'pm_nest_2', 'pm_nest_3+']\n",
    "r7 = ['pm_oth_low', 'pm_oth_mid', 'pm_oth_high'] if typ == '_c' else ['pm_oth_1', 'pm_oth_2', 'pm_oth_3+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(proj_path / 'experiments' / 'reports' / f'bo_eval{typ}.xlsx') as writer:\n",
    "    df.loc[:, idx_cols+count_cols].to_excel(writer, sheet_name='count')\n",
    "\n",
    "    df1 = df.loc[:, idx_cols+r1].round(2)\n",
    "    rename_cols = {\n",
    "        'bo_topk': 'Top-K', 'bo_desc_vt': 'Prompt Type',\n",
    "        'bo_level': 'Complexity Lv.' if typ == '_c' else 'Table Num.',\n",
    "        'ex_acc': 'Overall',\n",
    "    }\n",
    "    for l in iterator:\n",
    "        rename_cols[f'ex_acc_{l}'] = f'{l.capitalize()}'\n",
    "    df1.rename(columns=rename_cols, inplace=True)\n",
    "    df1.to_excel(writer, sheet_name='ex_acc', index=False)\n",
    "\n",
    "    df2 = df.loc[:, idx_cols+r2].round(2)\n",
    "    \n",
    "    df2.rename(columns={\n",
    "        'bo_topk': 'Top-K', 'bo_desc_vt': 'Prompt Type',\n",
    "        'bo_level': 'Complexity Lv.' if typ == '_c' else 'Table Num.',\n",
    "        'pm_sel': 'Selection',\n",
    "        'pm_cond': 'Condition',\n",
    "        'pm_agg': 'Aggregation',\n",
    "        'pm_nest': 'Nested',\n",
    "        'pm_oth': 'Others',\n",
    "    }, inplace=True)\n",
    "    df2.to_excel(writer, sheet_name='pm', index=False)\n",
    "\n",
    "    df3 = df.loc[:, idx_cols+r3+r4+r5+r6+r7].round(2)\n",
    "    rename_cols = {\n",
    "        'bo_topk': 'Top-K', 'bo_desc_vt': 'Prompt Type',\n",
    "        'bo_level': 'Complexity Lv.' if typ == '_c' else 'Table Num.',\n",
    "    }\n",
    "    for l in iterator:\n",
    "        for c in ['sel', 'cond', 'agg', 'nest', 'oth']:\n",
    "            rename_cols[f'pm_{c}_{l}'] = f'{c.capitalize()} {l.capitalize()}'\n",
    "    df3.rename(columns=rename_cols, inplace=True)\n",
    "    df3.to_excel(writer, sheet_name='pm_detail', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
