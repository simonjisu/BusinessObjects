{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "from src.spider_sparc_preprocess import process_all_tables\n",
    "from pathlib import Path\n",
    "from src.db_utils import get_schema_str\n",
    "from src.database import SqliteDatabase\n",
    "from tqdm import tqdm\n",
    "from src.spider_sparc_preprocess import DatabaseModel, SpiderSample, SpiderSample, QuestionSQL, BusinessObject\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "proj_path = Path('.').resolve()\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "def predict_sql(\n",
    "        samples: list[SpiderSample], \n",
    "        spider_tables: dict[str, DatabaseModel], \n",
    "        chain: RunnableSequence, \n",
    "        task: str, \n",
    "        k: int = 500, \n",
    "        file_name: str = 'full_sql_output',\n",
    "        vectorstore: Optional[FAISS] = None,\n",
    "        n_retrieval: int = 1,\n",
    "        score_threshold: float = 0.65,\n",
    "    ) -> list[dict]:\n",
    "    # check index exists, then start from the last index\n",
    "    check_files = list((proj_path / 'experiments' / task).glob(f'{file_name}_*.jsonl'))\n",
    "    check_ids = [int(x.stem.split('_')[-1]) for x in check_files]\n",
    "    if len(check_ids) > 0:\n",
    "        i = max(check_ids) * k\n",
    "        samples = samples[i:]\n",
    "    else:\n",
    "        i = 0\n",
    "    \n",
    "    all_outputs = list()\n",
    "    for i, data in tqdm(enumerate(samples, i), total=len(samples)):\n",
    "        db_schema = get_schema_str(\n",
    "            schema=spider_tables[data.db_id].db_schema, \n",
    "            foreign_keys=spider_tables[data.db_id].foreign_keys,\n",
    "            col_explanation=spider_tables[data.db_id].col_explanation\n",
    "        )\n",
    "        final_output = {}\n",
    "        if task == 'sql_gen_zero_shot':\n",
    "            input_data = {'schema': db_schema, 'input_query': data.final.question}\n",
    "        elif 'bo_desc_gen' in task:\n",
    "            input_data = {'schema': db_schema, 'virtual_table': data.bo.virtual_table}\n",
    "        elif 'sql_gen_hint' in task:\n",
    "            # name hint: 'sql_gen_hint_top{n_retrieval}_[low/mid/high or 1/2/3+]_[desc/descvt]'\n",
    "            assert vectorstore is not None, 'vectorstore is required for hint generation task.'\n",
    "            assert len(task.split('_')) == 6, f'Invalid task - {task} name hint: sql_gen_hint_top{n_retrieval}_[low/mid/high or 1/2/3+]_[desc/descvt]\"'\n",
    "            hint_type = task.split('_')[-1]\n",
    "            filter_type = task.split('_')[-2]\n",
    "            assert filter_type in ['low', 'mid', 'high', '1', '2', '3+'], f'Invalid filter type: {filter_type}'\n",
    "            assert hint_type in ['desc', 'descvt'], f'Invalid hint type: {hint_type}'\n",
    "\n",
    "            retriever = vectorstore.as_retriever(\n",
    "                search_kwargs={'k': n_retrieval, 'score_threshold': score_threshold, 'filter': {'level': filter_type, 'db_id': data.db_id}}\n",
    "            )\n",
    "            docs = retriever.invoke(data.final.question)\n",
    "            hint = ''\n",
    "            if len(docs) != 0:\n",
    "                if hint_type == 'desc':\n",
    "                    hint += 'Descriptions:\\n'\n",
    "                    hint += json.dumps({j: doc.page_content for j, doc in enumerate(docs)}, indent=4)\n",
    "                elif hint_type == 'descvt':\n",
    "                    hint += 'Descriptions and Virtual Tables:\\n'\n",
    "                    hint += json.dumps({j: {'description': doc.page_content, 'virtual_table': doc.metadata['virtual_table']} for j, doc in enumerate(docs)}, indent=4)\n",
    "            hint += '\\n'\n",
    "            input_data = {'schema': db_schema, 'input_query': data.final.question, 'hint': hint}\n",
    "        else:\n",
    "            raise ValueError(f'Invalid task: {task}')\n",
    "        \n",
    "        output = chain.invoke(input=input_data)\n",
    "\n",
    "        final_output['sample_id'] = data.sample_id\n",
    "        final_output['db_id'] = data.db_id\n",
    "        final_output['question'] = data.final.question\n",
    "        final_output['rationale'] = output.rationale\n",
    "        final_output['gold_sql'] = data.final.sql\n",
    "        final_output['source_tables'] = data.final.source_tables\n",
    "\n",
    "        if task == 'sql_gen_zero_shot':\n",
    "            final_output['pred_sql'] = output.output\n",
    "        elif 'bo_desc_gen' in task:\n",
    "            final_output['description'] = output.output\n",
    "            final_output['virtual_table'] = data.bo.virtual_table\n",
    "        elif 'sql_gen_hint' in task:\n",
    "            final_output['pred_sql'] = output.output\n",
    "            final_output['hint'] = input_data['hint']\n",
    "        else:\n",
    "            raise ValueError(f'Invalid task: {task}')\n",
    "        all_outputs.append(final_output)\n",
    "\n",
    "        if len(all_outputs) == k:\n",
    "            with open(proj_path / 'experiments' / task / f'{file_name}_{i//k}.jsonl', 'w') as f:\n",
    "                for d in all_outputs:\n",
    "                    f.write(json.dumps(d) + '\\n')\n",
    "            all_outputs = list()\n",
    "\n",
    "    if len(all_outputs) > 0:\n",
    "        with open(proj_path / 'experiments' / task / f'{file_name}_{i//k}.jsonl', 'w') as f:\n",
    "            for d in all_outputs:\n",
    "                f.write(json.dumps(d) + '\\n')\n",
    "\n",
    "def get_bo_sample_data(df):\n",
    "    data = []\n",
    "    for i, row in df.iterrows():\n",
    "        sample = SpiderSample(\n",
    "            sample_id=row['sample_id'],\n",
    "            db_id=row['db_id'],\n",
    "            final=QuestionSQL(\n",
    "                question=row['question'],\n",
    "                sql=row['gold_sql'],\n",
    "                source_tables=eval(row['source_tables']),\n",
    "            ),\n",
    "            bo=BusinessObject(\n",
    "                obj_id=row['sample_id'],\n",
    "                virtual_table=row['virtual_table'],\n",
    "                description='' if row.get('description') is None else row['description']\n",
    "            )\n",
    "        )\n",
    "        data.append(sample)\n",
    "    return data\n",
    "\n",
    "def run_bo_sql_gen(\n",
    "        proj_path, task, spider_tables, chain, type_exp,\n",
    "        vectorstore: FAISS, n_retrieval: int = 3, score_threshold: float = 0.65\n",
    "    ):\n",
    "    df = pd.read_csv(proj_path / 'data' / 'spilt_in_domain' / f'spider_bo_desc{type_exp}.csv')\n",
    "    samples = get_bo_sample_data(df)\n",
    "    predict_sql(samples, spider_tables, chain, task, \n",
    "                k=100, \n",
    "                file_name=f'bo_sql_output{type_exp}',\n",
    "                vectorstore=vectorstore,\n",
    "                n_retrieval=n_retrieval,\n",
    "                score_threshold=score_threshold)\n",
    "\n",
    "    bos = []\n",
    "    for p in sorted((proj_path / 'experiments' / task).glob(f'bo_sql_output{type_exp}_*.jsonl'), key=lambda x: int(x.stem.split('_')[-1])):\n",
    "        with p.open() as f:\n",
    "            for line in f:\n",
    "                bos.append(json.loads(line))\n",
    "\n",
    "    with (proj_path / 'experiments' / f'{task}.jsonl').open('w') as f:\n",
    "        for bo in bos:\n",
    "            f.write(json.dumps(bo) + '\\n')\n",
    "\n",
    "def get_vector_store(proj_path, typ):\n",
    "    df_train = pd.read_csv(proj_path / 'data' / 'spilt_in_domain' / f'spider_bo_desc{typ}_train.csv')\n",
    "    documents = []\n",
    "    for i, row in df_train.iterrows():\n",
    "        if typ == '_c':\n",
    "            if row['need_low|wrong']:\n",
    "                level = 'low'\n",
    "            elif row['need_mid|wrong']:\n",
    "                level = 'mid'\n",
    "            elif row['need_high|wrong']:\n",
    "                level = 'high'\n",
    "            else:\n",
    "                raise ValueError('The complexity level is not defined.')\n",
    "        elif typ == '_t':\n",
    "            if row['need_1|wrong']:\n",
    "                level = '1'\n",
    "            elif row['need_2|wrong']:\n",
    "                level = '2'\n",
    "            elif row['need_3+|wrong']:\n",
    "                level = '3+'\n",
    "            else:\n",
    "                raise ValueError('The complexity level is not defined.')\n",
    "        else:\n",
    "            raise ValueError('Invalid type (`typ`)')\n",
    "\n",
    "        doc = Document(\n",
    "            doc_id=row['sample_id'],\n",
    "            page_content=row['description'],\n",
    "            metadata={\n",
    "                'sample_id': row['sample_id'],\n",
    "                'db_id': row['db_id'],\n",
    "                'question': row['question'],\n",
    "                'gold_sql': row['gold_sql'],\n",
    "                'source_tables': row['source_tables'],\n",
    "                'cate_len_tbls': row['cate_len_tbls'],\n",
    "                'gold_c': row['gold_c'],\n",
    "                'level': level,\n",
    "                'virtual_table': row['virtual_table']\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents, \n",
    "        embedding = embeddings_model,\n",
    "        distance_strategy = DistanceStrategy.EUCLIDEAN_DISTANCE\n",
    "    )\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    output: str = Field(description='The full SQL query.')\n",
    "    rationale: list[str] = Field(description='The step-by-step reasoning to generate the SQL query. ')\n",
    "\n",
    "template = '''### TASK\n",
    "You are tasked with generating a SQL query(in a SQLite Database) according to a user input NL question.\n",
    "You should work in step-by-step reasoning before coming to the full SQL query.\n",
    "\n",
    "### SCHEMA\n",
    "You are working with the following schema in a SQLite Database:\n",
    "{schema}\n",
    "\n",
    "### HINT\n",
    "You will be provided a hint to help you. It is called \"virtual table\".\n",
    "You will get either descriptions of the virtual tables or descriptions and templates of virtual tables together.\n",
    "You can use or modify the hint to generate the full SQL query.\n",
    "\n",
    "### FORMATTING\n",
    "Your output should be of the following JSON format:\n",
    "{{\n",
    "    \"rationale\": \"<list[str]: the step-by-step reasoning to generate the SQL query>\",\n",
    "    \"full_sql_query\": \"<str: the full SQL query>\"\n",
    "}}\n",
    "\n",
    "### OUTPUT\n",
    "<INPUT QUERY>: {input_query}\n",
    "<HINT>: {hint}\n",
    "<OUTPUT>: \n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['schema', 'input_query', 'hint'],\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "model = model_openai.with_structured_output(Response)\n",
    "chain = (prompt | model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "typ = '_c'  # '_t', '_c'\n",
    "typ2 = 'desc'  # 'desc', 'descvt'\n",
    "n_retrieval = 3  # 1, 3 \n",
    "score_threshold = 0.65\n",
    "iterator = ['low', 'mid', 'high'] if typ == '_c' else ['1', '2', '3+']\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "with (proj_path / 'data' / 'spider' / f'tables.json').open() as f:\n",
    "    tables = json.load(f)\n",
    "\n",
    "with (proj_path / 'data' / 'description.json').open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "\n",
    "spider_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "vectorstore = get_vector_store(proj_path, typ)\n",
    "\n",
    "for level in iterator:\n",
    "    task = f'sql_gen_hint_top{n_retrieval}_{level}_{typ2}'\n",
    "    # name hint: 'sql_gen_hint_top{n_retrieval}_[low/mid/high or 1/2/3+]_[desc/descvt]'\n",
    "    if not (proj_path / 'experiments' / task).exists():\n",
    "        (proj_path / 'experiments' / task).mkdir()\n",
    "    run_bo_sql_gen(\n",
    "        proj_path, task, spider_tables, chain, \n",
    "        type_exp=f'{typ}_test',\n",
    "        vectorstore=vectorstore,\n",
    "        n_retrieval=n_retrieval,\n",
    "        score_threshold=score_threshold\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
