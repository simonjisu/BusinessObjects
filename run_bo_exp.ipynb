{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import sqlparse\n",
    "from src.database import SqliteDatabase\n",
    "from src.eval import result_eq, check_if_exists_orderby\n",
    "from src.eval_complexity import eval_all\n",
    "from src.process_sql import get_schema, Schema\n",
    "from src.parsing_sql import (\n",
    "    extract_selection, \n",
    "    extract_condition, \n",
    "    extract_aggregation, \n",
    "    extract_nested_setoperation, \n",
    "    extract_others,\n",
    "    extract_aliases,\n",
    ")\n",
    "proj_path = Path('.').resolve()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(proj_path / 'data' / 'split_in_domain' / 'spider_bo_desc_train.csv')\n",
    "df_test = pd.read_csv(proj_path / 'data' / 'split_in_domain' / 'test.csv')\n",
    "df_pred = pd.read_csv(proj_path / 'experiments' / 'bo_evals' / 'test_exp1.csv')\n",
    "df_test = pd.merge(df_test, df_pred, on='sample_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error samples 33: 100%|██████████| 2018/2018 [00:17<00:00, 117.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing SQL errors: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_error_infos(df_test):\n",
    "\n",
    "    iterator = tqdm(df_test.iterrows(), total=len(df_test))\n",
    "    error_infos = {\n",
    "        'pred_exec': [],\n",
    "        'result': [],\n",
    "        'parsing_sql': [],\n",
    "        'error_samples': set(),\n",
    "    }\n",
    "\n",
    "    test_cols = ['c_low', 'c_mid', 'c_high', 't_1',  't_2',  't_3+']\n",
    "    for i, x in iterator:\n",
    "        has_error = False\n",
    "        schema = get_schema(str(proj_path / 'data' / 'spider' / 'database' / x['db_id'] / f'{x[\"db_id\"]}.sqlite'))\n",
    "        schema = Schema(schema)\n",
    "        \n",
    "        for test_col in test_cols:\n",
    "            try:\n",
    "                sql = x[test_col]\n",
    "                statement = sqlparse.parse(sql.strip())[0]\n",
    "                aliases = extract_aliases(statement)\n",
    "                selection = extract_selection(statement, aliases, schema)\n",
    "                condition = extract_condition(statement, aliases, schema)\n",
    "                aggregation = extract_aggregation(statement, aliases, schema)\n",
    "                nested = extract_nested_setoperation(statement)\n",
    "                others = extract_others(statement, aliases, schema)\n",
    "\n",
    "            except Exception as e:\n",
    "                has_error = True\n",
    "                error_infos['parsing_sql'].append((x['sample_id'], test_col, str(e)))\n",
    "                error_infos['error_samples'].add(x['sample_id'])\n",
    "                break\n",
    "        \n",
    "        if has_error:\n",
    "            continue\n",
    "\n",
    "        iterator.set_description_str(f'error samples {len(error_infos[\"error_samples\"])}')\n",
    "\n",
    "    print(f'Parsing SQL errors: {len(error_infos[\"parsing_sql\"])}')\n",
    "    return error_infos\n",
    "\n",
    "error_infos = get_error_infos(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing c_low:   0%|          | 0/1985 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing c_low: 100%|██████████| 1985/1985 [00:20<00:00, 98.85it/s] \n",
      "Processing c_mid: 100%|██████████| 1985/1985 [00:19<00:00, 100.32it/s]\n",
      "Processing c_high: 100%|██████████| 1985/1985 [00:18<00:00, 104.50it/s]\n",
      "Processing t_1: 100%|██████████| 1985/1985 [00:20<00:00, 97.58it/s] \n",
      "Processing t_2: 100%|██████████| 1985/1985 [00:22<00:00, 89.15it/s] \n",
      "Processing t_3+: 100%|██████████| 1985/1985 [00:24<00:00, 81.25it/s] \n"
     ]
    }
   ],
   "source": [
    "# def process_task(task, error_infos):\n",
    "test_cols = ['c_low', 'c_mid', 'c_high', 't_1',  't_2',  't_3+']\n",
    "eval_cols = ['score', 's_sel', 's_cond', 's_agg', 's_nest', 's_oth']\n",
    "\n",
    "df = df_test.loc[~df_test['sample_id'].isin(error_infos['error_samples'])].reset_index(drop=True)\n",
    "for test_col in test_cols:\n",
    "    df_exp = df.loc[:, ['sample_id', 'db_id', 'gold_sql', test_col]]\n",
    "    iterator = tqdm(df_exp.iterrows(), total=len(df_exp), desc=f'Processing {test_col}')\n",
    "    # init task eval results\n",
    "    task_results = {'sample_id': []}\n",
    "    for col in eval_cols:\n",
    "        task_results[f'{test_col}_{col}'] = []\n",
    "\n",
    "    for i, x in iterator:\n",
    "        task_results['sample_id'].append(x['sample_id'])\n",
    "        # parsing sql\n",
    "        schema = get_schema(str(proj_path / 'data' / 'spider' / 'database' / x['db_id'] / f'{x[\"db_id\"]}.sqlite'))\n",
    "        schema = Schema(schema)\n",
    "        \n",
    "        # partial & complexity eval\n",
    "        parsed_result = {}\n",
    "        for k in ['gold', 'pred']:\n",
    "            sql = x[test_col] if k == 'pred' else x['gold_sql']\n",
    "            statement = sqlparse.parse(sql.strip())[0]\n",
    "            aliases = extract_aliases(statement)\n",
    "            selection = extract_selection(statement, aliases, schema)\n",
    "            condition = extract_condition(statement, aliases, schema)\n",
    "            aggregation = extract_aggregation(statement, aliases, schema)\n",
    "            nested = extract_nested_setoperation(statement)\n",
    "            others = extract_others(statement, aliases, schema)\n",
    "\n",
    "            parsed_result[k + '_selection'] = selection\n",
    "            parsed_result[k + '_condition'] = condition\n",
    "            parsed_result[k + '_aggregation'] = aggregation\n",
    "            parsed_result[k + '_nested'] = nested\n",
    "            parsed_result[k + '_others'] = {\n",
    "                'distinct': others['distinct'], \n",
    "                'order by': others['order by'], \n",
    "                'limit': others['limit']\n",
    "            }\n",
    "\n",
    "        eval_res = eval_all(parsed_result, k=6)\n",
    "        task_results[f'{test_col}_s_sel'].append(eval_res['score']['selection'])\n",
    "        task_results[f'{test_col}_s_cond'].append(eval_res['score']['condition'])\n",
    "        task_results[f'{test_col}_s_agg'].append(eval_res['score']['aggregation'])\n",
    "        task_results[f'{test_col}_s_nest'].append(eval_res['score']['nested'])\n",
    "        task_results[f'{test_col}_s_oth'].append(eval_res['score']['others'])\n",
    "        \n",
    "        # execution eval\n",
    "        database = SqliteDatabase(\n",
    "            str(proj_path / 'data' / 'spider' / 'database' / x['db_id'] / f'{x[\"db_id\"]}.sqlite')\n",
    "        )\n",
    "        error_info = ''\n",
    "        try:\n",
    "            pred_result = database.execute(x[test_col], rt_pandas=False)\n",
    "        except Exception as e:\n",
    "            pred_result = []\n",
    "            error_info = 'Predction Execution Error:' + str(e)\n",
    "            score = 0\n",
    "\n",
    "        try:\n",
    "            gold_result = database.execute(x['gold_sql'], rt_pandas=False)\n",
    "        except Exception as e:\n",
    "            error_info = 'Gold Execution Error:' + str(e)\n",
    "\n",
    "        if 'Gold Execution Error' in error_info:\n",
    "            continue\n",
    "        elif 'Predction Execution Error' in error_info:\n",
    "            task_results[f'{test_col}_score'].append(score)\n",
    "            continue\n",
    "        else:\n",
    "            exists_orderby = check_if_exists_orderby(x['gold_sql'])\n",
    "            score = int(result_eq(pred_result, gold_result, order_matters=exists_orderby))\n",
    "            task_results[f'{test_col}_score'].append(score)\n",
    "\n",
    "    df_temp = pd.DataFrame(task_results)\n",
    "    df_test = pd.merge(df_test, df_temp, on='sample_id', how='left')\n",
    "    df_temp.to_csv(proj_path / 'experiments' / 'bo_evals' / f'{test_col}.csv', index=False)\n",
    "    # return task_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(proj_path / 'experiments' / 'bo_evals' / 'all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numbers of BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>db_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>activity_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aircraft</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allergy_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apartment_rentals</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architecture</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.25</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workshop_paper</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>5.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrestler</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wta_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   min   25%   50%   75%   max\n",
       "db_id                                         \n",
       "activity_1         1.0  6.00  12.0  21.0  21.0\n",
       "aircraft           1.0  5.00   8.0  14.0  14.0\n",
       "allergy_1          1.0  8.00   8.0  21.0  21.0\n",
       "apartment_rentals  1.0  7.00  14.0  14.0  14.0\n",
       "architecture       1.0  2.00   6.0  10.0  10.0\n",
       "...                ...   ...   ...   ...   ...\n",
       "wine_1             1.0  8.25   9.0  22.0  22.0\n",
       "workshop_paper     1.0  3.75   5.5   7.0   7.0\n",
       "world_1            1.0  8.00  14.0  28.0  31.0\n",
       "wrestler           1.0  2.00   7.0   7.0   7.0\n",
       "wta_1              1.0  7.00  13.0  14.0  14.0\n",
       "\n",
       "[160 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(proj_path / 'data' / 'split_in_domain' / 'spider_bo_desc_train.csv')\n",
    "df_pm_stats = df_train.groupby(['db_id'])['pm_score_rank'].describe().loc[:, ['min', '25%', '50%', '75%', 'max']]\n",
    "df_pm_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "proj_path = Path('.').resolve()\n",
    "\n",
    "def filter_by_pm_score(x: pd.Series, df_pm_stats: pd.DataFrame, percentile: int):\n",
    "    rank_criteria = df_pm_stats.loc[x['db_id'], f'{percentile}%']\n",
    "    return x['pm_score_rank'] < rank_criteria\n",
    "\n",
    "def get_vector_store(proj_path, percentile: Optional[str]=None):\n",
    "    df_train = pd.read_csv(proj_path / 'data' / 'split_in_domain' / f'spider_bo_desc_train.csv')\n",
    "    documents = []\n",
    "    if percentile:\n",
    "        df_pm_stats = df_train.groupby(['db_id'])['pm_score_rank'].describe().loc[:, ['25%', '50%', '75%']]\n",
    "        pm_idx = df_train.apply(lambda x: filter_by_pm_score(x, df_pm_stats, percentile), axis=1)\n",
    "        df_train = df_train.loc[pm_idx].reset_index(drop=True)\n",
    "\n",
    "    for i, row in df_train.iterrows():\n",
    "        doc = Document(\n",
    "            doc_id=row['sample_id'],\n",
    "            page_content=row['description'],\n",
    "            metadata={\n",
    "                'sample_id': row['sample_id'],\n",
    "                'db_id': row['db_id'],\n",
    "                'cate_gold_c': row['cate_gold_c'],\n",
    "                'cate_len_tbls': row['cate_len_tbls'],\n",
    "                'virtual_table': row['virtual_table']\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents, \n",
    "        embedding = embeddings_model,\n",
    "        distance_strategy = DistanceStrategy.EUCLIDEAN_DISTANCE\n",
    "    )\n",
    "    return vectorstore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
