{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simonjisu/code/BusinessObjects/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/simonjisu/code/BusinessObjects/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "proj_path = Path('.').resolve()\n",
    "sys.path.append(str(proj_path))\n",
    "\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from src.db_utils import get_schema_str, get_db_file\n",
    "from src.pymodels import (\n",
    "    DatabaseModel, \n",
    "    SpiderSample, \n",
    "    BirdSample, \n",
    "    BODescription,\n",
    "    SQLResponse,\n",
    "    KeywordExtractionResponse,\n",
    "    GenTemplateResponse\n",
    ")\n",
    "from src.prompts import Prompts\n",
    "from src.database import SqliteDatabase\n",
    "from src.data_preprocess import (\n",
    "    load_raw_data,\n",
    "    process_all_tables,\n",
    "    filter_samples_by_count_spider_bird,\n",
    "    process_samples_bird,\n",
    "    split_train_dev_test,\n",
    "    save_samples_spider_bird,\n",
    "    load_samples_spider_bird,\n",
    ")\n",
    "from src.eval_utils import (\n",
    "    get_complexity, \n",
    "    run_sqls_parallel,\n",
    "    get_all_structural_score,\n",
    "    get_all_semantic_score,\n",
    "    # run_sqls,\n",
    "    execute_model,\n",
    "    SKIP_DB_IDS\n",
    ")\n",
    "\n",
    "from src.parsing_sql import (\n",
    "    Schema, extract_all, extract_aliases, _format_expression, STRING_TYPE, NUMERIC_TYPE\n",
    ")\n",
    "from run_bo_sql import get_vector_store\n",
    "from copy import deepcopy\n",
    "\n",
    "from src.eval_utils import get_structural_score, get_all_structural_score, get_all_semantic_score, partial_matching_with_penalty\n",
    "from run_evaluation import get_target_parsed_sql, get_prediction_parsed_sql\n",
    "from run_bo_sql import batched, _get_categories, _format_interval, get_retriever\n",
    "from bert_score import score as bscore\n",
    "from transformers import logging as tfloggings\n",
    "tfloggings.set_verbosity_error()\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.globals import set_llm_cache\n",
    "# from langchain_community.cache import SQLiteCache\n",
    "# set_llm_cache(SQLiteCache(database_path=f\"./cache/valid_bo_bird_dev.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def get_content(x: dict):\n",
    "    return json.loads(x['kwargs']['message']['kwargs']['content'])\n",
    "task = 'direct'  \n",
    "# task = 'keyword_extraction'\n",
    "db_id = 'movie_3'\n",
    "prefix = 'x-'\n",
    "file_name = f'{task}_{db_id}'\n",
    "db = SqliteDatabase(f\"./cache/{prefix}{file_name}.db\")\n",
    "\n",
    "# df = db.execute(\"SELECT * FROM full_llm_cache WHERE ROWID = (SELECT MAX(ROWID) FROM full_llm_cache);\")\n",
    "df = db.execute(\"SELECT ROWID, response FROM full_llm_cache;\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hint_used': False,\n",
      " 'rationale': ['Identify the customer by name: We need to find the customer '\n",
      "               \"'Marcelia Goering' in the 'customer' table to get the \"\n",
      "               'customer_id.',\n",
      "               'Join the necessary tables: To find the orders placed by this '\n",
      "               \"customer, we need to join the 'customer', 'cust_order', and \"\n",
      "               \"'order_line' tables.\",\n",
      "               'Filter by year: We need to filter the orders that were placed '\n",
      "               'in the year 2021 by checking the order_date in the '\n",
      "               \"'cust_order' table.\",\n",
      "               'Filter by shipping method: We also need to ensure that the '\n",
      "               \"shipping method used is 'Priority Shipping' by joining with \"\n",
      "               \"the 'shipping_method' table.\"],\n",
      " 'sql': 'SELECT COUNT(*) FROM cust_order AS co INNER JOIN customer AS c ON '\n",
      "        'co.customer_id = c.customer_id INNER JOIN shipping_method AS sm ON '\n",
      "        \"co.shipping_method_id = sm.method_id WHERE c.first_name = 'Marcelia' \"\n",
      "        \"AND c.last_name = 'Goering' AND co.order_date LIKE '2021%' AND \"\n",
      "        \"sm.method_name = 'Priority Shipping'\"}\n"
     ]
    }
   ],
   "source": [
    "content = get_content(json.loads(df['response'].iloc[-3]))\n",
    "pprint(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowid</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>601</td>\n",
       "      <td>{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rowid                                           response\n",
       "599    601  {\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langc..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = '''To return latitude and longitude along with station details, I will select the lat and long columns from the station table.\n",
    "'''.strip()\n",
    "df.loc[df['response'].str.contains(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'601'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idxes = df['response'].apply(lambda x: True if get_content(json.loads(x)).get(' ') else False)\n",
    "# idxes = df['response'].str.contains(\"google_entity_id_id\")\n",
    "# idxes = ','.join(df.loc[idxes, 'rowid'].astype(str).tolist())\n",
    "# idxes\n",
    "idxes = ','.join(df.loc[df['response'].str.contains(x)].loc[:, 'rowid'].astype(str).values.tolist())\n",
    "idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'{task}_{db_id}'\n",
    "db = SqliteDatabase(f\"./cache/{prefix}{file_name}.db\")\n",
    "db.start()\n",
    "c = db.con.cursor()\n",
    "c.execute('BEGIN TRANSACTION')\n",
    "\n",
    "# remove the last row record\n",
    "c.execute(f\"\"\"\n",
    "DELETE FROM full_llm_cache\n",
    "WHERE ROWID IN ({idxes});\n",
    "\"\"\")\n",
    "db.con.commit()\n",
    "db.close()\n",
    "# (SELECT MAX(ROWID) FROM full_llm_cache)\n",
    "# JSONDecodeError\n",
    "# ValidationError\n",
    "# c.execute(\"\"\"\n",
    "# DELETE FROM full_llm_cache WHERE response LIKE '%JSONDecodeError%';\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "df = db.execute(\n",
    "'''\n",
    "SELECT * FROM full_llm_cache\n",
    "WHERE prompt LIKE '%Which teams have had a player awarded the Purple Cap and another with the Orange Cap%'\n",
    "''')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval_utils import get_structural_score, get_all_structural_score, get_all_semantic_score, partial_matching_with_penalty\n",
    "from run_evaluation import get_target_parsed_sql, get_prediction_parsed_sql\n",
    "from run_bo_sql import _get_categories, _format_interval, get_retriever\n",
    "from bert_score import score as bscore\n",
    "from transformers import logging as tfloggings\n",
    "tfloggings.set_verbosity_error()\n",
    "import warnings\n",
    "\n",
    "ds = 'spider'\n",
    "task = 'zero_shot_hint'\n",
    "typ = 'test'\n",
    "scenario = 0\n",
    "description_file = f'description.json' if ds == 'spider' else f'{ds}_description.json'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bird_path = proj_path / 'data' / 'bird'\n",
    "# tables, train_data, dev_data = load_raw_data(bird_path, load_test=False)\n",
    "\n",
    "# with (proj_path / 'data' / 'bird_description.json').open() as f:\n",
    "#     all_descriptions = json.load(f)\n",
    "\n",
    "# bird_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "# train_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_train.json')\n",
    "# dev_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_dev.json')\n",
    "# test_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_post_fix = f'{ds}_{typ}' if scenario < 0 else f'{ds}_{typ}_{scenario}'\n",
    "# final_file = f'final_{file_post_fix}.json'\n",
    "# samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_{typ}.json')\n",
    "\n",
    "# if not (prediction_path / final_file).exists():\n",
    "#     all_results = []\n",
    "#     paths = sorted(list(prediction_path.glob(f'{file_post_fix}_*.json')))\n",
    "#     for p in paths:\n",
    "#         with p.open() as f:\n",
    "#             results = json.load(f)\n",
    "            \n",
    "#         for r in results:\n",
    "#             r.pop('rationale')\n",
    "#             r['db_id'] = p.stem.split('_', 3)[-1]\n",
    "\n",
    "#             found = False\n",
    "#             for s in samples:\n",
    "#                 if r['sample_id'] == s.sample_id:\n",
    "#                     found = True\n",
    "#                     break\n",
    "#             r['gold_sql'] = s.final.sql\n",
    "#             assert found, r['sample_id']\n",
    "\n",
    "#         all_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(prediction_path / final_file, 'r') as f:\n",
    "#     preds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_parsed, _ = get_prediction_parsed_sql(preds, tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'bird'\n",
    "typ = 'dev'\n",
    "task = 'retrieval' # 'zero_shot', 'retrieval', 'valid_bo'\n",
    "description_file = f'description.json' if ds == 'spider' else f'{ds}_description.json'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_{typ}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.json', 'r') as f:\n",
    "#     train_bo = json.load(f)\n",
    "\n",
    "# df = []\n",
    "# for db_id, xs in train_bo.items():\n",
    "#     for x in xs:\n",
    "#         x['db_id'] = db_id\n",
    "#         df.append(x)\n",
    "\n",
    "# df = pd.DataFrame(df)\n",
    "# cates = pd.qcut(df['gold_complexity'], q=5)\n",
    "# df['gold_complexity_cates'] = cates\n",
    "# df['gold_complexity_codes'] = cates.cat.codes\n",
    "# df.to_csv(experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.csv', index=False)\n",
    "# df = pd.read_csv(experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.csv')\n",
    "\n",
    "# df_train = df.groupby('gold_complexity_codes').sample(frac=0.9, random_state=42)\n",
    "# df_dev = df.drop(df_train.index)\n",
    "\n",
    "# print(df_train['gold_complexity_codes'].value_counts().sort_index())\n",
    "# print(df_dev['gold_complexity_codes'].value_counts().sort_index())\n",
    "\n",
    "# df_train.to_csv(experiment_folder / 'predictions' / 'create_bo' / f'{ds}_{task}_trainset.csv', index=False)\n",
    "# df_dev.to_csv(experiment_folder / 'predictions' / 'create_bo' / f'{ds}_{task}_devset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedures\n",
    "\n",
    "## task: valid_bo\n",
    "\n",
    "1. (retrieve) retrieve and rerank for dev dataset to get relevant train BOs\n",
    "2. (gen_template) generate sql template with dev dataset and retrieved BOs --> output: dev sql templates\n",
    "3. (keyword_extraction) extract keywords from dev sql templates --> output: keywords\n",
    "4. (search_value) search for the most helpful values for the dev sql templates --> output: columns with values\n",
    "5. (fill_in) fill-in values with dev sql templates --> output: dev sql\n",
    "6. (evaluate) evaluate the dev sql to get the most helpful train BOs --> output: BOs Pool\n",
    "\n",
    "## task: inference_with_bo\n",
    "\n",
    "1. (retrieve) retrieve and rerank for test dataset from BOs Pool\n",
    "2. (gen_template) generate sql template with test dataset and retrieved BOs --> output: test sql templates\n",
    "3. (keyword_extraction) extract keywords from dev sql templates --> output: keywords\n",
    "4. (search_value) search for the most helpful values for the dev sql templates --> output: columns with values\n",
    "5. (fill_in) fill-in values with test sql templates --> output: test sql\n",
    "6. (evaluate) evaluate the test sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS():\n",
    "    exp_name = 'pipeline_exp' # 'pipeline_exp'\n",
    "    ds = 'bird'\n",
    "    type = 'dev'\n",
    "    task = 'evaluate' \n",
    "    eval_target = 'fill_in' # 'fill_in'\n",
    "    with_bos = False\n",
    "    \n",
    "args = ARGS()\n",
    "\n",
    "description_file = f'description.json' if args.ds == 'spider' else f'{args.ds}_description.json'\n",
    "experiment_folder = proj_path / args.exp_name / args.ds\n",
    "eval_path = experiment_folder / 'evals'\n",
    "\n",
    "description_file = 'bird_description.json' if args.ds == 'bird' else 'description.json'\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / args.ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "def parse_sql_to_output(sql: str, schema: Schema):\n",
    "    try:\n",
    "        ei = extract_all(sql, schema)\n",
    "        assert len(ei['sel']) > 0, f'No selection found'\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    return ei\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{args.ds}_{args.type}.json')\n",
    "\n",
    "file_name = f'{\"with_bos\" if args.with_bos else \"no_bos\"}-{args.type}'\n",
    "eval_target_path = experiment_folder / 'predictions' / args.eval_target / f'{file_name}.json'\n",
    "assert eval_target_path.exists(), f'Run with the `task={args.eval_target}, type={args.type}` first'\n",
    "with eval_target_path.open() as f:\n",
    "    eval_targets = json.load(f)\n",
    "\n",
    "# TODO: for loop should be start from pred_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preparing eval data: 100%|██████████| 56912/56912 [00:02<00:00, 22534.92it/s]\n"
     ]
    }
   ],
   "source": [
    "parsed_queries_path: Path = eval_path / f'parsed_queries-{file_name}.pkl'\n",
    "parsed_queries_exist = parsed_queries_path.exists()\n",
    "if parsed_queries_exist:\n",
    "    with parsed_queries_path.open('rb') as f:\n",
    "        parsed_queries = pickle.load(f)\n",
    "else:\n",
    "    parsed_queries = defaultdict(dict)\n",
    "    parsed_queries['unable'] = set()\n",
    "\n",
    "samples_by_id = {s.sample_id: s for s in samples}\n",
    "eval_data: dict[str, list] = defaultdict(list)\n",
    "eval_data2doc_ids = defaultdict(set)\n",
    "\n",
    "# from flatten to nested by doc_ids\n",
    "for pred in tqdm(eval_targets, total=len(eval_targets), desc='preparing eval data'):\n",
    "    sample_id = pred['sample_id']\n",
    "    pred_sql = pred['sql']\n",
    "    doc_ids = pred['doc_ids']\n",
    "    target_sample = samples_by_id[sample_id]\n",
    "    target_sql = target_sample.final.sql\n",
    "    db_id = target_sample.db_id\n",
    "    key = hashlib.sha256(f'{sample_id}-{pred_sql}'.encode()).hexdigest()\n",
    "\n",
    "    if not parsed_queries_exist:\n",
    "        schema = Schema(tables[db_id].db_schema)\n",
    "        if parsed_queries['target'].get(target_sql) is None:\n",
    "            target_output = parse_sql_to_output(target_sql, schema)\n",
    "            parsed_queries['target'][target_sql] = target_output\n",
    "        else:\n",
    "            target_output = parsed_queries['target'][target_sql]\n",
    "        if parsed_queries['pred'].get(pred_sql) is None:\n",
    "            pred_output = parse_sql_to_output(pred_sql, schema)\n",
    "            parsed_queries['pred'][pred_sql] = pred_output\n",
    "        else:\n",
    "            pred_output = parsed_queries['pred'][pred_sql]\n",
    "    \n",
    "        if (not pred_output) or (not target_output):\n",
    "            if key not in parsed_queries['unable']:\n",
    "                parsed_queries['unable'].add(key)\n",
    "            continue\n",
    "    else:\n",
    "        if key in parsed_queries['unable']:\n",
    "            continue\n",
    "    \n",
    "    if key not in eval_data2doc_ids:\n",
    "        eval_data['sample_ids'].append(sample_id)\n",
    "        eval_data['target_queries'].append(target_sql)\n",
    "        eval_data['db_paths'].append(get_db_file(proj_path, args.ds, db_id))\n",
    "        eval_data['pred_queries'].append(pred_sql)\n",
    "        eval_data2doc_ids[key].update(doc_ids)\n",
    "\n",
    "    for doc_id in doc_ids:\n",
    "        eval_data2doc_ids[key].add(doc_id)\n",
    "\n",
    "if not parsed_queries_exist:\n",
    "    with parsed_queries_path.open('wb') as f:\n",
    "        pickle.dump(parsed_queries, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2136\n",
      "SELECT CAST(SUM(CASE WHEN T2.value > 50 THEN 1 ELSE 0 END) AS REAL) * 100 / COUNT(T1.CountryCode) FROM Country AS T1 INNER JOIN Indicators AS T2 ON T1.CountryCode = T2.CountryCode WHERE T1.Region = 'South Asia' AND T2.IndicatorName = 'Life expectancy at birth, female (years)'\n",
      "/home/simonjisu/code/BusinessObjects/data/bird/train/train_databases/world_development_indicators/world_development_indicators.sqlite\n",
      "SELECT (COUNT(CASE WHEN indicators.value > 50 THEN 1 END) * 100.0 / COUNT(*)) AS percentage FROM country INNER JOIN indicators ON country.countrycode = indicators.countrycode WHERE country.region = 'South Asia' AND indicators.indicatorname = 'Life expectancy at birth, female (years)'\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "sample_id = eval_data['sample_ids'][i]\n",
    "target_query = eval_data['target_queries'][i]\n",
    "db_file = eval_data['db_paths'][i]\n",
    "pred_query = eval_data['pred_queries'][i]\n",
    "db = SqliteDatabase(db_file)\n",
    "\n",
    "print(sample_id)\n",
    "print(target_query)\n",
    "print(db_file)\n",
    "print(pred_query)\n",
    "result = execute_model(pred_query, target_query, db_file, sample_id, 30.0)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, islice\n",
    "def batched(iterable, n, *, strict=False):\n",
    "    # batched('ABCDEFG', 3) → ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    iterator = iter(iterable)\n",
    "    while batch := tuple(islice(iterator, n)):\n",
    "        if strict and len(batch) != n:\n",
    "            raise ValueError('batched(): incomplete batch')\n",
    "        yield batch\n",
    "\n",
    "n_batch = 2000\n",
    "n_samples = len(eval_data['sample_ids'])\n",
    "batches = list(batched(range(n_samples), n_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 7\n",
    "idxes = batches[batch_i]\n",
    "batch_results = []\n",
    "batch_eval_data = {k: [v[i] for i in idxes] for k, v in eval_data.items()}\n",
    "batch_preds = [x for x in batch_eval_data['pred_queries']]\n",
    "batch_sample_ids = [x for x in batch_eval_data['sample_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = batch_eval_data['sample_ids']\n",
    "pred_queries = batch_eval_data['pred_queries']\n",
    "target_queries = batch_eval_data['target_queries']\n",
    "db_paths = batch_eval_data['db_paths']\n",
    "exec_result = [None] * len(sample_ids)\n",
    "\n",
    "samples_by_db = defaultdict(list)\n",
    "for i, (sample_id, pred, target, db_file) in enumerate(zip(sample_ids, pred_queries, target_queries, db_paths)):\n",
    "    samples_by_db[db_file].append((i, sample_id, pred, target))\n",
    "\n",
    "# for db_file, samples in samples_by_db.items():\n",
    "#     for i, sample_id, pred, target in tqdm(\n",
    "#         samples, total=len(samples), desc=f\"Processing {Path(db_file).stem}\"\n",
    "#     ):\n",
    "        # if Path(db_file).stem in SKIP_DB_IDS:\n",
    "        #     result = {\"sample_id\": sample_id, \"res\": 0, \"target_error\": False}\n",
    "        # else:\n",
    "        #     result = execute_model(pred, target, db_file, sample_id, 30.0)\n",
    "        # exec_result[i] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT(product.productid)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COUNT(product.productid)\n",
       "0                         0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid = 10904\n",
    "\n",
    "sample_id = list(filter(lambda x: x == sid, sample_ids))[0]\n",
    "pred = pred_queries[i]\n",
    "target = target_queries[i]\n",
    "db_file = db_paths[i]\n",
    "\n",
    "target = target.replace(';', '')\n",
    "target_sql = f'SELECT * FROM ({target}) LIMIT {10};'\n",
    "pred = pred.replace(';', '')\n",
    "pred_sql = f'SELECT * FROM ({pred}) LIMIT {10};'\n",
    "db = SqliteDatabase(db_file)\n",
    "\n",
    "db.execute(pred_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = 10904\n",
    "\n",
    "idxes = list(filter(lambda x: x[1] == sid, enumerate(sample_ids)))\n",
    "for i, _ in idxes:\n",
    "    print(i)\n",
    "    pred = pred_queries[i]\n",
    "    target = target_queries[i]\n",
    "    db_file = db_paths[i]\n",
    "\n",
    "    target = target.replace(';', '')\n",
    "    target_sql = f'SELECT * FROM ( {target} ) LIMIT {10};'\n",
    "    pred = pred.replace(';', '')\n",
    "    pred_sql = f'SELECT * FROM ( {pred} ) LIMIT {10};'\n",
    "    db = SqliteDatabase(db_file)\n",
    "\n",
    "    db.execute(pred_sql)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SELECT customers.segment, ((SUM(CASE WHEN yearmonth.date = '2013-08-23' THEN yearmonth.consumption ELSE 0 END) - SUM(CASE WHEN yearmonth.date = '2012-08-23' THEN yearmonth.consumption ELSE 0 END)) / NULLIF(SUM(CASE WHEN yearmonth.date = '2012-08-23' THEN yearmonth.consumption ELSE 0 END), 0)) * 100 AS percentage_increase FROM customers INNER JOIN yearmonth ON customers.customerid = yearmonth.customerid WHERE yearmonth.date IN ('2012-08-23', '2013-08-23') AND customers.segment IN ('SME', 'LAM', 'KAM') GROUP BY customers.segment ORDER BY percentage_increase DESC LIMIT 1 SELECT customers.segment, ((SUM(CASE WHEN yearmonth.date = '2013-08-23' THEN yearmonth.consumption ELSE 0 END) - SUM(CASE WHEN yearmonth.date = '2012-08-23' THEN yearmonth.consumption ELSE 0 END)) / NULLIF(SUM(CASE WHEN yearmonth.date = '2012-08-23' THEN yearmonth.consumption ELSE 0 END), 0)) * 100 AS percentage_increase FROM customers INNER JOIN yearmonth ON customers.customerid = yearmonth.customerid WHERE yearmonth.date IN ('2012-08-23', '2013-08-23') AND customers.segment IN ('SME', 'LAM', 'KAM') GROUP BY customers.segment ORDER BY percentage_increase ASC LIMIT 1\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SELECT \n",
    "    customers.segment, (\n",
    "    (SUM(CASE WHEN yearmonth.date = '2013-08-23' \n",
    "    THEN yearmonth.consumption ELSE 0 END) - SUM(CASE WHEN yearmonth.date = '2012-08-23' \n",
    "    THEN yearmonth.consumption ELSE 0 END)) / NULLIF(SUM(CASE WHEN yearmonth.date = '2012-08-23' \n",
    "    THEN yearmonth.consumption ELSE 0 END), 0)) * 100 AS percentage_increase \n",
    "    FROM customers \n",
    "    INNER JOIN yearmonth \n",
    "    ON customers.customerid = yearmonth.customerid \n",
    "    WHERE yearmonth.date IN ('2012-08-23', '2013-08-23') AND customers.segment IN ('SME', 'LAM', 'KAM') \n",
    "    GROUP BY customers.segment ORDER BY percentage_increase DESC LIMIT 1 SELECT customers.segment, ((SUM(CASE WHEN yearmonth.date = '2013-08-23' THEN yearmonth.consumption ELSE 0 END) - SUM(CASE WHEN yearmonth.date = '2012-08-23' THEN yearmonth.consumption ELSE 0 END)) / NULLIF(SUM(CASE WHEN yearmonth.date = '2012-08-23' THEN yearmonth.consumption ELSE 0 END), 0)) * 100 AS percentage_increase FROM customers INNER JOIN yearmonth ON customers.customerid = yearmonth.customerid WHERE yearmonth.date IN ('2012-08-23', '2013-08-23') AND customers.segment IN ('SME', 'LAM', 'KAM') GROUP BY customers.segment ORDER BY percentage_increase ASC LIMIT 1\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS():\n",
    "    exp_name = 'pipeline_exp'# direct_exp, pipeline_exp\n",
    "    ds = 'bird'\n",
    "    type = 'dev'\n",
    "    task = 'evaluate'\n",
    "    eval_target = 'fill_in'  # direct, fill_in\n",
    "    with_bos = False\n",
    "    \n",
    "args = ARGS()\n",
    "\n",
    "description_file = f'description.json' if args.ds == 'spider' else f'{args.ds}_description.json'\n",
    "experiment_folder = proj_path / args.exp_name / args.ds\n",
    "eval_path = experiment_folder / 'evals'\n",
    "\n",
    "description_file = 'bird_description.json' if args.ds == 'bird' else 'description.json'\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / args.ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{args.ds}_{args.type}.json')\n",
    "# drop samples that are in SKIP_DB_IDS\n",
    "# samples = [s for s in samples if s.db_id not in SKIP_DB_IDS]\n",
    "\n",
    "file_name = f'{\"with_bos\" if args.with_bos else \"no_bos\"}-{args.type}'\n",
    "samples_by_id = {s.sample_id: s for s in samples}\n",
    "\n",
    "if args.eval_target == 'fill_in':  # pipeline method\n",
    "    # gen_template\n",
    "    with open(experiment_folder / 'predictions' / 'gen_template' / f'{file_name}.json', 'r') as f:\n",
    "        sql_templates = json.load(f)\n",
    "\n",
    "    # keyword_extraction\n",
    "    with open(experiment_folder / 'predictions' / 'keyword_extraction' / f'{file_name}.json', 'r') as f:\n",
    "        keyword_extraction = json.load(f)\n",
    "\n",
    "    # search_value\n",
    "    with open(experiment_folder / 'predictions' / 'search_value' / f'{file_name}.json', 'r') as f:\n",
    "        search_value = json.load(f)\n",
    "else:\n",
    "    sql_templates = []\n",
    "    keyword_extraction = []\n",
    "    search_value = []\n",
    "\n",
    "with open(experiment_folder / 'evals' / f'execution_result-{file_name}.json', 'r') as f:\n",
    "    exec_results = json.load(f)\n",
    "\n",
    "with open(experiment_folder / 'evals' / f'merit_result-{file_name}.json', 'r') as f:\n",
    "    merit_results = json.load(f)\n",
    "\n",
    "# print(len(exec_results))\n",
    "# skip_ids = [s.sample_id for s in samples if s.db_id in SKIP_DB_IDS]\n",
    "# exec_results = [r for r in exec_results if r['sample_id'] not in skip_ids]\n",
    "# merit_results = [r for r in merit_results if r['sample_id'] not in skip_ids]\n",
    "# samples_by_id = {s.sample_id: s for s in samples}\n",
    "# print(len(exec_results))\n",
    "\n",
    "all_results = defaultdict(list)\n",
    "# column: sample_id, db_id, retrieved, exec_res, structural_score, semantic_score, f1_score, target_complexity\n",
    "# if fill_in: bo_used, keywords, values\n",
    "for i in range(len(exec_results)):\n",
    "    sample_id = exec_results[i]['sample_id']\n",
    "    doc_ids = exec_results[i]['doc_ids']\n",
    "    exec_res = exec_results[i]['res']\n",
    "    structural_score = merit_results[i]['structural_score']\n",
    "    semantic_score = merit_results[i]['semantic_score']\n",
    "    f1_score = merit_results[i]['f1_score']\n",
    "    target_complexity = merit_results[i]['target_complexity']\n",
    "    sample = samples_by_id[sample_id]\n",
    "    db_id = sample.db_id\n",
    "\n",
    "    all_results['sample_id'].append(sample_id)\n",
    "    all_results['db_id'].append(db_id)\n",
    "    all_results['retrieved'].append(','.join(map(str, doc_ids)))\n",
    "    all_results['exec_res'].append(exec_res)\n",
    "    all_results['structural_score'].append(structural_score)\n",
    "    all_results['semantic_score'].append(semantic_score)\n",
    "    all_results['f1_score'].append(f1_score)\n",
    "    all_results['target_complexity'].append(target_complexity)\n",
    "    \n",
    "    if args.eval_target == 'fill_in':\n",
    "        if args.with_bos:\n",
    "            bo_used = int(sql_templates[i]['hint_used'])\n",
    "            all_results['bo_used'].append(bo_used)\n",
    "        keywords = json.dumps({column_name: kws for column_name, kws in keyword_extraction[i]['keywords'].items() if kws})\n",
    "        values = json.dumps(search_value[i]['values'])\n",
    "        all_results['keywords'].append(keywords)\n",
    "        all_results['values'].append(values)\n",
    "\n",
    "df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_path = experiment_folder / 'predictions' / 'create_bo' / f'final_{args.ds}_train_bo.json'\n",
    "with bo_path.open() as f:\n",
    "    bos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "address 5082",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m bo_ids \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m bos[db_id]]\n\u001b[1;32m      7\u001b[0m check \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m bo_id \u001b[38;5;129;01min\u001b[39;00m bo_ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m bo_id \u001b[38;5;129;01min\u001b[39;00m retrieved]\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(check), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: address 5082"
     ]
    }
   ],
   "source": [
    "sample_id_retrieved = df.groupby(['db_id', 'sample_id'])['retrieved'].apply(set).reset_index()\n",
    "for i, row in sample_id_retrieved.iterrows():\n",
    "    db_id = row['db_id']\n",
    "    sample_id = row['sample_id']\n",
    "    retrieved = row['retrieved']\n",
    "    bo_ids = [x['sample_id'] for x in bos[db_id]]\n",
    "    check = [True if bo_id in bo_ids else False for bo_id in retrieved]\n",
    "    assert all(check), f'{db_id} {sample_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### valid_bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS():\n",
    "    exp_name = 'pipeline_exp'\n",
    "    ds = 'bird'\n",
    "    type = 'dev'\n",
    "    task = 'fill_in'\n",
    "    with_bos = True\n",
    "    \n",
    "args = ARGS()\n",
    "\n",
    "description_file = f'description.json' if args.ds == 'spider' else f'{args.ds}_description.json'\n",
    "experiment_folder = proj_path / args.exp_name / args.ds\n",
    "\n",
    "description_file = 'bird_description.json' if args.ds == 'bird' else 'description.json'\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / args.ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{args.ds}_{args.type}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_DB_IDS = ['movie_platform']\n",
    "eval_path = experiment_folder / 'evals'\n",
    "\n",
    "no_bos_path = eval_path / f'result-no_bos-{args.type}.csv'\n",
    "with_bos_path = eval_path / f'result-with_bos-{args.type}.csv'\n",
    "assert no_bos_path.exists(), f'Run with the `task=evaluate, type={args.type}` first'\n",
    "assert with_bos_path.exists(), f'Run with the `task=evaluate, type={args.type}` first'\n",
    "\n",
    "df_no_bos = pd.read_csv(no_bos_path)\n",
    "df_no_bos = df_no_bos[~df_no_bos['db_id'].isin(SKIP_DB_IDS)]\n",
    "df_no_bos.reset_index(drop=True, inplace=True)\n",
    "df_no_bos.drop(columns=['retrieved'], inplace=True)\n",
    "df_with_bos = pd.read_csv(with_bos_path)\n",
    "df_cates = df_no_bos.groupby('db_id')['target_complexity'].apply(_get_categories).rename('category').apply(_format_interval)\n",
    "df_no_bos = pd.merge(df_no_bos, df_cates.reset_index('db_id', drop=True), left_index=True, right_index=True)\n",
    "df = pd.merge(\n",
    "    left=df_with_bos,\n",
    "    right=df_no_bos,\n",
    "    how='inner',\n",
    "    on=['db_id'],\n",
    "    suffixes=('_bo', '')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "address 5082",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m bos_ids \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m bos[db_id]]\n\u001b[1;32m      7\u001b[0m check \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m bo_id \u001b[38;5;129;01min\u001b[39;00m bos_ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m bo_id \u001b[38;5;129;01min\u001b[39;00m retrieved]\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(check), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: address 5082"
     ]
    }
   ],
   "source": [
    "sample_id_retrieved = df.groupby(['db_id', 'sample_id'])['retrieved'].apply(set).reset_index()\n",
    "for i, row in sample_id_retrieved.iterrows():\n",
    "    db_id = row['db_id']\n",
    "    sample_id = row['sample_id']\n",
    "    retrieved = row['retrieved']\n",
    "    bos_ids = [x['sample_id'] for x in bos[db_id]]\n",
    "    check = [True if bo_id in bos_ids else False for bo_id in retrieved]\n",
    "    assert all(check), f'{db_id} {sample_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.setdiff1d(df_with_bos['db_id'], df['db_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_column = ['db_id', 'retrieved']\n",
    "# count the exec result with/without BOs\n",
    "# > 0 means improved with BOs\n",
    "# = 0 means execution is same with/witout BOs\n",
    "# < 0 means worse with BOs\n",
    "execution_improvement = df.groupby(group_column)[['exec_res', 'exec_res_bo']].sum().diff(axis=1)['exec_res_bo'].rename('execution_improvement')\n",
    "\n",
    "# no_bos: tsed between (source=pred_sql, target=target_sql)\n",
    "# with_bos: tsed between (source=pred_sql, target=target_sql)\n",
    "# merit = tsed(pred_sql_bo, target_sql) - tsed(pred_sql, target_sql)\n",
    "# merit > 0 means similarity to the targer_sql improved with BOs\n",
    "# merit = 0 means similarity to the target_sql is same with/without BOs\n",
    "# merit < 0 means similarity to the target_sql getting worse with BOs\n",
    "merit_structural = df.groupby(group_column)[['structural_score', 'structural_score_bo']].mean().diff(axis=1)['structural_score_bo'].rename('merit_structural')\n",
    "merit_semantic = df.groupby(group_column)[['semantic_score', 'semantic_score_bo']].mean().diff(axis=1)['semantic_score_bo'].rename('merit_semantic')\n",
    "merit = df.groupby(group_column)[['f1_score', 'f1_score_bo']].mean().diff(axis=1)['f1_score_bo'].rename('merit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = merit.reset_index().groupby(['db_id'])['merit'].rank(method='first', ascending=False).rename('rank').astype(np.int64)\n",
    "merit = pd.concat([merit.reset_index(), ranks], axis=1)\n",
    "merit_by_rank = merit.sort_values(by=['db_id', 'rank'], ascending=True)\n",
    "# merit_by_rank.to_csv(experiment_folder / 'evals' / f'merits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "3058 not in address",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(test_bos[db_id]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n_bos_select:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m bo_id \u001b[38;5;129;01min\u001b[39;00m bos_ids_by_db_id[db_id], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m bo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m bo_id, bos[db_id]))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m test_bos[db_id]\u001b[38;5;241m.\u001b[39mappend(bo)\n",
      "\u001b[0;31mAssertionError\u001b[0m: 3058 not in address"
     ]
    }
   ],
   "source": [
    "n_bos_select = 25\n",
    "\n",
    "bo_path = experiment_folder / 'predictions' / 'create_bo' / f'final_{args.ds}_train_bo.json'\n",
    "with bo_path.open() as f:\n",
    "    bos = json.load(f)\n",
    "\n",
    "bos_ids_by_db_id = defaultdict(set)\n",
    "for db_id, xs in bos.items():\n",
    "    bos_ids_by_db_id[db_id].update({x['sample_id'] for x in xs})\n",
    "\n",
    "test_bos = defaultdict(list)\n",
    "for x in merit_by_rank.loc[:, ['db_id', 'retrieved']].to_dict(orient='records'):\n",
    "    bo_id = x['retrieved']\n",
    "    db_id = x['db_id']\n",
    "    if len(test_bos[db_id]) >= n_bos_select:\n",
    "        continue\n",
    "    \n",
    "    assert bo_id in bos_ids_by_db_id[db_id], f'{bo_id} not in {db_id}'\n",
    "    bo = list(filter(lambda x: x['sample_id'] == bo_id, bos[db_id]))[0]\n",
    "    test_bos[db_id].append(bo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed_sql(samples: dict, tables: dict):\n",
    "    error_ids = []\n",
    "    parsed = defaultdict(dict)\n",
    "    iterator = tqdm(samples, total=len(samples))\n",
    "    for sample in iterator:\n",
    "        db_id = sample.db_id\n",
    "        sample_id = sample.sample_id\n",
    "        iterator.set_description(f\"{db_id}\")\n",
    "        schema = Schema(tables[db_id].db_schema)\n",
    "        sql_i = sample.final.sql\n",
    "        try:\n",
    "            ei = extract_all(sql_i, schema)\n",
    "            assert len(ei['sel']) > 0, f'No selection found-{db_id}-{sample_id}'\n",
    "        except Exception as e:\n",
    "            error_ids.append((db_id, sample_id, str(e)))\n",
    "            parsed[db_id].append(None)\n",
    "            continue\n",
    "        parsed[db_id][sample_id] = ei\n",
    "    return parsed, error_ids\n",
    "\n",
    "train_parsed, error_ids = get_parsed_sql(train_samples, bird_tables)\n",
    "dev_parsed, error_ids = get_parsed_sql(dev_samples, bird_tables)\n",
    "test_parsed, error_ids = get_parsed_sql(test_samples, bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_parsed, f)\n",
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(dev_parsed, f)\n",
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_test_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_parsed, f)\n",
    "\n",
    "with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'rb') as f:\n",
    "    dev_parsed = pickle.load(f)\n",
    "\n",
    "with open(proj_path / 'data' / 'pkl_files' / 'bird_test_parsed.pkl', 'rb') as f:\n",
    "    test_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, product\n",
    "from collections import defaultdict\n",
    "from src.eval_utils import get_all_partial_score\n",
    "\n",
    "def measure_inter_score(parsed1: dict[str, tuple], parsed2: dict[str, tuple]):\n",
    "    results = defaultdict()\n",
    "    assert len(parsed1) == len(parsed2), f\"Length mismatch-1: {len(parsed1)} 2:{len(parsed2)}\"\n",
    "    db_ids = list(parsed1.keys())\n",
    "    for db_id in db_ids:\n",
    "        o1 = parsed1[db_id]\n",
    "        o2 = parsed2[db_id]\n",
    "        n1 = len(o1)\n",
    "        n2 = len(o2)\n",
    "        semantic_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "        structural_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "        overall_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "\n",
    "        idxs = list(product(range(n1), range(n2)))\n",
    "        iterator = tqdm(idxs, total=len(idxs), desc=f\"{db_id}\")\n",
    "        for i, j in iterator:\n",
    "            ei = o1[i]\n",
    "            ej = o2[j]\n",
    "\n",
    "            _, final_score = get_all_partial_score(ei, ej, use_bert=True)\n",
    "\n",
    "            structural_sim[i, j] = final_score['structural']\n",
    "            semantic_sim[i, j] = final_score['semantic']\n",
    "            overall_sim[i, j] = final_score['overall']\n",
    "\n",
    "        results[db_id] = {\n",
    "            'semantic': semantic_sim,\n",
    "            'struct': structural_sim,\n",
    "            'overall': overall_sim\n",
    "        }\n",
    "    return results\n",
    "\n",
    "results = measure_inter_score(dev_parsed, test_parsed)\n",
    "with (proj_path / 'data' / 'pkl_files' / 'bird_dev_test_similarity.pkl').open('wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity between datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_complexity(samples, tables):\n",
    "    cs = []\n",
    "    for s in tqdm(samples, total=len(samples)):\n",
    "        schema = Schema(tables[s.db_id].db_schema)\n",
    "        output = extract_all(s.final.sql, schema)\n",
    "        complexity = get_complexity(output)\n",
    "        cs.append(complexity)\n",
    "    return cs\n",
    "\n",
    "train_complexities = measure_complexity(train_samples, bird_tables)\n",
    "dev_complexities = measure_complexity(dev_samples, bird_tables)\n",
    "test_complexities = measure_complexity(test_samples, bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, n in zip([train_complexities, dev_complexities, test_complexities], ['train', 'dev  ', 'test ']):\n",
    "    print(f'[{n}] Mean={np.mean(c):.4f} +/-{np.std(c):.4f}, Median={np.median(c):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = defaultdict(list)\n",
    "for s in dev_samples:\n",
    "    stats[s.db_id].append(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
