{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 42\n",
    "# all_data = filter_samples_by_count_spider_bird(train_data+dev_data, n=10)\n",
    "\n",
    "# with open(proj_path / 'data' / 'bird_skip.txt') as f:\n",
    "#     skip = [int(line.strip()) for line in f]\n",
    "\n",
    "# bird_samples = process_samples_bird(all_data, bird_tables, skip=skip)\n",
    "# train_samples, dev_samples, test_samples = split_train_dev_test(bird_samples, train_ratio=0.6, dev_ratio=0.2, seed=seed)\n",
    "\n",
    "# save_samples_spider_bird(train_samples, proj_path / 'data' / 'bird_train.json')\n",
    "# save_samples_spider_bird(dev_samples, proj_path / 'data' / 'bird_dev.json')\n",
    "# save_samples_spider_bird(test_samples, proj_path / 'data' / 'bird_test.json')\n",
    "# print(len(train_samples), len(dev_samples), len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "proj_path = Path('.').resolve()\n",
    "sys.path.append(str(proj_path))\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from src.db_utils import get_schema_str, get_data_dict\n",
    "from src.pymodels import (\n",
    "    DatabaseModel, \n",
    "    SpiderSample, \n",
    "    BirdSample, \n",
    "    BODescription,\n",
    "    SQLResponse\n",
    ")\n",
    "from src.prompts import Prompts\n",
    "from src.database import SqliteDatabase\n",
    "from src.data_preprocess import (\n",
    "    load_raw_data,\n",
    "    process_all_tables,\n",
    "    filter_samples_by_count_spider_bird,\n",
    "    process_samples_bird,\n",
    "    split_train_dev_test,\n",
    "    save_samples_spider_bird,\n",
    "    load_samples_spider_bird,\n",
    ")\n",
    "\n",
    "from src.parsing_sql import Schema, extract_all\n",
    "from src.eval_utils import get_complexity, result_eq, check_if_exists_orderby\n",
    "from run_bo_sql import get_vector_store\n",
    "from copy import deepcopy\n",
    "bird_path = proj_path / 'data' / 'bird'\n",
    "tables, train_data, dev_data = load_raw_data(bird_path, load_test=False)\n",
    "\n",
    "with (proj_path / 'data' / 'bird_description.json').open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "\n",
    "bird_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "train_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_train.json')\n",
    "dev_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_dev.json')\n",
    "test_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_parsed.pkl', 'rb') as f:\n",
    "#     train_parsed = pickle.load(f)\n",
    "\n",
    "# # prediction parsed\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'rb') as f:\n",
    "#     dev_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = proj_path / 'experiments' / 'bird' / 'evals' / 'zero_shot'\n",
    "\n",
    "df = []\n",
    "for p in eval_path.glob('bird_dev_*.json'):\n",
    "    with p.open() as f:\n",
    "        for line in f:\n",
    "            eval_data = json.loads(line)\n",
    "            df.append(eval_data)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv(eval_path / 'bird_dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean      0.450361\n",
       "std       0.055482\n",
       "min       0.318118\n",
       "max       0.726155\n",
       "median    0.446118\n",
       "Name: gold_complexity, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gold_complexity'].agg(['mean', 'std', 'min', 'max', 'median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_path = proj_path / 'experiments' / 'bird' / 'predictions' / 'create_bo'\n",
    "bos = defaultdict(list)\n",
    "for p in prediction_path.glob('bird_train_bo_*.json'):\n",
    "    with p.open() as f:\n",
    "        temp = json.load(f)\n",
    "    \n",
    "    bos[p.stem.split('_', 3)[-1]] = temp\n",
    "\n",
    "# with (prediction_path / 'final_bird_train_bo.json').open('w') as f:\n",
    "#     json.dump(bos, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = get_vector_store({'address': bos['address']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search_with_score(\n",
    "    \"Will it be hot tomorrow?\", k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'sample_id': 5169, 'db_id': 'address', 'vt': \"SELECT COUNT(zip_data.zip_code) FROM state INNER JOIN zip_data AS T2 ON T1.abbreviation = T2.state WHERE state.name = '[placeholder-type:string]' AND zip_data.daylight_savings = '[placeholder-type:string]' AND zip_data.region = '[placeholder-type:string]'\"}, page_content=\"The virtual table counts the number of zip codes from the 'zip_data' table that are associated with a specific state, while also filtering for those that observe daylight savings and belong to a particular region. The placeholders represent the state name, daylight savings status, and region respectively.\"),\n",
       "  0.59726036)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def retriever(query: str) -> list[Document]:\n",
    "    docs, scores = zip(*vectorstore.similarity_search_with_score(query))\n",
    "    for doc, score in zip(docs, scores):\n",
    "        doc.metadata[\"score\"] = score\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "base_retriever = vectorstore.as_retriever(\n",
    "    search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.6}\n",
    ")\n",
    "model = HuggingFaceCrossEncoder(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=base_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_retriever.invoke('How many zip data are there?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_id': 5189,\n",
       " 'db_id': 'address',\n",
       " 'vt': 'SELECT DISTINCT zip_data.state FROM state INNER JOIN zip_data AS T2 ON T1.abbreviation = T2.state WHERE zip_data.female_population > (SELECT AVG(zip_data.female_population) FROM zip_data)'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(langchain_core.retrievers.BaseRetriever,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContextualCompressionRetriever.__bases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2091"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_path = proj_path / 'experiments' / 'bird' / 'predictions' / 'zero_shot'\n",
    "predictions = []\n",
    "for p in prediction_path.glob('bird_dev_*.json'):\n",
    "    with open(p) as f:\n",
    "        pred = json.load(f)\n",
    "        new_pred = []\n",
    "        for x in pred:\n",
    "            x.pop('rationale')\n",
    "            new_pred.append(x)\n",
    "        predictions.extend(new_pred)\n",
    "\n",
    "with open(prediction_path / 'final_bird_dev.jsonl', 'w') as f:\n",
    "    for p in predictions:\n",
    "        f.write(json.dumps(p) + '\\n')\n",
    "\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_id': 8773,\n",
       " 'db_id': 'food_inspection',\n",
       " 'gold_sql': \"SELECT COUNT(owner_state) FROM businesses WHERE owner_state = 'CA'\",\n",
       " 'pred_sql': \"SELECT COUNT(DISTINCT owner_name) AS owner_count FROM businesses WHERE owner_state = 'California';\"}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typ = 'test'\n",
    "# samples = test_samples\n",
    "# predictions = []\n",
    "# with open(prediction_path / f'final_bird_{typ}.jsonl', 'r') as f:\n",
    "#     preds = f.readlines()\n",
    "#     for p in preds:\n",
    "#         pred = json.loads(p)\n",
    "#         found = False\n",
    "#         for sample in samples:\n",
    "#             if sample.sample_id == pred['sample_id']:\n",
    "#                 pred['gold_sql'] = sample.final.sql\n",
    "#                 found = True\n",
    "#                 break\n",
    "#         if not found:\n",
    "#             raise ValueError(f\"Sample ID {pred['sample_id']} not found\")\n",
    "        \n",
    "#         predictions.append(pred)\n",
    "\n",
    "# with open(prediction_path / f'final_bird_{typ}.jsonl', 'w') as f:\n",
    "#     for p in predictions:\n",
    "#         f.write(json.dumps(p) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "import sqlglot.expressions as exp\n",
    "from src.eval_utils import result_eq, check_if_exists_orderby\n",
    "\n",
    "def get_pred_results(\n",
    "        proj_path: Path,\n",
    "        predictions: list[dict],\n",
    "        tables: dict[str, DatabaseModel],\n",
    "        ds: str = 'bird' # spider or bird\n",
    "    ):\n",
    "    output_results = []\n",
    "    error_infos = {\n",
    "        'pred_exec': [],\n",
    "        'gold_exec': [],\n",
    "        'python_script': [],\n",
    "        'result': []\n",
    "    }\n",
    "    predictions_by_db_id = defaultdict(list)\n",
    "    for pred in predictions:\n",
    "        predictions_by_db_id[pred['db_id']].append(pred)\n",
    "    \n",
    "    for db_id, preds in predictions_by_db_id.items():\n",
    "        schema = Schema(tables[db_id].db_schema)\n",
    "        if ds == 'bird':\n",
    "            try:\n",
    "                database = SqliteDatabase(\n",
    "                    db_file=str(proj_path / 'data' / ds / 'train' / 'train_databases' / db_id / f'{db_id}.sqlite'),\n",
    "                    foreign_keys=tables[db_id].foreign_keys\n",
    "                )\n",
    "            except:\n",
    "                database = SqliteDatabase(\n",
    "                    db_file=str(proj_path / 'data' / ds / 'dev' / 'dev_databases' / db_id / f'{db_id}.sqlite'),\n",
    "                    foreign_keys=tables[db_id].foreign_keys\n",
    "                )\n",
    "        else:\n",
    "            database = SqliteDatabase(\n",
    "                db_file=str(proj_path / 'data' / 'spider' / 'database' / db_id / f'{db_id}.sqlite'), \n",
    "                foreign_keys=tables[db_id].foreign_keys\n",
    "            )\n",
    "        iterator = tqdm(preds, total=len(preds))\n",
    "        for pred in iterator:\n",
    "            iterator.set_description(f'{db_id} | pred_exec: {len(error_infos[\"pred_exec\"])} | gold_exec: {len(error_infos[\"gold_exec\"])} | python_script: {len(error_infos[\"python_script\"])} | result: {len(error_infos[\"result\"])}')\n",
    "\n",
    "            pred_sql = pred['pred_sql'] \n",
    "            gold_sql = pred['gold_sql']\n",
    "            \n",
    "            error_info = ''\n",
    "            try:\n",
    "                pred_result = database.execute(pred_sql, rt_pandas=False)\n",
    "            except Exception as e:\n",
    "                pred_result = []\n",
    "                error_infos['pred_exec'].append((db_id, pred['sample_id']))\n",
    "                error_info = 'Predction Execution Error:' + str(e)\n",
    "                score = 0\n",
    "\n",
    "            try:\n",
    "                gold_result = database.execute(gold_sql, rt_pandas=False)\n",
    "            except Exception as e:\n",
    "                error_infos['gold_exec'].append((db_id, pred['sample_id']))\n",
    "                error_info = 'Gold Execution Error:' + str(e)\n",
    "            \n",
    "            if 'Gold Execution Error' in error_info:\n",
    "                continue\n",
    "            elif 'Predction Execution Error' in error_info:\n",
    "                output_results.append(\n",
    "                    {\n",
    "                        'sample_id': pred['sample_id'], \n",
    "                        'db_id': db_id,\n",
    "                        'score': score,\n",
    "                        'gold_sql': gold_sql,\n",
    "                        'pred_sql': pred_sql,\n",
    "                    }\n",
    "                )\n",
    "                continue\n",
    "            else:\n",
    "                exists_orderby = check_if_exists_orderby(gold_sql)\n",
    "                \n",
    "                try:\n",
    "                    score = int(result_eq(pred_result, gold_result, order_matters=exists_orderby))\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    score = 0\n",
    "                    error_info = 'Python Script Error:' + str(e)\n",
    "                    error_infos['python_script'].append((db_id, pred['sample_id']))\n",
    "\n",
    "                if score == 0 and error_info == '':\n",
    "                    error_info = 'Result not equal'\n",
    "                    error_infos['result'].append((db_id, pred['sample_id']))\n",
    "                output_results.append(\n",
    "                    {\n",
    "                        'sample_id': pred['sample_id'], \n",
    "                        'db_id': db_id,\n",
    "                        'score': score,\n",
    "                        'gold_sql': gold_sql,\n",
    "                        'pred_sql': pred_sql,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return output_results, error_infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_path = proj_path / 'experiments' / 'bird' / 'predictions' / 'zero_shot'\n",
    "\n",
    "def load_predictions(prediction_path: Path, filename: str):\n",
    "    predictions = []\n",
    "    with open(prediction_path / filename, 'r') as f:\n",
    "        preds = f.readlines()\n",
    "        for p in preds:\n",
    "            predictions.append(json.loads(p))\n",
    "    return predictions\n",
    "predictions = []\n",
    "with open(prediction_path / 'final_bird_dev.jsonl', 'r') as f:\n",
    "    preds = f.readlines()\n",
    "    for p in preds:\n",
    "        predictions.append(json.loads(p))\n",
    "\n",
    "# output_results, error_infos = get_pred_results(proj_path, predictions, tables, ds='bird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_evaluation import get_prediction_parsed_sql, get_target_parsed_sql\n",
    "from src.eval_utils import get_complexity, get_all_partial_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "california_schools: 100%|██████████| 3/3 [00:00<00:00, 456.88it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "california_schools: 100%|██████████| 3/3 [00:00<00:00, 444.52it/s]\n"
     ]
    }
   ],
   "source": [
    "target_parsed, error_ids = get_target_parsed_sql([x for x in dev_samples if x.sample_id in [9503, 9468, 9494]], tables=bird_tables)\n",
    "pred_parsed, error_ids = get_prediction_parsed_sql(predictions[:3], tables=bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_id = 'california_schools'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ID: 9503\n",
      "f1=0.7798 | 0.7657 | 0.8017\n",
      "Sample ID: 9468\n",
      "f1=0.4271 | 0.4170 | 0.5096\n"
     ]
    }
   ],
   "source": [
    "for pred in predictions[:3]:\n",
    "    sample_id = pred['sample_id']\n",
    "    target_o = target_parsed[db_id][sample_id]\n",
    "    pred_o = pred_parsed[db_id][sample_id]\n",
    "    if pred_o:\n",
    "        _, all_score = get_all_partial_score(pred_o, target_o)\n",
    "\n",
    "        print(f\"Sample ID: {sample_id}\")\n",
    "        print(f'f1={all_score[\"overall\"]} | {all_score[\"structural\"]:.4f} | {all_score[\"semantic\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9503: defaultdict(set,\n",
       "             {'aliases': {'table': {'T1': 'frpm', 'T2': 'schools'},\n",
       "               'column': {}},\n",
       "              'distinct': False,\n",
       "              'limit': True,\n",
       "              'table_asts': {('frpm',\n",
       "                Table(\n",
       "                  this=Table(\n",
       "                    this=Identifier(this=frpm, quoted=False))),\n",
       "                'from'),\n",
       "               ('schools',\n",
       "                Table(\n",
       "                  this=Table(\n",
       "                    this=Identifier(this=schools, quoted=False))),\n",
       "                'join')},\n",
       "              'sel': {'__frpm.low grade__',\n",
       "               '__frpm.school name__',\n",
       "               '__schools.city__'},\n",
       "              'sel_asts': {('__frpm.low grade__',\n",
       "                Column(\n",
       "                  this=Identifier(this=low grade, quoted=True),\n",
       "                  table=Identifier(this=frpm, quoted=False)),\n",
       "                '<select>'),\n",
       "               ('__frpm.school name__',\n",
       "                Column(\n",
       "                  this=Identifier(this=school name, quoted=True),\n",
       "                  table=Identifier(this=frpm, quoted=False)),\n",
       "                '<select>'),\n",
       "               ('__schools.city__',\n",
       "                Column(\n",
       "                  this=Identifier(this=city, quoted=False),\n",
       "                  table=Identifier(this=schools, quoted=False)),\n",
       "                '<select>')},\n",
       "              'cond_asts': {('__schools.state__ eq [placeholder-type:string]',\n",
       "                EQ(\n",
       "                  this=Column(\n",
       "                    this=Identifier(this=state, quoted=False),\n",
       "                    table=Identifier(this=schools, quoted=False)),\n",
       "                  expression=Literal(this=[placeholder-type:string], is_string=True)),\n",
       "                'eq')},\n",
       "              'op_types': {'eq'},\n",
       "              'agg': set(),\n",
       "              'agg_asts': set(),\n",
       "              'orderby': {'__schools.latitude__'},\n",
       "              'orderby_asts': {('__schools.latitude__',\n",
       "                Column(\n",
       "                  this=Identifier(this=latitude, quoted=False),\n",
       "                  table=Identifier(this=schools, quoted=False)),\n",
       "                '<select>')},\n",
       "              'nested': 1,\n",
       "              'subqueries': [Select(\n",
       "                 expressions=[\n",
       "                   Column(\n",
       "                     this=Identifier(this=city, quoted=False),\n",
       "                     table=Identifier(this=schools, quoted=False)),\n",
       "                   Column(\n",
       "                     this=Identifier(this=low grade, quoted=True),\n",
       "                     table=Identifier(this=frpm, quoted=False)),\n",
       "                   Column(\n",
       "                     this=Identifier(this=school name, quoted=True),\n",
       "                     table=Identifier(this=frpm, quoted=False))],\n",
       "                 limit=Limit(\n",
       "                   expression=Literal(this=1, is_string=False)),\n",
       "                 from=From(\n",
       "                   this=Table(\n",
       "                     this=Identifier(this=frpm, quoted=False))),\n",
       "                 joins=[\n",
       "                   Join(\n",
       "                     this=Table(\n",
       "                       this=Identifier(this=schools, quoted=False),\n",
       "                       alias=TableAlias(\n",
       "                         this=Identifier(this=T2, quoted=False))),\n",
       "                     kind=INNER,\n",
       "                     on=EQ(\n",
       "                       this=Column(\n",
       "                         this=Identifier(this=CDSCode, quoted=False),\n",
       "                         table=Identifier(this=T1, quoted=False)),\n",
       "                       expression=Column(\n",
       "                         this=Identifier(this=CDSCode, quoted=False),\n",
       "                         table=Identifier(this=T2, quoted=False))))],\n",
       "                 where=Where(\n",
       "                   this=EQ(\n",
       "                     this=Column(\n",
       "                       this=Identifier(this=state, quoted=False),\n",
       "                       table=Identifier(this=schools, quoted=False)),\n",
       "                     expression=Literal(this=CA, is_string=True))),\n",
       "                 order=Order(\n",
       "                   expressions=[\n",
       "                     Column(\n",
       "                       this=Identifier(this=latitude, quoted=False),\n",
       "                       table=Identifier(this=schools, quoted=False))]))]}),\n",
       " 9468: defaultdict(set,\n",
       "             {'aliases': {'table': {'T1': 'frpm', 'T2': 'schools'},\n",
       "               'column': {}},\n",
       "              'distinct': False,\n",
       "              'limit': True,\n",
       "              'table_asts': {('frpm',\n",
       "                Table(\n",
       "                  this=Table(\n",
       "                    this=Identifier(this=frpm, quoted=False))),\n",
       "                'from'),\n",
       "               ('schools',\n",
       "                Table(\n",
       "                  this=Table(\n",
       "                    this=Identifier(this=schools, quoted=False))),\n",
       "                'join')},\n",
       "              'sel': {'__schools.school__'},\n",
       "              'sel_asts': {('__schools.school__',\n",
       "                Column(\n",
       "                  this=Identifier(this=school, quoted=False),\n",
       "                  table=Identifier(this=schools, quoted=False)),\n",
       "                '<select>')},\n",
       "              'cond_asts': {('__schools.doc__ eq [placeholder-type:numeric]',\n",
       "                EQ(\n",
       "                  this=Column(\n",
       "                    this=Identifier(this=doc, quoted=False),\n",
       "                    table=Identifier(this=schools, quoted=False)),\n",
       "                  expression=Literal(this=[placeholder-type:numeric], is_string=False)),\n",
       "                'eq')},\n",
       "              'op_types': {'eq'},\n",
       "              'agg': set(),\n",
       "              'agg_asts': set(),\n",
       "              'orderby': {'__frpm.enrollment (k-12)__'},\n",
       "              'orderby_asts': {('__frpm.enrollment (k-12)__',\n",
       "                Column(\n",
       "                  this=Identifier(this=enrollment (k-12), quoted=True),\n",
       "                  table=Identifier(this=frpm, quoted=False)),\n",
       "                '<select>')},\n",
       "              'nested': 1,\n",
       "              'subqueries': [Select(\n",
       "                 expressions=[\n",
       "                   Column(\n",
       "                     this=Identifier(this=school, quoted=False),\n",
       "                     table=Identifier(this=schools, quoted=False))],\n",
       "                 limit=Limit(\n",
       "                   expression=Literal(this=1, is_string=False)),\n",
       "                 from=From(\n",
       "                   this=Table(\n",
       "                     this=Identifier(this=frpm, quoted=False))),\n",
       "                 joins=[\n",
       "                   Join(\n",
       "                     this=Table(\n",
       "                       this=Identifier(this=schools, quoted=False),\n",
       "                       alias=TableAlias(\n",
       "                         this=Identifier(this=T2, quoted=False))),\n",
       "                     kind=INNER,\n",
       "                     on=EQ(\n",
       "                       this=Column(\n",
       "                         this=Identifier(this=CDSCode, quoted=False),\n",
       "                         table=Identifier(this=T1, quoted=False)),\n",
       "                       expression=Column(\n",
       "                         this=Identifier(this=CDSCode, quoted=False),\n",
       "                         table=Identifier(this=T2, quoted=False))))],\n",
       "                 where=Where(\n",
       "                   this=EQ(\n",
       "                     this=Column(\n",
       "                       this=Identifier(this=doc, quoted=False),\n",
       "                       table=Identifier(this=schools, quoted=False)),\n",
       "                     expression=Literal(this=31, is_string=False))),\n",
       "                 order=Order(\n",
       "                   expressions=[\n",
       "                     Column(\n",
       "                       this=Identifier(this=enrollment (k-12), quoted=True),\n",
       "                       table=Identifier(this=frpm, quoted=False))]))]}),\n",
       " 9494: defaultdict(set,\n",
       "             {'aliases': {'table': {'T1': 'frpm', 'T2': 'schools'},\n",
       "               'column': {}},\n",
       "              'distinct': False,\n",
       "              'limit': False,\n",
       "              'table_asts': {('frpm',\n",
       "                Table(\n",
       "                  this=Table(\n",
       "                    this=Identifier(this=frpm, quoted=False))),\n",
       "                'from'),\n",
       "               ('schools',\n",
       "                Table(\n",
       "                  this=Table(\n",
       "                    this=Identifier(this=schools, quoted=False))),\n",
       "                'join')},\n",
       "              'sel': {'__frpm.enrollment (ages 5-17)__'},\n",
       "              'sel_asts': {('__frpm.enrollment (ages 5-17)__',\n",
       "                Column(\n",
       "                  this=Identifier(this=enrollment (ages 5-17), quoted=True),\n",
       "                  table=Identifier(this=frpm, quoted=False)),\n",
       "                '<select>')},\n",
       "              'cond_asts': {('__frpm.academic year__ between [placeholder-type:numeric] and [placeholder-type:numeric]',\n",
       "                Between(\n",
       "                  this=Column(\n",
       "                    this=Identifier(this=academic year, quoted=True),\n",
       "                    table=Identifier(this=frpm, quoted=False)),\n",
       "                  low=Literal(this=[placeholder-type:numeric], is_string=False),\n",
       "                  high=Literal(this=[placeholder-type:numeric], is_string=False)),\n",
       "                'between'),\n",
       "               ('__schools.city__ eq [placeholder-type:string]',\n",
       "                EQ(\n",
       "                  this=Column(\n",
       "                    this=Identifier(this=city, quoted=False),\n",
       "                    table=Identifier(this=schools, quoted=False)),\n",
       "                  expression=Literal(this=[placeholder-type:string], is_string=True)),\n",
       "                'eq'),\n",
       "               ('__schools.edopscode__ eq [placeholder-type:string]',\n",
       "                EQ(\n",
       "                  this=Column(\n",
       "                    this=Identifier(this=edopscode, quoted=False),\n",
       "                    table=Identifier(this=schools, quoted=False)),\n",
       "                  expression=Literal(this=[placeholder-type:string], is_string=True)),\n",
       "                'eq')},\n",
       "              'op_types': {'between', 'eq'},\n",
       "              'agg': set(),\n",
       "              'agg_asts': set(),\n",
       "              'orderby': set(),\n",
       "              'orderby_asts': set(),\n",
       "              'nested': 1,\n",
       "              'subqueries': [Select(\n",
       "                 expressions=[\n",
       "                   Column(\n",
       "                     this=Identifier(this=enrollment (ages 5-17), quoted=True),\n",
       "                     table=Identifier(this=frpm, quoted=False))],\n",
       "                 from=From(\n",
       "                   this=Table(\n",
       "                     this=Identifier(this=frpm, quoted=False))),\n",
       "                 joins=[\n",
       "                   Join(\n",
       "                     this=Table(\n",
       "                       this=Identifier(this=schools, quoted=False),\n",
       "                       alias=TableAlias(\n",
       "                         this=Identifier(this=T2, quoted=False))),\n",
       "                     kind=INNER,\n",
       "                     on=EQ(\n",
       "                       this=Column(\n",
       "                         this=Identifier(this=CDSCode, quoted=False),\n",
       "                         table=Identifier(this=T1, quoted=False)),\n",
       "                       expression=Column(\n",
       "                         this=Identifier(this=CDSCode, quoted=False),\n",
       "                         table=Identifier(this=T2, quoted=False))))],\n",
       "                 where=Where(\n",
       "                   this=And(\n",
       "                     this=And(\n",
       "                       this=EQ(\n",
       "                         this=Column(\n",
       "                           this=Identifier(this=edopscode, quoted=False),\n",
       "                           table=Identifier(this=schools, quoted=False)),\n",
       "                         expression=Literal(this=SSS, is_string=True)),\n",
       "                       expression=EQ(\n",
       "                         this=Column(\n",
       "                           this=Identifier(this=city, quoted=False),\n",
       "                           table=Identifier(this=schools, quoted=False)),\n",
       "                         expression=Literal(this=Fremont, is_string=True))),\n",
       "                     expression=Between(\n",
       "                       this=Column(\n",
       "                         this=Identifier(this=academic year, quoted=True),\n",
       "                         table=Identifier(this=frpm, quoted=False)),\n",
       "                       low=Literal(this=2014, is_string=False),\n",
       "                       high=Literal(this=2015, is_string=False)))))]})}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_parsed[db_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_id': 9494,\n",
       " 'rationale': ['Identify the relevant table: frpm contains the enrollment data.',\n",
       "  \"Determine the columns needed: 'enrollment (ages 5-17)' for the number of students, 'academic year' to filter by the specific year, and 'district type' to specify the type of school.\",\n",
       "  'Filter the records for the academic year 2014-2015.',\n",
       "  \"Filter for the district type 'State Special Schools'.\",\n",
       "  \"Filter for the county name 'Fremont'.\",\n",
       "  'Use SUM() to aggregate the total enrollment for the specified filters.'],\n",
       " 'pred_sql': \"SELECT SUM(enrollment (ages 5-17)) AS total_enrollment\\nFROM frpm\\nWHERE academic year = '2014-2015' AND district type = 'State Special Schools' AND county name = 'Fremont';\",\n",
       " 'db_id': 'california_schools',\n",
       " 'gold_sql': 'SELECT T1.\"Enrollment (Ages 5-17)\" FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.EdOpsCode = \\'SSS\\' AND T2.City = \\'Fremont\\' AND T1.\"Academic Year\" BETWEEN 2014 AND 2015'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "california_schools: 100%|██████████| 1/1 [00:00<00:00, 465.31it/s]\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT SUM(\"enrollment (ages 5-17)\") AS total_enrollment\n",
    "FROM frpm\n",
    "WHERE \"academic year\" = '2014-2015' AND \"district type\" = 'State Special Schools' AND \"county name\" = 'Fremont';\n",
    "\"\"\"\n",
    "x = {'sample_id': 9494,\n",
    " 'rationale': ['Identify the relevant table: frpm contains the enrollment data.',\n",
    "  \"Determine the columns needed: 'enrollment (ages 5-17)' for the number of students, 'academic year' to filter by the specific year, and 'district type' to specify the type of school.\",\n",
    "  'Filter the records for the academic year 2014-2015.',\n",
    "  \"Filter for the district type 'State Special Schools'.\",\n",
    "  \"Filter for the county name 'Fremont'.\",\n",
    "  'Use SUM() to aggregate the total enrollment for the specified filters.'],\n",
    " 'pred_sql': sql,\n",
    " 'db_id': 'california_schools',\n",
    " 'gold_sql': 'SELECT T1.\"Enrollment (Ages 5-17)\" FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.EdOpsCode = \\'SSS\\' AND T2.City = \\'Fremont\\' AND T1.\"Academic Year\" BETWEEN 2014 AND 2015'}\n",
    "\n",
    "parsed, error_ids = get_prediction_parsed_sql([x], tables=bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_id = 'california_schools'\n",
    "tables = bird_tables\n",
    "db_schema = get_schema_str(\n",
    "    schema=tables[db_id].db_schema, \n",
    "    foreign_keys=tables[db_id].foreign_keys,\n",
    "    col_explanation=tables[db_id].col_explanation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = SqliteDatabase(\n",
    "    db_file=str(proj_path / 'data' / 'bird' / 'dev' / 'dev_databases' / 'california_schools' / 'california_schools.sqlite'),\n",
    "    foreign_keys=bird_tables['california_schools'].foreign_keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Enrollment (Ages 5-17)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1070.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Enrollment (Ages 5-17)\n",
       "0                  1070.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.execute(\"\"\"\n",
    "SELECT \"enrollment (ages 5-17)\"\n",
    "FROM frpm\n",
    "LIMIT 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('california_schools',\n",
       "  9494,\n",
       "  'Invalid expression / Unexpected token. Line 4, Col: 19.\\n  \\nSELECT SUM(\"enrollment (ages 5-17)\") AS total_enrollment\\nFROM frpm\\nWHERE academic \\x1b[4myear\\x1b[0m = \\'2014-2015\\' AND district type = \\'State Special Schools\\' AND county name = \\'Fremont\\';\\n')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_evaluation import get_parsed_sql\n",
    "\n",
    "parsed_path = proj_path / 'data' / 'pkl_files' \n",
    "file_name = f'bird_dev_parsed_pred.pkl'\n",
    "if not (parsed_path / file_name).exists():\n",
    "    pred_parsed, error_ids = get_parsed_sql(predictions, tables)\n",
    "    with open(parsed_path / file_name, 'wb') as f:\n",
    "        pickle.dump(pred_parsed, f)\n",
    "    print(f'Error parsing pred bird_dev: {len(error_ids)}')\n",
    "\n",
    "with (parsed_path / file_name).open('rb') as f:\n",
    "    pred_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create VT, BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "from collections import defaultdict\n",
    "from src.parsing_sql import (\n",
    "    Schema, _format_expression, extract_aliases\n",
    ")\n",
    "from run_bo_sql import create_vt_ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:32<00:00,  3.20s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=Prompts.bo_description,\n",
    "    input_variables=['schema', 'virtual_table']\n",
    ")\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0,\n",
    "    frequency_penalty=0.1,\n",
    ")\n",
    "model = model_openai.with_structured_output(BODescription)\n",
    "chain = (prompt | model)\n",
    "res = create_vt_ba(samples=train_samples[:10], tables=bird_tables, chain=chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(bos: dict[str, list[dict[str, str]]]):\n",
    "    documents = []\n",
    "    for db_id, samples in bos.items():\n",
    "        for x in samples:\n",
    "            doc = Document(\n",
    "                doc_id=x['sample_id'],\n",
    "                page_content=x['ba'],\n",
    "                metadata={\n",
    "                    'sample_id': x['sample_id'],\n",
    "                    'db_id': db_id,\n",
    "                    'vt': x['vt']\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents, \n",
    "        embedding = embeddings_model,\n",
    "    )\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = get_vector_store(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sql_bo(\n",
    "    to_pred_samples: list[SpiderSample|BirdSample],\n",
    "    tables: dict[DatabaseModel],\n",
    "    vectorstore: FAISS,\n",
    "    chain: RunnableSequence,\n",
    "    prediction_path: Path,\n",
    "    file_name: str = '[args.ds]_[args.type]',\n",
    "    n_retrieval: int = 3,\n",
    "    score_threshold: float = 0.65,\n",
    "):\n",
    "    processed_db_ids = [p.stem.split('_')[-1] for p in prediction_path.glob(f'{file_name}_*')]\n",
    "    # restart from checkpoint\n",
    "    if processed_db_ids:\n",
    "        to_pred_samples = [sample for sample in to_pred_samples if sample.db_id not in processed_db_ids]\n",
    "    \n",
    "    samples_by_db_id = defaultdict(list)\n",
    "    for sample in to_pred_samples:\n",
    "        samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "    for db_id, samples in samples_by_db_id.items():\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_kwargs={'k': n_retrieval, 'score_threshold': score_threshold, 'filter': {'db_id': db_id}}\n",
    "        )\n",
    "        schema_str = get_schema_str(\n",
    "            schema=tables[db_id].db_schema, \n",
    "            foreign_keys=tables[db_id].foreign_keys,\n",
    "            col_explanation=tables[db_id].col_explanation\n",
    "        )\n",
    "        results = []\n",
    "        for sample in tqdm(samples, total=len(samples), desc=f\"{db_id}\"):\n",
    "            question = sample.final.question\n",
    "            docs = retriever.invoke(question)\n",
    "            hint = '\\nDescriptions and Virtual Tables:\\n'\n",
    "            hint += json.dumps({j: {'description': doc.page_content, 'virtual_table': doc.metadata['vt']} for j, doc in enumerate(docs)}, indent=4)\n",
    "            hint += '\\n'\n",
    "            input_data = {'schema': schema_str, 'input_query': question, 'hint': hint}\n",
    "            output = chain.invoke(input=input_data)\n",
    "            \n",
    "            full_sql_output = {}\n",
    "            full_sql_output['sample_id'] = sample.sample_id\n",
    "            full_sql_output['rationale'] = output.rationale\n",
    "            full_sql_output['pred_sql'] = output.full_sql_query\n",
    "            # full_sql_output = 1\n",
    "            results.append(full_sql_output)\n",
    "\n",
    "        with open(prediction_path / f'{file_name}_{db_id}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "movie_platform: 100%|██████████| 10/10 [00:04<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_bo.json', 'r') as f:\n",
    "#     bos = json.load(res, f, indent=4)\n",
    "# vectorstore = get_vector_store(bos)\n",
    "\n",
    "\n",
    "data_path = proj_path / 'data' / 'bird'\n",
    "experiment_folder = proj_path / 'experiments' / 'bird'\n",
    "prediction_path = experiment_folder / 'predictions' / 'zero_shot_hint'\n",
    "eval_path = experiment_folder / 'evals'\n",
    "for p in [prediction_path, eval_path]:\n",
    "    if not p.exists():\n",
    "        p.mkdir(parents=True)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=Prompts.zero_shot_hints_inference,\n",
    "    input_variables=['schema', 'input_query', 'hint'],\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0,\n",
    "    frequency_penalty=0.1,\n",
    ")\n",
    "\n",
    "model = model_openai.with_structured_output(SQLResponse)\n",
    "chain = (prompt | model)\n",
    "\n",
    "n_retrieval = 3\n",
    "score_threshold = 0.65\n",
    "\n",
    "predict_sql_bo(\n",
    "    to_pred_samples=dev_samples[:10],\n",
    "    tables=bird_tables,\n",
    "    vectorstore=vectorstore,\n",
    "    chain=chain,\n",
    "    prediction_path=prediction_path,\n",
    "    n_retrieval=n_retrieval,\n",
    "    score_threshold=score_threshold,\n",
    "    file_name='bird_dev',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptions and Virtual Tables:\n",
      "{\n",
      "    \"0\": {\n",
      "        \"description\": \"The virtual table retrieves the titles of movies that have been rated, filtering by a specific rating timestamp and grouping the results by movie title. The results are ordered by the count of ratings for each movie title, and a limit is applied to restrict the number of returned titles.\",\n",
      "        \"virtual_table\": \"SELECT movies.movie_title FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE ratings.rating_timestamp_utc LIKE '[placeholder-type:string]' GROUP BY movies.movie_title ORDER BY COUNT(movies.movie_title) LIMIT [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"1\": {\n",
      "        \"description\": \"The virtual table provides a count of users who have rated a specific movie, identified by its title, while also filtering for users who were trialists at the time of rating. It combines data from the 'ratings' and 'movies' tables to achieve this.\",\n",
      "        \"virtual_table\": \"SELECT COUNT(ratings.user_id) FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE movies.movie_title = '[placeholder-type:string]' AND ratings.user_trialist = [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"description\": \"The virtual table retrieves the titles of movies that have been rated, ordered by the number of likes received on the critics' comments. The query joins the 'ratings' table with the 'movies' table to access the movie titles associated with each rating. The result is limited to a specified number of entries, allowing users to see the most liked critics' comments for rated movies.\",\n",
      "        \"virtual_table\": \"SELECT movies.movie_title FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id ORDER BY ratings.critic_likes LIMIT [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"description\": \"The virtual table displays the titles of movies from the 'movies' table that have received a certain number of likes on user-written critiques. The query joins the 'ratings' table with the 'movies' table to filter movies based on the number of likes their critiques have received, using a placeholder for the minimum number of likes.\",\n",
      "        \"virtual_table\": \"SELECT movies.movie_title FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE ratings.critic_likes > [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"description\": \"The virtual table counts the number of ratings for a specific movie title from the 'movies' table, filtering by the movie's title and a specified rating timestamp. The placeholders represent the movie title and the date from which to count ratings.\",\n",
      "        \"virtual_table\": \"SELECT COUNT(ratings.rating_id) FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE movies.movie_title = '[placeholder-type:string]' AND ratings.rating_timestamp_utc >= '[placeholder-type:string]'\"\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(sample.final.question)\n",
    "hint = '\\nDescriptions and Virtual Tables:\\n'\n",
    "hint += json.dumps({j: {'description': doc.page_content, 'virtual_table': doc.metadata['vt']} for j, doc in enumerate(docs)}, indent=4)\n",
    "hint += '\\n'\n",
    "input_data = {'schema': db_schema, 'input_query': row['question'], 'hint': hint}\n",
    "output = chain.invoke(input=input_data)\n",
    "\n",
    "print(hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "movie_platform:   0%|          | 0/6341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debit_card_specializing: 100%|██████████| 6341/6341 [00:21<00:00, 299.00it/s]    \n",
      "debit_card_specializing: 100%|██████████| 2091/2091 [00:05<00:00, 408.98it/s]    \n",
      "debit_card_specializing: 100%|██████████| 2193/2193 [00:09<00:00, 225.64it/s]    \n"
     ]
    }
   ],
   "source": [
    "def get_parsed_sql(samples: dict, tables: dict):\n",
    "    error_ids = []\n",
    "    parsed = defaultdict(dict)\n",
    "    iterator = tqdm(samples, total=len(samples))\n",
    "    for sample in iterator:\n",
    "        db_id = sample.db_id\n",
    "        sample_id = sample.sample_id\n",
    "        iterator.set_description(f\"{db_id}\")\n",
    "        schema = Schema(tables[db_id].db_schema)\n",
    "        sql_i = sample.final.sql\n",
    "        try:\n",
    "            ei = extract_all(sql_i, schema)\n",
    "            assert len(ei['sel']) > 0, f'No selection found-{db_id}-{sample_id}'\n",
    "        except Exception as e:\n",
    "            error_ids.append((db_id, sample_id, str(e)))\n",
    "            parsed[db_id].append(None)\n",
    "            continue\n",
    "        parsed[db_id][sample_id] = ei\n",
    "    return parsed, error_ids\n",
    "\n",
    "train_parsed, error_ids = get_parsed_sql(train_samples, bird_tables)\n",
    "dev_parsed, error_ids = get_parsed_sql(dev_samples, bird_tables)\n",
    "test_parsed, error_ids = get_parsed_sql(test_samples, bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_parsed, f)\n",
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(dev_parsed, f)\n",
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_test_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_parsed, f)\n",
    "\n",
    "with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'rb') as f:\n",
    "    dev_parsed = pickle.load(f)\n",
    "\n",
    "with open(proj_path / 'data' / 'pkl_files' / 'bird_test_parsed.pkl', 'rb') as f:\n",
    "    test_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, product\n",
    "from collections import defaultdict\n",
    "from src.eval_utils import get_all_partial_score\n",
    "\n",
    "def measure_inter_score(parsed1: dict[str, tuple], parsed2: dict[str, tuple]):\n",
    "    results = defaultdict()\n",
    "    assert len(parsed1) == len(parsed2), f\"Length mismatch-1: {len(parsed1)} 2:{len(parsed2)}\"\n",
    "    db_ids = list(parsed1.keys())\n",
    "    for db_id in db_ids:\n",
    "        o1 = parsed1[db_id]\n",
    "        o2 = parsed2[db_id]\n",
    "        n1 = len(o1)\n",
    "        n2 = len(o2)\n",
    "        semantic_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "        structural_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "        overall_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "\n",
    "        idxs = list(product(range(n1), range(n2)))\n",
    "        iterator = tqdm(idxs, total=len(idxs), desc=f\"{db_id}\")\n",
    "        for i, j in iterator:\n",
    "            ei = o1[i]\n",
    "            ej = o2[j]\n",
    "\n",
    "            _, final_score = get_all_partial_score(ei, ej, use_bert=True)\n",
    "\n",
    "            structural_sim[i, j] = final_score['structural']\n",
    "            semantic_sim[i, j] = final_score['semantic']\n",
    "            overall_sim[i, j] = final_score['overall']\n",
    "\n",
    "        results[db_id] = {\n",
    "            'semantic': semantic_sim,\n",
    "            'struct': structural_sim,\n",
    "            'overall': overall_sim\n",
    "        }\n",
    "    return results\n",
    "\n",
    "results = measure_inter_score(dev_parsed, test_parsed)\n",
    "with (proj_path / 'data' / 'pkl_files' / 'bird_dev_test_similarity.pkl').open('wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity between datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6341/6341 [00:10<00:00, 631.17it/s]\n",
      "100%|██████████| 2091/2091 [00:03<00:00, 589.55it/s]\n",
      "100%|██████████| 2193/2193 [00:03<00:00, 591.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def measure_complexity(samples, tables):\n",
    "    cs = []\n",
    "    for s in tqdm(samples, total=len(samples)):\n",
    "        schema = Schema(tables[s.db_id].db_schema)\n",
    "        output = extract_all(s.final.sql, schema)\n",
    "        complexity = get_complexity(output)\n",
    "        cs.append(complexity)\n",
    "    return cs\n",
    "\n",
    "train_complexities = measure_complexity(train_samples, bird_tables)\n",
    "dev_complexities = measure_complexity(dev_samples, bird_tables)\n",
    "test_complexities = measure_complexity(test_samples, bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Mean=0.2753 +/-0.0476, Median=0.2710\n",
      "[dev  ] Mean=0.2758 +/-0.0471, Median=0.2710\n",
      "[test ] Mean=0.2760 +/-0.0477, Median=0.2709\n"
     ]
    }
   ],
   "source": [
    "for c, n in zip([train_complexities, dev_complexities, test_complexities], ['train', 'dev  ', 'test ']):\n",
    "    print(f'[{n}] Mean={np.mean(c):.4f} +/-{np.std(c):.4f}, Median={np.median(c):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = defaultdict(list)\n",
    "for s in dev_samples:\n",
    "    stats[s.db_id].append(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
