{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "proj_path = Path('.').resolve()\n",
    "sys.path.append(str(proj_path))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from src.db_utils import get_schema_str, get_data_dict\n",
    "from src.pymodels import (\n",
    "    DatabaseModel, \n",
    "    SpiderSample, \n",
    "    BirdSample, \n",
    "    BODescription,\n",
    "    SQLResponse\n",
    ")\n",
    "from src.prompts import Prompts\n",
    "from src.database import SqliteDatabase\n",
    "from src.data_preprocess import (\n",
    "    load_raw_data,\n",
    "    process_all_tables,\n",
    "    filter_samples_by_count_spider_bird,\n",
    "    process_samples_bird,\n",
    "    split_train_dev_test,\n",
    "    save_samples_spider_bird,\n",
    "    load_samples_spider_bird,\n",
    ")\n",
    "\n",
    "from src.parsing_sql import Schema, extract_all\n",
    "from src.eval_utils import get_complexity, result_eq, check_if_exists_orderby\n",
    "from run_bo_sql import get_vector_store\n",
    "from copy import deepcopy\n",
    "\n",
    "from src.eval_utils import get_structural_score, get_all_structural_score, get_all_semantic_score, partial_matching_with_penalty\n",
    "from run_evaluation import get_target_parsed_sql, get_prediction_parsed_sql\n",
    "from run_bo_sql import _get_categories, _format_interval, get_retriever\n",
    "from bert_score import score as bscore\n",
    "from transformers import logging as tfloggings\n",
    "tfloggings.set_verbosity_error()\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.globals import set_llm_cache\n",
    "# from langchain_community.cache import SQLiteCache\n",
    "# set_llm_cache(SQLiteCache(database_path=f\"./cache/valid_bo_bird_dev.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'soccer_2016-0'\n",
    "db = SqliteDatabase(f\"./cache/valid_bo_{file_name}.db\")\n",
    "db.start()\n",
    "c = db.con.cursor()\n",
    "c.execute('BEGIN TRANSACTION')\n",
    "c.execute(\"\"\"\n",
    "DELETE FROM full_llm_cache WHERE response LIKE '%JSONDecodeError%';\n",
    "\"\"\")\n",
    "# c.execute('''\n",
    "# DELETE FROM full_llm_cache\n",
    "# WHERE prompt LIKE '%Which teams have had a player awarded the Purple Cap and another with the Orange Cap%'\n",
    "# ''')\n",
    "db.con.commit()\n",
    "db.close()\n",
    "# JSONDecodeError\n",
    "# ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>llm</th>\n",
       "      <th>idx</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt, llm, idx, response]\n",
       "Index: []"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "df = db.execute(\n",
    "'''\n",
    "SELECT * FROM full_llm_cache\n",
    "WHERE prompt LIKE '%Which teams have had a player awarded the Purple Cap and another with the Orange Cap%'\n",
    "''')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval_utils import get_structural_score, get_all_structural_score, get_all_semantic_score, partial_matching_with_penalty\n",
    "from run_evaluation import get_target_parsed_sql, get_prediction_parsed_sql\n",
    "from run_bo_sql import _get_categories, _format_interval, get_retriever\n",
    "from bert_score import score as bscore\n",
    "from transformers import logging as tfloggings\n",
    "tfloggings.set_verbosity_error()\n",
    "import warnings\n",
    "\n",
    "ds = 'spider'\n",
    "task = 'zero_shot_hint'\n",
    "typ = 'test'\n",
    "scenario = 0\n",
    "description_file = f'description.json' if ds == 'spider' else f'{ds}_description.json'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bird_path = proj_path / 'data' / 'bird'\n",
    "# tables, train_data, dev_data = load_raw_data(bird_path, load_test=False)\n",
    "\n",
    "# with (proj_path / 'data' / 'bird_description.json').open() as f:\n",
    "#     all_descriptions = json.load(f)\n",
    "\n",
    "# bird_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "# train_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_train.json')\n",
    "# dev_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_dev.json')\n",
    "# test_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_post_fix = f'{ds}_{typ}' if scenario < 0 else f'{ds}_{typ}_{scenario}'\n",
    "# final_file = f'final_{file_post_fix}.json'\n",
    "# samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_{typ}.json')\n",
    "\n",
    "# if not (prediction_path / final_file).exists():\n",
    "#     all_results = []\n",
    "#     paths = sorted(list(prediction_path.glob(f'{file_post_fix}_*.json')))\n",
    "#     for p in paths:\n",
    "#         with p.open() as f:\n",
    "#             results = json.load(f)\n",
    "            \n",
    "#         for r in results:\n",
    "#             r.pop('rationale')\n",
    "#             r['db_id'] = p.stem.split('_', 3)[-1]\n",
    "\n",
    "#             found = False\n",
    "#             for s in samples:\n",
    "#                 if r['sample_id'] == s.sample_id:\n",
    "#                     found = True\n",
    "#                     break\n",
    "#             r['gold_sql'] = s.final.sql\n",
    "#             assert found, r['sample_id']\n",
    "\n",
    "#         all_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(prediction_path / final_file, 'r') as f:\n",
    "#     preds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wta_1: 100%|██████████| 1508/1508 [00:02<00:00, 548.94it/s]                         \n"
     ]
    }
   ],
   "source": [
    "# pred_parsed, _ = get_prediction_parsed_sql(preds, tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'bird'\n",
    "typ = 'dev'\n",
    "task = 'retrieval' # 'zero_shot', 'retrieval', 'valid_bo'\n",
    "description_file = f'description.json' if ds == 'spider' else f'{ds}_description.json'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.json', 'r') as f:\n",
    "#     train_bo = json.load(f)\n",
    "\n",
    "# df = []\n",
    "# for db_id, xs in train_bo.items():\n",
    "#     for x in xs:\n",
    "#         x['db_id'] = db_id\n",
    "#         df.append(x)\n",
    "\n",
    "# df = pd.DataFrame(df)\n",
    "# cates = pd.qcut(df['gold_complexity'], q=5)\n",
    "# df['gold_complexity_cates'] = cates\n",
    "# df['gold_complexity_codes'] = cates.cat.codes\n",
    "# df.to_csv(experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.csv', index=False)\n",
    "# df = pd.read_csv(experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.csv')\n",
    "\n",
    "# df_train = df.groupby('gold_complexity_codes').sample(frac=0.9, random_state=42)\n",
    "# df_dev = df.drop(df_train.index)\n",
    "\n",
    "# print(df_train['gold_complexity_codes'].value_counts().sort_index())\n",
    "# print(df_dev['gold_complexity_codes'].value_counts().sort_index())\n",
    "\n",
    "# df_train.to_csv(experiment_folder / 'predictions' / 'create_bo' / f'{ds}_{task}_trainset.csv', index=False)\n",
    "# df_dev.to_csv(experiment_folder / 'predictions' / 'create_bo' / f'{ds}_{task}_devset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create hard negative sample pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.json', 'r') as f:\n",
    "    train_bo = json.load(f)\n",
    "# create dataframe\n",
    "df = []\n",
    "for db_id, xs in train_bo.items():\n",
    "    for x in xs:\n",
    "        x['db_id'] = db_id\n",
    "        df.append(x)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "cates = pd.qcut(df['gold_complexity'], q=5)\n",
    "df['gold_complexity_cates'] = cates\n",
    "df['gold_complexity_codes'] = cates.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_folder / 'predictions' / 'create_bo' / f'data_retrieval_rerank.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import (\n",
    "    SentenceTransformer, \n",
    "    SentenceTransformerTrainer, \n",
    "    SentenceTransformerTrainingArguments, \n",
    "    losses,\n",
    "    CrossEncoder,\n",
    "    InputExample\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator, RerankingEvaluator, InformationRetrievalEvaluator\n",
    "from sentence_transformers.cross_encoder.evaluation import CERerankingEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "import random \n",
    "from itertools import product\n",
    "import uuid\n",
    "\n",
    "class BADataset():\n",
    "    def __init__(\n",
    "            self, \n",
    "            data: dict[str, dict[str, str|dict[str, int]]],\n",
    "            bert_scores: dict[str, float],\n",
    "            task: str, \n",
    "            n_neg: int=1,\n",
    "            threshold: float=0.1,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        samples: dict[str, dict[int|str, str]] = {\n",
    "            'question': {sample_id: question, ...},\n",
    "            'ba': {sample_id: ba, ...},\n",
    "            'gold_complexity_codes': {sample_id: gold_complexity_codes, ...}\n",
    "        }\n",
    "        bert_scores: dict[str, float] = {\n",
    "            'sample_id1-sample_id2': score, ...  # intra-db sample pairs\n",
    "        }\n",
    "\n",
    "        # task: retrieval\n",
    "        Instead of using MultipleNegativesRankingLoss we use TripletLoss, since most samples are similar. \n",
    "        So we need to design triplets that negative samples are from \n",
    "        1. same databases: harder to train (avoid ba is too similar to positive with certain threshold)\n",
    "        2. different databases: easier to train\n",
    "\n",
    "        construct: (anchor, positive, negative) \n",
    "        - anchor: question\n",
    "        - positive: ba\n",
    "        - negative: ba (`n_neg` from same database and `n_neg` from different database)\n",
    "        sample number: `n_neg` (same database) + `n_neg` (different database)\n",
    "        \n",
    "        # task: rerank\n",
    "        We train a binary classifier to predict whether the question and ba are relevant (share the complexity code) or not.\n",
    "        If we do random sampling, it will be too easy for the model to predict the label.\n",
    "        So we need sample negative samples from:\n",
    "        1. same database: harder to train\n",
    "        2. different database: easier to train\n",
    "\n",
    "        construct: (text, label)\n",
    "        - text: question, ba\n",
    "        - label: gold_complexity_codes\n",
    "        sample number: 5 (1 positive, `n_neg` negative)\n",
    "        - positive: 1 query, ba (target code, db_id)\n",
    "        - `n_neg` negative: from same db, different code to the target code (if impossible for a certain code, try to replace with other codes)\n",
    "        - `n_neg` negative: from different db, different code to the target code\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.bert_scores: dict[str, float] = bert_scores\n",
    "        # if threshold < 0 and task == 'retrieval':\n",
    "        #     self.use_adaptive_threshold = True\n",
    "        #     self.stats: dict[str, float] = self._check_stats_bert_score()['raw']\n",
    "        # else:\n",
    "        #     self.use_adaptive_threshold = False\n",
    "        self.threshold = threshold\n",
    "    \n",
    "        self.n_neg = n_neg\n",
    "        self._reorganize_samples(data)\n",
    "        # self.samples = self.create_samples_by_task()\n",
    "\n",
    "    def save_samples(self, sample_save_path):\n",
    "        with open(sample_save_path, 'w') as f:\n",
    "            json.dump(self.samples, f)\n",
    "\n",
    "    def load_samples(self, sample_save_path):\n",
    "        with open(sample_save_path, 'r') as f:\n",
    "            self.samples = json.load(f)\n",
    "\n",
    "    def _reorganize_samples(self, samples: dict[str, dict[int|str, str]]):\n",
    "        # sid == sample_id, qid is from 0 to len(samples['question'])\n",
    "        # [sid1, sid2, ...] -- index of this list is qid\n",
    "        self.sample_ids = list(samples['question'].keys())\n",
    "        \n",
    "        # question: {qid: question}\n",
    "        # ba: {sid: ba}\n",
    "        # db_id: {qid: db_id}\n",
    "        # db_id2qids: {db_id: [qid1, qid2, ...]}\n",
    "        # codes: {qid: gold_complexity_codes}\n",
    "        # codes2qids: {db_id: {gold_complexity_codes: [qid1, qid2, ...]}}\n",
    "        self.questions = {}\n",
    "        self.bas = {}\n",
    "        self.db_ids = {}\n",
    "        self.db_id2qids: dict[str, list[int]] = defaultdict(list)\n",
    "        self.codes = {}\n",
    "        self.codes2qids: dict[str, dict[int, list[int]]] = defaultdict(dict)\n",
    "\n",
    "        for qid, sid in enumerate(self.sample_ids):\n",
    "            self.questions[qid] = samples['question'][sid]\n",
    "            self.bas[sid] = samples['ba'][sid]\n",
    "            \n",
    "            db_id = samples['db_id'][sid]\n",
    "            self.db_ids[qid] = db_id\n",
    "            self.db_id2qids[db_id].append(qid)\n",
    "\n",
    "            code = samples['gold_complexity_codes'][sid]\n",
    "            self.codes[qid] = code\n",
    "            if code not in self.codes2qids[db_id]:\n",
    "                self.codes2qids[db_id][code] = []\n",
    "            self.codes2qids[db_id][code].append(qid)\n",
    "\n",
    "        self.n_unique_codes = len(set(self.codes.values()))\n",
    "\n",
    "    def _check_stats_bert_score(self):\n",
    "        dist = defaultdict(list)\n",
    "        unique_keys = defaultdict(set)\n",
    "        for key in self.bert_scores:\n",
    "            k1, k2 = key.split('-')\n",
    "            unique_keys[k1].add(k2)\n",
    "            unique_keys[k2].add(k1)\n",
    "\n",
    "        for key, cand_keys in unique_keys.items():\n",
    "            scores = [] \n",
    "            for cand_key in cand_keys:\n",
    "                search_key = f'{key}-{cand_key}' if f'{key}-{cand_key}' in self.bert_scores else f'{cand_key}-{key}'\n",
    "                scores.append(self.bert_scores[search_key])\n",
    "\n",
    "            dist[key] = np.mean(scores)\n",
    "\n",
    "        x = list(dist.values())\n",
    "        \n",
    "        return {\n",
    "            'mean': np.mean(x), 'std': np.std(x), 'min': np.min(x), 'max': np.max(x), 'raw': x\n",
    "        }\n",
    "\n",
    "    def create_samples_by_task(self):\n",
    "        \"\"\"\n",
    "        question: {qid: question}\n",
    "        ba: {cid: ba}\n",
    "        db_id: {qid: db_id}\n",
    "        db_id2qids: {db_id: [qid1, qid2, ...]}\n",
    "        codes: {qid: gold_complexity_codes}\n",
    "        codes2qids: {db_id: {gold_complexity_codes: [qid1, qid2, ...]}}\n",
    "        \"\"\"\n",
    "        if self.task == 'retrieval':\n",
    "            # create triplets: (anchor, positive, negative)\n",
    "            # sample number: n_neg (same database) + n_neg (different database)\n",
    "            self.samples = self._retrieval_sample_generation()\n",
    "        elif self.task == 'rerank':\n",
    "            # create pairs: (text=[question, ba], label)\n",
    "            # sample number: <=11 (3 positive, 8 negative)\n",
    "            # - 1 query, ba (target code, db_id)\n",
    "            # - 1 positive: from same db, same code to the target code\n",
    "            # - 1 positive: from different db, same code to the target code\n",
    "            # - 4 (at least) negative: from same db, different code to the target code\n",
    "            # - 4 negative: from different db, different code to the target code\n",
    "            self.samples = self._rerank_sample_generation()\n",
    "        else:  # task not found\n",
    "            raise ValueError(f'Invalid task: {self.task}')\n",
    "\n",
    "    def _retrieval_sample_generation(self):\n",
    "        samples = []\n",
    "        distributions = defaultdict(lambda: {'same_db': 0, 'diff_db': 0})  # check how many hard samples are generated(hard samples = neg from same db)\n",
    "        iterator = tqdm(enumerate(self.sample_ids), total=len(self.sample_ids))\n",
    "        for qid, sid in iterator:\n",
    "            s: dict[str, int|str|list[int]] = {\n",
    "                'anchor': qid,\n",
    "                'positive': sid,\n",
    "                'negative': []  # [neg_sid1, neg_sid2, ...]\n",
    "            }\n",
    "            db_id = self.db_ids[qid]\n",
    "\n",
    "            # negative samples in same database\n",
    "            iterator.set_postfix_str(f'[{self.task}-{qid}] sampling neg sample from same database')\n",
    "            i = 0\n",
    "            sampled = set()  # {qid}\n",
    "            neg_qids_same_db = []\n",
    "            n_neg_same_db = self.n_neg\n",
    "            n_neg_diff_db = self.n_neg\n",
    "            while i < n_neg_same_db:\n",
    "                candidates = np.setdiff1d(self.db_id2qids[db_id], [qid]+list(sampled))\n",
    "                if len(candidates) == 0:\n",
    "                    # no existing samples that are in the same db, need to sample from different db\n",
    "                    # warnings.warn(f'[{db_id}-{sid}] No more samples, will sample from different database')\n",
    "                    n_neg_diff_db += 1\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                # check sampled ba is too similar to positive ba\n",
    "                keys: list[tuple[int, str]] = []\n",
    "                for cqid in candidates:\n",
    "                    csid = self.sample_ids[cqid]\n",
    "                    if f'{sid}-{csid}' in self.bert_scores:\n",
    "                        keys.append((cqid, f'{sid}-{csid}'))\n",
    "                    elif f'{csid}-{sid}' in self.bert_scores:\n",
    "                        keys.append((cqid, f'{csid}-{sid}'))\n",
    "                    else:\n",
    "                        raise KeyError(f'[{db_id}-{sid}] No bert score found for {sid}-{csid} or {csid}-{sid}')\n",
    "                # filter too similar samples that over threshold\n",
    "                candidates = [cqid for (cqid, key) in keys if self.bert_scores[key] < self.threshold]\n",
    "                if len(candidates) == 0:\n",
    "                    # warnings.warn(f'[{db_id}-{sid}] No more samples to be sampled that match criteria, it will sample from different database')\n",
    "                    n_neg_diff_db += 1\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                sampled_qid = np.random.choice(candidates)\n",
    "\n",
    "                # check if sampled_qid not in sampled\n",
    "                if sampled_qid not in sampled:\n",
    "                    neg_qids_same_db.append(sampled_qid)\n",
    "                    sampled.add(sampled_qid)\n",
    "                    i += 1\n",
    "                    distributions[qid]['same_db'] += 1\n",
    "\n",
    "            neg_sids_same_db = [self.sample_ids[neg_qid] for neg_qid in neg_qids_same_db]\n",
    "            s['negative'].extend(neg_sids_same_db)\n",
    "\n",
    "            # negative samples in different database\n",
    "            iterator.set_postfix_str(f'[{self.task}-{qid}] sampling neg sample from different database')\n",
    "            i = 0\n",
    "            sampled = set()  # {qid}\n",
    "            neg_qids_diff_db = []\n",
    "            while i < n_neg_diff_db:\n",
    "                db_ids_candidates = np.setdiff1d(list(self.db_id2qids.keys()), [db_id])\n",
    "                sampled_db_id = np.random.choice(db_ids_candidates)\n",
    "                candidates = np.setdiff1d(self.db_id2qids[sampled_db_id], list(sampled))\n",
    "                sampled_qid = np.random.choice(candidates)\n",
    "\n",
    "                # don't need to check sampled ba is too similar to positive ba for different db\n",
    "                # check if sampled_qid not in sampled\n",
    "                if sampled_qid not in sampled:\n",
    "                    neg_qids_diff_db.append(sampled_qid)\n",
    "                    sampled.add(sampled_qid)\n",
    "                    i += 1\n",
    "                    distributions[qid]['diff_db'] += 1\n",
    "\n",
    "            neg_sids_diff_db = [self.sample_ids[neg_qid] for neg_qid in neg_qids_diff_db]\n",
    "            s['negative'].extend(neg_sids_diff_db)\n",
    "\n",
    "            for neg_sid in s['negative']:\n",
    "                samples.append({\n",
    "                    'anchor': int(s['anchor']),\n",
    "                    'positive': s['positive'],\n",
    "                    'negative': neg_sid\n",
    "                })\n",
    "\n",
    "        self._distributions = distributions\n",
    "        return samples\n",
    "\n",
    "    def _rerank_sample_generation(self):\n",
    "        samples = []\n",
    "        iterator = tqdm(enumerate(self.sample_ids), total=len(self.sample_ids))\n",
    "        \n",
    "        # same db, diff code = sd\n",
    "        # diff db, diff code = dd\n",
    "        # since it will run for all samples, positive samples are fixed\n",
    "        # so we need to sample negative samples\n",
    "        def verifier():\n",
    "            pass\n",
    "\n",
    "        def sum_sampled(db_sampled):\n",
    "            return sum([len(qids) for qids in db_sampled.values()])\n",
    "\n",
    "        for qid, sid in iterator:\n",
    "            s: dict[str, list[list[str]]|list[int]] = {\n",
    "                'text': [],\n",
    "                'label': []\n",
    "            }\n",
    "            # target\n",
    "            target_code = self.codes[qid]\n",
    "            db_id = self.db_ids[qid]\n",
    "            s['text'].append([qid, sid])\n",
    "            s['label'].append(1)\n",
    "\n",
    "            # negative from same db, different code: sd\n",
    "            iterator.set_postfix_str(f'[{self.task}-{qid}] sampling neg sample from same database')\n",
    "            i = 0\n",
    "            n_neg_dd = self.n_neg\n",
    "            n_neg_ds = self.n_neg\n",
    "            same_db_sampled = set()  # {qid}\n",
    "            while i < n_neg_ds:\n",
    "                # check enough qids to sample for other codes\n",
    "                # code_candidate should have at least 1 sample that is not in sampled before\n",
    "                sd_code_candidates = []\n",
    "                for code, qids in self.codes2qids[db_id].items():\n",
    "                    if (code != target_code) and \\\n",
    "                        (len(np.setdiff1d(qids, list(same_db_sampled))) > 0):\n",
    "                        sd_code_candidates.append(code)\n",
    "                if len(sd_code_candidates) == 0:\n",
    "                    # means that there is no more samples in db_id, keep sampling from other dbs\n",
    "                    n_neg_dd += 1\n",
    "                    i += 1\n",
    "                    continue\n",
    "                    \n",
    "                sd_code = np.random.choice(sd_code_candidates)\n",
    "                sd_qid_candidates = np.setdiff1d(self.codes2qids[db_id][sd_code], \n",
    "                                                 [qid]+list(same_db_sampled))\n",
    "                sd_qid = np.random.choice(sd_qid_candidates)\n",
    "                sd_sid = self.sample_ids[sd_qid]\n",
    "                \n",
    "                # check if sd_qid not in sampled\n",
    "                if sd_qid not in same_db_sampled:\n",
    "                    s['text'].append([qid, sd_sid])\n",
    "                    s['label'].append(0)\n",
    "                    same_db_sampled.add(sd_qid)\n",
    "                    i += 1\n",
    "\n",
    "            # negative from different db, different code\n",
    "            iterator.set_postfix_str(f'[{self.task}-{qid}] sampling neg sample from different database')\n",
    "            i = 0\n",
    "            diff_db_sampled = defaultdict(set)  # {db_id: {qid}}\n",
    "            dd_sampled_db_ids = set()  # {db_id} only added if there is no samples that have the same code in different db\n",
    "            while i < n_neg_dd:\n",
    "                dd_db_ids_candidates = np.setdiff1d(list(self.db_id2qids.keys()), \n",
    "                                                    [db_id]+list(dd_sampled_db_ids))\n",
    "                dd_sampled_db_id = np.random.choice(dd_db_ids_candidates)\n",
    "\n",
    "                # check enough qids to sample for other codes\n",
    "                # code_candidate should have at least 1 sample that is not in sampled before\n",
    "                dd_code_candidates = []\n",
    "                for code, qids in self.codes2qids[dd_sampled_db_id].items():\n",
    "                    if (code != target_code) and \\\n",
    "                        (len(np.setdiff1d(qids, list(diff_db_sampled[dd_sampled_db_id]))) > 0):\n",
    "                        dd_code_candidates.append(code)\n",
    "                if len(dd_code_candidates) == 0:\n",
    "                    # means that there is no more samples in sampled_db_id, keep sampling from other dbs\n",
    "                    dd_sampled_db_ids.add(dd_sampled_db_id)\n",
    "                    continue\n",
    "\n",
    "                dd_code = np.random.choice(dd_code_candidates)\n",
    "                dd_qid_candidates = np.setdiff1d(self.codes2qids[dd_sampled_db_id][dd_code], \n",
    "                                                 list(diff_db_sampled[dd_sampled_db_id]))\n",
    "                dd_qid = np.random.choice(dd_qid_candidates)\n",
    "                dd_sid = self.sample_ids[dd_qid]\n",
    "\n",
    "                # check if sampled_qid not in sampled\n",
    "                if dd_qid not in diff_db_sampled[dd_sampled_db_id]:\n",
    "                    s['text'].append([qid, dd_sid])\n",
    "                    s['label'].append(0)\n",
    "                    diff_db_sampled[dd_sampled_db_id].add(dd_qid)\n",
    "                    i += 1\n",
    "\n",
    "            for i in range(len(s['text'])):\n",
    "                samples.append({\n",
    "                    'text': [int(s['text'][i][0]), s['text'][i][1]], # [qid, sid]\n",
    "                    'label': s['label'][i]\n",
    "                })\n",
    "\n",
    "        return samples\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.task == 'retrieval':\n",
    "            return self._retrieval_task_itemgetter(idx)\n",
    "        elif self.task == 'rerank':\n",
    "            return self._rerank_task_itemgetter(idx)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid task: {self.task}')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]\n",
    "\n",
    "    def _retrieval_task_itemgetter(self, idx):\n",
    "        # {anchor: qid, positive: sid, negative: sid}\n",
    "        sample = self.samples[idx]\n",
    "        qid = sample['anchor']\n",
    "        pos_id = sample['positive']\n",
    "        neg_id = sample['negative']\n",
    "        \n",
    "        question = self.questions[qid]\n",
    "        pos_ba = self.bas[pos_id]\n",
    "        neg_ba = self.bas[neg_id]\n",
    "\n",
    "        return {\n",
    "            'anchor': question,\n",
    "            'positive': pos_ba,\n",
    "            'negative': neg_ba\n",
    "        }\n",
    "    \n",
    "    def _rerank_task_itemgetter(self, idx):\n",
    "        # `sentence_transfomer` will update there code th newer version later. \n",
    "        # Here, we use old version for training cross-encoder\n",
    "        # {text: [question, ba], label: label}\n",
    "        sample = self.samples[idx]\n",
    "        qid, sid = sample['text']\n",
    "        label = sample['label']\n",
    "\n",
    "        question = self.questions[qid]\n",
    "        ba = self.bas[sid]\n",
    "        return {\n",
    "            'text': [question, ba],\n",
    "            'label': label\n",
    "        }\n",
    "    \n",
    "    def get_rerank_samples(self, is_train: bool=True):\n",
    "        samples = []\n",
    "        if is_train:\n",
    "            for x in self:\n",
    "                samples.append(InputExample(texts=x['text'], label=x['label']))\n",
    "        else:\n",
    "            # {qid: {'positive': [sid1, sid2, ...], 'negative': [sid1, sid2, ...]}}\n",
    "            sample_ids = defaultdict(lambda : {'positive': [], 'negative': []})\n",
    "            for s in self.samples:\n",
    "                qid, sid = s['text']\n",
    "                label = s['label']\n",
    "                if label == 1:\n",
    "                    sample_ids[qid]['positive'].append(sid)\n",
    "                else:\n",
    "                    sample_ids[qid]['negative'].append(sid)\n",
    "\n",
    "            for qid, v in sample_ids.items():\n",
    "                samples.append({\n",
    "                    'query': self.questions[qid],\n",
    "                    'positive': [self.bas[sid] for sid in v['positive']],\n",
    "                    'negative': [self.bas[sid] for sid in v['negative']]\n",
    "                })\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3612745325582957,\n",
       " 0.06275672978943266,\n",
       " 0.1056786763171355,\n",
       " 0.5432230182762804)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = BADataset(\n",
    "    data=data['train'],\n",
    "    bert_scores=data['bert_scores'], \n",
    "    task='retrieval', \n",
    "    n_neg=2,\n",
    "    threshold=0.25,\n",
    ")\n",
    "x = train_dataset._check_stats_bert_score()\n",
    "x['mean'], x['std'], x['min'], x['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5708/5708 [00:10<00:00, 542.40it/s, [retrieval-5707] sampling neg sample from different database]\n",
      "100%|██████████| 633/633 [00:00<00:00, 672.73it/s, [retrieval-632] sampling neg sample from different database]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 22832, dev: 1266\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BADataset(\n",
    "    data=data['train'],\n",
    "    bert_scores=data['bert_scores'], \n",
    "    task='retrieval', \n",
    "    n_neg=2,\n",
    "    threshold=0.3,\n",
    ")\n",
    "train_dataset.create_samples_by_task()\n",
    "train_save_path = experiment_folder / 'predictions' / 'create_bo' / f'retrieval_train.json'\n",
    "train_dataset.save_samples(sample_save_path=train_save_path)\n",
    "\n",
    "dev_dataset = BADataset(\n",
    "    data=data['dev'],\n",
    "    bert_scores=data['bert_scores'], \n",
    "    task='retrieval', \n",
    "    n_neg=1,\n",
    "    threshold=0.3,\n",
    ")\n",
    "dev_dataset.create_samples_by_task()\n",
    "dev_save_path = experiment_folder / 'predictions' / 'create_bo' / f'retrieval_dev.json'\n",
    "dev_dataset.save_samples(sample_save_path=dev_save_path)\n",
    "\n",
    "print(f'train: {len(train_dataset)}, dev: {len(dev_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='diff_db', ylabel='count'>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(train_dataset._distributions).T\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.countplot(data=df, x='same_db', ax=axes[0], label='same_db')\n",
    "sns.countplot(data=df, x='diff_db', ax=axes[1], label='diff_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5708/5708 [00:16<00:00, 353.64it/s, [rerank-5707] sampling neg sample from different database]\n",
      "100%|██████████| 633/633 [00:01<00:00, 382.33it/s, [rerank-632] sampling neg sample from different database]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 51372, dev: 5697\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BADataset(\n",
    "    data=data['train'],\n",
    "    bert_scores=data['bert_scores'], \n",
    "    task='rerank',\n",
    "    n_neg=4,\n",
    ")\n",
    "train_dataset.create_samples_by_task()\n",
    "train_save_path = experiment_folder / 'predictions' / 'create_bo' / f'rerank_train.json'\n",
    "train_dataset.save_samples(sample_save_path=train_save_path)\n",
    "\n",
    "dev_dataset = BADataset(\n",
    "    data=data['dev'],\n",
    "    bert_scores=data['bert_scores'], \n",
    "    task='rerank', \n",
    "    n_neg=4,\n",
    ")\n",
    "dev_dataset.create_samples_by_task()\n",
    "dev_save_path = experiment_folder / 'predictions' / 'create_bo' / f'rerank_dev.json'\n",
    "dev_dataset.save_samples(sample_save_path=dev_save_path)\n",
    "\n",
    "print(f'train: {len(train_dataset)}, dev: {len(dev_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BADataset(\n",
    "    data=data['train'],\n",
    "    bert_scores=data['bert_scores'], \n",
    "    task='retrieval', \n",
    "    n_neg=2,\n",
    "    threshold=0.3,\n",
    ")\n",
    "train_dataset.load_samples(experiment_folder / 'predictions' / 'create_bo' / f'retrieval_train.json')\n",
    "\n",
    "dev_dataset = BADataset(\n",
    "    data=data['dev'],\n",
    "    bert_scores=data['bert_scores'], \n",
    "    task='retrieval', \n",
    "    n_neg=1,\n",
    "    threshold=0.3,\n",
    ")\n",
    "dev_dataset.load_samples(experiment_folder / 'predictions' / 'create_bo' / f'retrieval_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = 'msmarco-MiniLM-L6-cos-v5'\n",
    "model = SentenceTransformer(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_generator(train_dataset.__iter__)\n",
    "dev_ds = Dataset.from_generator(dev_dataset.__iter__)\n",
    "\n",
    "exp_name = f'{base_model_name.split(\"/\")[-1]}-q_ba'\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=f'models/{exp_name}',\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=0.01,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=1,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1,\n",
    "    torch_empty_cache_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=f'logs/{exp_name}',\n",
    ")\n",
    "\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "dev_evaluator = TripletEvaluator(\n",
    "    anchors=dev_ds['anchor'],\n",
    "    positives =dev_ds['positive'],\n",
    "    negatives=dev_ds['negative'],\n",
    "    name='q_ba_dev',\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    loss=loss,\n",
    "    args=args,\n",
    "    eval_dataset=dev_ds,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BADataset(\n",
    "    data=data['train'],\n",
    "    bert_scores=data['bert_scores'], \n",
    "    task='rerank',\n",
    "    n_neg=4,\n",
    ")\n",
    "train_dataset.load_samples(experiment_folder / 'predictions' / 'create_bo' / f'rerank_train.json')\n",
    "dev_dataset = BADataset(\n",
    "    data=data['dev'],\n",
    "    bert_scores=data['bert_scores'], \n",
    "    task='rerank', \n",
    "    n_neg=4,\n",
    ")\n",
    "dev_dataset.load_samples(experiment_folder / 'predictions' / 'create_bo' / f'rerank_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "model = CrossEncoder(base_model_name, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_dataset.get_rerank_samples(is_train=True)\n",
    "train_loader = DataLoader(\n",
    "    train_samples, batch_size=256, shuffle=True, num_workers=0)\n",
    "dev_samples = dev_dataset.get_rerank_samples(is_train=False)\n",
    "# dev_loader = DataLoader(\n",
    "#     dev_samples, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "rank_evaluator = CERerankingEvaluator(dev_samples, name=\"train-eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = 5000\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataloader=train_loader,\n",
    "    evaluator=rank_evaluator,\n",
    "    epochs=1,\n",
    "    evaluation_steps=20,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='temp',\n",
    "    use_amp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [0, '6081'], 'label': 1}\n",
      "{'text': [0, '6006'], 'label': 0}\n",
      "{'text': [0, '5993'], 'label': 0}\n",
      "{'text': [0, '5939'], 'label': 0}\n",
      "{'text': [0, '6086'], 'label': 0}\n",
      "{'text': [0, '255'], 'label': 0}\n",
      "{'text': [0, '10633'], 'label': 0}\n",
      "{'text': [0, '745'], 'label': 0}\n",
      "{'text': [0, '8543'], 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "for x in dev_dataset.samples:\n",
    "    if x['text'][0] == 0:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=f'models/temp',\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=0.001,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=1,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "loss = losses.MarginMSELoss(model)\n",
    "dev_samples = dev_dataset.create_rerank_eval_set()\n",
    "dev_evaluator = RerankingEvaluator(dev_samples, at_k=3, name=\"train-eval\")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    loss=loss,\n",
    "    args=args,\n",
    "    eval_dataset=dev_ds,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=2, shuffle=True, num_workers=0)\n",
    "dev_loader = DataLoader(\n",
    "    dev_ds, batch_size=2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_samples = dev_dataset.create_rerank_eval_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name cross-encoder/ms-marco-MiniLM-L-6-v2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train-eval_map': 0.16666666666666666,\n",
       " 'train-eval_mrr@10': 0.16666666666666666,\n",
       " 'train-eval_ndcg@10': 0.35620718710802235}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev_samples = {'query': '', 'positive': [], 'negative': []}\n",
    "eval_samples = dev_dataset.create_rerank_eval_set()\n",
    "# evaluator = CERerankingEvaluator(dev_samples[-10:], at_k=10, name=\"train-eval\")\n",
    "evaluator = CEBinaryClassificationEvaluator(\n",
    "    sentence_pairs=[x['text'] for x in dev_dataset],\n",
    "    labels=[x['label'] for x in dev_dataset]\n",
    ")\n",
    "res = evaluator(model)\n",
    "\n",
    "# model2 = SentenceTransformer(base_model_name)\n",
    "# evaluator2 = RerankingEvaluator(dev_samples, at_k=10, name=\"train-eval\")\n",
    "# evaluator2(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([6081, 5993, 6086, 6017, 6056, 6006, 6108, 6109, 6030, 5971, 6005, 5939, 5934, 6034, 2559, 2554, 2522, 2556, 727, 715, 699, 7275, 7250, 7335, 7269, 7246, 7446, 7253, 7340, 7223, 7359, 7310, 7047, 7150, 7262, 7379, 7171, 7099, 7377, 301, 255, 309, 328, 265, 345, 389, 362, 392, 245, 288, 22, 144, 140, 10, 73, 4641, 4654, 4653, 4720, 4644, 4695, 4723, 4660, 5306, 5293, 5337, 5290, 5353, 5357, 5333, 5315, 5227, 5144, 5175, 5110, 5197, 5149, 5095, 5210, 5121, 5097, 2007, 2011, 2033, 1974, 2014, 2019, 1843, 1911, 1803, 1963, 1932, 1937, 1919, 1879, 2035, 982, 1007, 2494, 1122, 1086, 1148, 1058, 1191, 1108, 1062, 1163, 1167, 1207, 1157, 1059, 1116, 6411, 6508, 6316, 3675, 3584, 3660, 3607, 3663, 3649, 3522, 3602, 3578, 3564, 3528, 2698, 2615, 2600, 2631, 2635, 2722, 2652, 2705, 2660, 4889, 4844, 4749, 4780, 4856, 4786, 4872, 4887, 4790, 4767, 4802, 4806, 4783, 10767, 10881, 10775, 10809, 10772, 10792, 10739, 10707, 10584, 10633, 10704, 10710, 10587, 10663, 10572, 10685, 10682, 10691, 8783, 8782, 8778, 8849, 8781, 8821, 8803, 9436, 9425, 9476, 9422, 9464, 5027, 4926, 5042, 5019, 5008, 4918, 4989, 4953, 5023, 5044, 4956, 4914, 5079, 4915, 4922, 4411, 4518, 4381, 4552, 4475, 4540, 4438, 4422, 4532, 2192, 2129, 2156, 2186, 2220, 2226, 2131, 2093, 2181, 2452, 2434, 2395, 2383, 2371, 4590, 4601, 10368, 10296, 10269, 10362, 10285, 10307, 10297, 10320, 10319, 10353, 613, 676, 638, 598, 670, 645, 575, 631, 2474, 5903, 5868, 5890, 5823, 5826, 5830, 5820, 5844, 5885, 5882, 5871, 5610, 5601, 5611, 5599, 5663, 5662, 2064, 2058, 2069, 2056, 2059, 2086, 3757, 3716, 3739, 3714, 3749, 3741, 3736, 8976, 8982, 8942, 5471, 5507, 5572, 5528, 5510, 5527, 3019, 2982, 2985, 3030, 3025, 6727, 6688, 6716, 6904, 6782, 6849, 6814, 6759, 6896, 6869, 6870, 6747, 6867, 6796, 6903, 6847, 6817, 6754, 6772, 2854, 2861, 2866, 2936, 2901, 2802, 2881, 2749, 2786, 2790, 2756, 10921, 10931, 1587, 1569, 1626, 1579, 1562, 1608, 1643, 1623, 7941, 7930, 7934, 7918, 7923, 8098, 8050, 8136, 8035, 8104, 8084, 8039, 8025, 8116, 9665, 9710, 9660, 9622, 9720, 9713, 9714, 9685, 9708, 9635, 9757, 9752, 9646, 9702, 9758, 9733, 8466, 8381, 8239, 8439, 8337, 8273, 8278, 8378, 8338, 8342, 8279, 8297, 8502, 8383, 8268, 8217, 8220, 8497, 8453, 8489, 8275, 8232, 8438, 8409, 8500, 8547, 8526, 8530, 8569, 8543, 8580, 9552, 9545, 9519, 9561, 9594, 756, 745, 762, 5371, 5403, 5410, 5443, 5450, 1455, 1363, 1498, 1538, 1527, 1448, 1388, 1436, 1453, 1525, 858, 930, 923, 951, 953, 903, 9417, 9178, 9370, 9334, 9308, 9239, 9166, 9114, 9327, 9182, 9205, 9324, 9339, 9394, 9177, 9230, 9365, 9227, 9187, 9119, 9369, 9253, 9127, 9120, 9231, 9131, 9299, 9245, 8182, 9073, 9065, 9002, 9072, 9014, 9090, 9033, 9085, 9957, 10109, 10061, 9961, 10103, 9986, 10032, 10117, 6277, 6253, 4149, 9951, 9851, 9912, 9905, 9822, 9918, 9848, 2262, 2276, 8870, 8899, 8914, 8863, 8877, 8919, 829, 851, 823, 6149, 6179, 6171, 6139, 6193, 6113, 6147, 7788, 7775, 7619, 7761, 7631, 7693, 7745, 7744, 4062, 3933, 3787, 4093, 4120, 3826, 3936, 3797, 3888, 3988, 3870, 3964, 3999, 3792, 4006, 4136, 3884, 4130, 3997, 4002, 4072, 3833, 4127, 3841, 3871, 4074, 3809, 3960, 10458, 10528, 10539, 10474, 10546, 10532, 10552, 10452, 10536, 10448, 10511, 10527, 10496, 10469, 6927, 6966, 6985, 6920, 477, 449, 428, 480, 489, 522, 561, 451, 419, 543, 4194, 4195, 4354, 4254, 4249, 4183, 4304, 4255, 4272, 4322, 4299, 4164, 4201, 4345, 4231, 4176, 8726, 8670, 8672, 8729, 8713, 8671, 8648, 8587, 8767, 8763, 3156, 3182, 3183, 3240, 3188, 3163, 3152, 3223, 3304, 3197, 3233, 10258, 10227, 10139, 10186, 10231, 1699, 1776, 1671, 5803, 5818, 5755, 5716, 5814, 5813, 5726, 5723, 5781, 7852, 7853, 7864, 7830, 7878, 7820, 3465, 3378, 3322, 3468, 3319, 3416, 3314, 3349, 3362, 7597, 7566, 7573, 7581, 182, 225, 3123, 3071, 3064, 1294, 1301, 1265, 1317, 1283, 1342, 1347])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset.samples['question'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'ba', 'gold_complexity_codes'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset.samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir_evaluator = InformationRetrievalEvaluator(\n",
    "    dev_queries,\n",
    "    corpus,\n",
    "    dev_rel_docs,\n",
    "    show_progress_bar=True,\n",
    "    corpus_chunk_size=100000,\n",
    "    precision_recall_at_k=[10, 100],\n",
    "    name=\"msmarco dev\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FitMixin.fit() got an unexpected keyword argument 'train_dataloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: FitMixin.fit() got an unexpected keyword argument 'train_dataloader'"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataloader=train_loader,\n",
    "    evaluator=evaluator,\n",
    "    epochs=1,\n",
    "    evaluation_steps=1,\n",
    "    warmup_steps=1,\n",
    "    use_amp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train-eval_map': 0.5,\n",
       " 'train-eval_mrr@10': 0.5,\n",
       " 'train-eval_ndcg@10': 0.6309297535714573}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_samples = [{\n",
    "    'query': 'What is the capital of France?',\n",
    "    'positive': ['Pariss'],\n",
    "    'negative': ['London', 'Berlin', 'Madrid']\n",
    "}]\n",
    "\n",
    "# evaluator = CERerankingEvaluator(dev_samples, at_k=3, name=\"train-eval\")\n",
    "evaluator = RerankingEvaluator(dev_samples, at_k=10, name=\"train-eval\")\n",
    "evaluator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedures\n",
    "\n",
    "## task: valid_bo\n",
    "\n",
    "1. (retrieve) retrieve and rerank for dev dataset to get relevant train BOs\n",
    "2. (gen_template) generate sql template with dev dataset and retrieved BOs --> output: dev sql templates\n",
    "3. (fill_in) fill-in values with dev sql templates --> output: dev sql\n",
    "4. (evaluate) evaluate the dev sql to get the most helpful train BOs --> output: BOs Pool\n",
    "\n",
    "## task: inference_with_bo\n",
    "\n",
    "1. (retrieve) retrieve and rerank for test dataset from BOs Pool\n",
    "2. (gen_template) generate sql template with test dataset and retrieved BOs --> output: test sql templates\n",
    "3. (fill_in) fill-in values with test sql templates --> output: test sql\n",
    "4. (evaluate) evaluate the test sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_core.embeddings import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'retrieval_dev.json'\n",
    "prediction_path = experiment_folder / 'predictions' / 'retrieve'\n",
    "with open(prediction_path / file_name, 'w') as file:\n",
    "    all_results = []\n",
    "    for p in prediction_path.glob(f'x-*.json'):\n",
    "        with p.open() as f:\n",
    "            temp = json.load(f)\n",
    "        all_results.extend(temp)\n",
    "    json.dump(all_results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in prediction_path.glob(f'x-*.json'):\n",
    "    p.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(\n",
    "        bos: dict[str, list[dict[str, str]]], \n",
    "        embeddings_model: Embeddings,\n",
    "        is_question_query: bool=False\n",
    "    ) -> FAISS:\n",
    "    documents = []\n",
    "    for db_id, samples in bos.items():\n",
    "        for x in samples:\n",
    "            doc = Document(\n",
    "                page_content=x['ba'] if not is_question_query else x['question'],\n",
    "                metadata={\n",
    "                    'sample_id': x['sample_id'],\n",
    "                    'db_id': db_id,\n",
    "                    'vt': x['vt'],\n",
    "                    'complexity': x['gold_complexity']\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "    vector_store = FAISS.from_documents(\n",
    "        documents, \n",
    "        embedding = embeddings_model,\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "def get_retriever(\n",
    "        vectorstore: FAISS,\n",
    "        db_id: str='',\n",
    "        cross_encoder: HuggingFaceCrossEncoder=None,\n",
    "        n_retrieval: int=3,\n",
    "        k_retrieval: int=10,\n",
    "        score_threshold: float=0.60,\n",
    "        use_reranker: bool= False,\n",
    "    ) -> BaseRetriever:\n",
    "    k_retrieval = k_retrieval if use_reranker else n_retrieval\n",
    "    base_retriever = vectorstore.as_retriever(\n",
    "        search_type='similarity_score_threshold', # 'similarity',\n",
    "        search_kwargs={\n",
    "            'k': k_retrieval, \n",
    "            'score_threshold': score_threshold, \n",
    "            'filter': {'db_id': db_id},\n",
    "        }\n",
    "    )\n",
    "    if use_reranker:\n",
    "        compressor = CrossEncoderReranker(model=cross_encoder, top_n=n_retrieval)\n",
    "        retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor, base_retriever=base_retriever\n",
    "        )\n",
    "    else:\n",
    "        retriever = base_retriever\n",
    "    return retriever\n",
    "\n",
    "embedding_model = 'custom'\n",
    "reranker_model = 'custom'\n",
    "if embedding_model == 'openai':\n",
    "        embeddings_model = OpenAIEmbeddings()\n",
    "elif embedding_model == 'custom':\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name='./models/msmarco-MiniLM-L6-cos-v5-q_ba')\n",
    "\n",
    "if reranker_model == 'msmarco':\n",
    "    cross_encoder = HuggingFaceCrossEncoder(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "elif reranker_model == 'custom':\n",
    "    cross_encoder = HuggingFaceCrossEncoder(model_name='./models/ms-marco-MiniLM-L-6-v2-q_ba-rerank')\n",
    "\n",
    "bo_path = experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.json'\n",
    "with bo_path.open() as f:\n",
    "    bos = json.load(f)\n",
    "\n",
    "# remove duplicate bos\n",
    "hash_map = defaultdict(set)\n",
    "for db_id, xs in bos.items():\n",
    "    for x in xs:\n",
    "        hash_map[x['vt']].add(x['sample_id'])\n",
    "\n",
    "ids = list(map(lambda x: list(x)[0], hash_map.values()))\n",
    "new_bos = {}\n",
    "for db_id, xs in bos.items():\n",
    "    new_bos[db_id] = [x for x in xs if x['sample_id'] in ids]\n",
    "\n",
    "bos = new_bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[movie_platform] 33\n",
      "[book_publishing_company] 14\n",
      "[retail_complains] 33\n",
      "[movies_4] 31\n",
      "[codebase_comments] 24\n",
      "[trains] 8\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "typ = 'dev'\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_{typ}.json')\n",
    "# df = pd.read_csv(experiment_folder / 'evals' / 'zero_shot' / f'{ds}_dev.csv')\n",
    "# df_error = df.loc[df['exec_result'] == 0]\n",
    "# error_ids = df_error['sample_id'].tolist()\n",
    "# samples = list(filter(lambda x: x.sample_id in error_ids, samples))\n",
    "samples_by_db_id = defaultdict(list)\n",
    "for sample in samples:\n",
    "    samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "for i, (db_id, ss) in enumerate(samples_by_db_id.items()):\n",
    "    print(f'[{db_id}] {len(ss)}')\n",
    "    if i == 5:\n",
    "        print('...')\n",
    "        break\n",
    "\n",
    "db_id = list(bos.keys())[0]\n",
    "x_samples = list(filter(lambda x: x.db_id == db_id, samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = get_vector_store(\n",
    "    bos, embeddings_model=embeddings_model, is_question_query=False)\n",
    "retriever = get_retriever(\n",
    "    vectorstore=vector_store,\n",
    "    db_id=db_id,\n",
    "    cross_encoder=cross_encoder,\n",
    "    n_retrieval=3,\n",
    "    k_retrieval=10,\n",
    "    score_threshold=0.50,\n",
    "    use_reranker=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the author's name of the books that cost 19 dollars and above.\n"
     ]
    }
   ],
   "source": [
    "question = x_samples[0].final.question\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT DISTINCT T3.author_name FROM book AS T1 INNER JOIN book_author AS T2 ON T1.book_id = T2.book_id INNER JOIN author AS T3 ON T3.author_id = T2.author_id INNER JOIN order_line AS T4 ON T4.book_id = T1.book_id WHERE T4.price > 19'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_samples[0].final.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[movie_platform] 33\n",
      "[book_publishing_company] 14\n",
      "[retail_complains] 33\n",
      "[movies_4] 31\n",
      "[codebase_comments] 24\n",
      "[trains] 8\n",
      "...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cates = df_base.groupby('db_id')['gold_complexity'].apply(_get_categories).rename('category').apply(_format_interval)\n",
    "df_base = pd.merge(df_base, df_cates.reset_index('db_id', drop=True), left_index=True, right_index=True)\n",
    "\n",
    "df = pd.merge(\n",
    "    left=df_bo,\n",
    "    right=df_base,\n",
    "    how='inner',\n",
    "    on=['db_id', 'sample_id', 'gold_complexity'],\n",
    "    suffixes=('_bo', '_base')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_column = ['db_id', 'retrieved']\n",
    "execution_improvement = df.groupby(group_column)[['exec_result_base', 'exec_result_bo']].sum().diff(axis=1)['exec_result_bo'].rename('execution_improvement')\n",
    "merit_structural = df.groupby(group_column)[['structural_score_base', 'structural_score_bo']].mean().diff(axis=1)['structural_score_bo'].rename('merit_structural')\n",
    "merit_semantic = df.groupby(group_column)[['semantic_score_base', 'semantic_score_bo']].mean().diff(axis=1)['semantic_score_bo'].rename('merit_semantic')\n",
    "merit = df.groupby(group_column)[['f1_score_base', 'f1_score_bo']].mean().diff(axis=1)['f1_score_bo'].rename('merit')\n",
    "\n",
    "ranks = merit.reset_index().groupby(['db_id'])['merit'].rank(method='first', ascending=False).rename('rank').astype(np.int64)\n",
    "merit = pd.concat([merit.reset_index(), ranks], axis=1)\n",
    "merit_by_rank = merit.sort_values(by=['db_id', 'rank'], ascending=True)\n",
    "\n",
    "# merit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "merit\n",
       "True     85\n",
       "False    41\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(merit_by_rank.groupby('db_id')['merit'].mean().sort_values(ascending=False) > 0.0).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bos = defaultdict(list)\n",
    "for x in merit_by_rank.loc[:, ['db_id', 'retrieved']].to_dict(orient='records'):\n",
    "    test_bos[x['db_id']].append(x['retrieved'])\n",
    "\n",
    "n_bos = range(5, 26, 5)\n",
    "test_scenarios = defaultdict(dict)\n",
    "for n_bo in n_bos:\n",
    "    for db_id in test_bos:\n",
    "        test_scenarios[n_bo][db_id] = test_bos[db_id][:n_bo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (experiment_folder / 'test_scenarios.json').open('w') as f:\n",
    "    json.dump(test_scenarios, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.parsing_sql import Schema, extract_all\n",
    "\n",
    "sql1 = \"\"\"SELECT * FROM lineitem\"\"\"\n",
    "\n",
    "schema = Schema({\n",
    "    'lineitem': {'l_receiptdate': 'date', 'l_commitdate': 'date'}\n",
    "})\n",
    "\n",
    "output = extract_all(sql1, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_complexity(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_path = proj_path / 'data' / 'spider'\n",
    "# tables, train_data, dev_data = load_raw_data(spider_path, load_test=False)\n",
    "\n",
    "# with (proj_path / 'data' / 'description.json').open() as f:\n",
    "#     all_descriptions = json.load(f)\n",
    "# seed = 42\n",
    "# all_data = filter_samples_by_count_spider_bird(train_data+dev_data, n=10)\n",
    "\n",
    "# with open(proj_path / 'data' / 'bird_skip.txt') as f:\n",
    "#     skip = [int(line.strip()) for line in f]\n",
    "\n",
    "# bird_samples = process_samples_bird(all_data, bird_tables, skip=skip)\n",
    "# train_samples, dev_samples, test_samples = split_train_dev_test(bird_samples, train_ratio=0.6, dev_ratio=0.2, seed=seed)\n",
    "\n",
    "# save_samples_spider_bird(train_samples, proj_path / 'data' / 'bird_train.json')\n",
    "# save_samples_spider_bird(dev_samples, proj_path / 'data' / 'bird_dev.json')\n",
    "# save_samples_spider_bird(test_samples, proj_path / 'data' / 'bird_test.json')\n",
    "# print(len(train_samples), len(dev_samples), len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_folder = proj_path / 'experiments' / 'bird'\n",
    "# prediction_path = experiment_folder / 'predictions' / 'create_bo'\n",
    "# tables = bird_tables\n",
    "# bos = []\n",
    "# for p in prediction_path.glob('bird_train_bo_*.json'):\n",
    "#     with p.open() as f:\n",
    "#         bos = json.load(f)\n",
    "\n",
    "#     db_id = p.stem.split('_', 3)[-1]\n",
    "#     schema = Schema(tables[db_id].db_schema)\n",
    "#     for bo in bos:\n",
    "#         output = extract_all(bo['gold_sql'], schema)\n",
    "#         bo['gold_complexity'] = get_complexity(output)\n",
    "    \n",
    "#     with p.open('w') as f:\n",
    "#         json.dump(bos, f, indent=4)\n",
    "\n",
    "# bos = {}\n",
    "# for p in prediction_path.glob('bird_train_bo_*.json'):\n",
    "#     db_id = p.stem.split('_', 3)[-1]\n",
    "#     with p.open() as f:\n",
    "#         bos[db_id] = json.load(f)\n",
    "\n",
    "# with (experiment_folder / 'predictions' / 'create_bo' / f'final_bird_train_bo.json').open('w') as f:\n",
    "#     json.dump(bos, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_bo_sql import Sampler, get_vector_store, get_retriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "k_retrieval: int = 5  # for test\n",
    "n_retrieval: int = 1   # for test\n",
    "score_threshold: float = 0.65\n",
    "use_reranker: bool = True\n",
    "# TODO: run spider 4567\n",
    "ds = 'bird'\n",
    "task = 'zero_shot'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "\n",
    "bo_path = experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.json'\n",
    "with bo_path.open() as f:\n",
    "    bos = json.load(f)\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_dev.json')\n",
    "df = pd.read_csv(experiment_folder / 'evals' / 'zero_shot' / f'bird_dev.csv')\n",
    "df_score = df.loc[:, ['sample_id', 'db_id', 'exec_result']]\n",
    "df_error = df_score.loc[df_score['exec_result'] == 0, ['db_id', 'sample_id']]\n",
    "error_ids = df_error['sample_id'].tolist()\n",
    "samples = list(filter(lambda x: x.sample_id in error_ids, samples))\n",
    "\n",
    "\n",
    "samples_by_db_id = defaultdict(list)\n",
    "for sample in samples:\n",
    "    samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "cross_encoder = HuggingFaceCrossEncoder(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "vectorstore = get_vector_store(bos)\n",
    "# dev_samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_dev.json')\n",
    "# pred_res = defaultdict(dict)  # db_id -> train_bo -> list[dict]\n",
    "# for p in prediction_path.glob(f'{ds}_dev_*.json'):\n",
    "#     name = p.stem.split('_', 2)[-1]\n",
    "#     db_id, idx = name.split('-')\n",
    "#     with p.open() as f:\n",
    "# \n",
    "#     for r in res:\n",
    "#         train_bo_id = r['retrieved']\n",
    "#         if not pred_res[db_id].get(train_bo_id):\n",
    "#             pred_res[db_id][train_bo_id] = []\n",
    "#         pred_res[db_id][r['retrieved']].append(r)\n",
    "#     break\n",
    "\n",
    "# save_path = prediction_path / f'final_{ds}_dev.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = []\n",
    "\n",
    "for db_id, samples in samples_by_db_id.items():\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type='similarity_score_threshold',\n",
    "        search_kwargs={\n",
    "            'k': k_retrieval, \n",
    "            'score_threshold': score_threshold, \n",
    "            'filter': {'db_id': db_id, 'sample_id': {'$nin' : sample_ids}},\n",
    "        }\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7ff2c8db3e90>, search_type='similarity_score_threshold', search_kwargs={'k': 5, 'score_threshold': 0.65, 'filter': {'db_id': 'movie_platform', 'sample_id': {'$nin': []}}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3385365853658537"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(eval_path / f'bird_dev.csv')\n",
    "df_score = df.loc[:, ['sample_id', 'db_id', 'exec_result']]\n",
    "df_score['exec_result'].sum() / len(df_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "n_sample = 3\n",
    "n_stop = 50\n",
    "typ = 'dev'\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_{typ}.json')\n",
    "df = pd.read_csv(experiment_folder / 'evals' / 'zero_shot' / f'{ds}_dev.csv')\n",
    "df_score = df.loc[:, ['sample_id', 'db_id', 'exec_result']]\n",
    "df_error = df_score.loc[df_score['exec_result'] == 0, ['db_id', 'sample_id']]\n",
    "error_ids = df_error['sample_id'].tolist()\n",
    "samples = list(filter(lambda x: x.sample_id in error_ids, samples))\n",
    "\n",
    "with open(experiment_folder / f'partial_{ds}_db_ids.json') as f:\n",
    "    partial_db_ids = json.load(f)\n",
    "\n",
    "bo_path = experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.json'\n",
    "assert bo_path.exists(), 'Run with the `task=create_bo, type=train` first'\n",
    "with bo_path.open() as f:\n",
    "    bos = json.load(f)\n",
    "\n",
    "# with open(experiment_folder / f'partial_{ds}_db_ids.json', 'w') as f:\n",
    "#     json.dump(partial_db_ids, f, indent=4)\n",
    "\n",
    "sampler = Sampler(bos)\n",
    "\n",
    "sampled_bos = {}\n",
    "for db_id_group in partial_db_ids:\n",
    "    sampled_bos[str(db_id_group)] = defaultdict()\n",
    "    for db_id in partial_db_ids[str(db_id_group)]:\n",
    "        x_samples = list(filter(lambda x: x.db_id == db_id, samples))\n",
    "        for idx_bos, train_bos in enumerate(sampler.sample(db_id, n_sample, n_stop, rt_idx=False)):\n",
    "            # print(f'{db_id}-{idx_bos} :', f'{len(train_bos)}', f'{len(list(product(train_bos, x_samples)))}')\n",
    "            sampled_bos[str(db_id_group)][f'{db_id}-{idx_bos}'] = {\n",
    "                'train_bos': train_bos,\n",
    "                'n_iter': len(list(product(train_bos, x_samples))), \n",
    "                'total_bos_in_batch': len(train_bos),\n",
    "                'total_samples_in_batch': len(x_samples)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'bird'\n",
    "task = 'zero_shot'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "typ = 'dev'\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_{typ}.json')\n",
    "df = pd.read_csv(experiment_folder / 'evals' / 'zero_shot' / f'{ds}_dev.csv')\n",
    "df_error = df.loc[df['exec_result'] == 0]\n",
    "error_ids = df_error['sample_id'].tolist()\n",
    "samples = list(filter(lambda x: x.sample_id in error_ids, samples))\n",
    "\n",
    "with (experiment_folder / f'partial_{ds}_batch.json').open('r') as f:\n",
    "    partial_batch = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_files = 0\n",
    "for db_group_id, batch in partial_batch.items():\n",
    "    count_files += len(batch)\n",
    "count_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] before: file = 82 bos= 644 n_iter= 8394\n",
      "[0] after: file = 59 bos= 511 n_iter= 1957\n",
      "[1] before: file = 50 bos= 411 n_iter= 4583\n",
      "[1] after: file = 45 bos= 367 n_iter= 881\n",
      "[2] before: file = 62 bos= 517 n_iter= 6707\n",
      "[2] after: file = 46 bos= 396 n_iter= 1184\n",
      "[3] before: file = 75 bos= 690 n_iter= 10838\n",
      "[3] after: file = 52 bos= 505 n_iter= 1900\n",
      "[4] before: file = 68 bos= 561 n_iter= 6246\n",
      "[4] after: file = 53 bos= 470 n_iter= 1264\n",
      "[5] before: file = 64 bos= 508 n_iter= 6218\n",
      "[5] after: file = 50 bos= 409 n_iter= 1073\n",
      "[6] before: file = 70 bos= 573 n_iter= 7298\n",
      "[6] after: file = 52 bos= 436 n_iter= 1577\n",
      "[7] before: file = 70 bos= 583 n_iter= 8072\n",
      "[7] after: file = 50 bos= 434 n_iter= 1442\n"
     ]
    }
   ],
   "source": [
    "new_partial_batch = defaultdict()\n",
    "to_be = 30\n",
    "for db_id_group, batches in partial_batch.items():\n",
    "    new_batch = defaultdict(dict)\n",
    "    db_id_count = defaultdict(int)\n",
    "    for file_name, batch in batches.items():\n",
    "        db_id, idx = file_name.split('-')\n",
    "        x_samples = list(filter(lambda x: x.db_id == db_id, samples))\n",
    "        if db_id_count[db_id] >= to_be:\n",
    "            # print('drop ', file_name)\n",
    "            continue\n",
    "\n",
    "        if db_id_count[db_id] + len(batch['train_bos']) < to_be:\n",
    "            train_bos = batch['train_bos']\n",
    "            new_batch[file_name] = {\n",
    "                'train_bos': train_bos,\n",
    "                'n_iter': len(list(product(train_bos, x_samples))),\n",
    "                'total_bos_in_batch': len(train_bos),\n",
    "                'total_samples_in_batch': len(x_samples)\n",
    "            }\n",
    "            db_id_count[db_id] += len(train_bos)\n",
    "        else: # count + len(batch['train_bos']) > to_be:\n",
    "            n = to_be - db_id_count[db_id]\n",
    "            train_bos = batch['train_bos'][:n]\n",
    "            new_batch[file_name] = {\n",
    "                'train_bos': train_bos,\n",
    "                'n_iter': len(list(product(train_bos, x_samples))),\n",
    "                'total_bos_in_batch': len(train_bos),\n",
    "                'total_samples_in_batch': len(x_samples)\n",
    "            }\n",
    "            db_id_count[db_id] += len(train_bos)\n",
    "    \n",
    "    new_partial_batch[db_id_group] = new_batch\n",
    "    \n",
    "    print(f'[{db_id_group}] before: file = {len(batches)} bos=' , sum([len(v['train_bos']) for v in batches.values()]), 'n_iter=', sum([v['n_iter'] for v in batches.values()]))\n",
    "    print(f'[{db_id_group}] after: file = {len(new_batch)} bos=' , sum([len(v['train_bos']) for v in new_batch.values()]), 'n_iter=', sum([v['n_iter'] for v in new_batch.values()]))\n",
    "    print(f'[{db_id_group}] count = {db_id_count}')\n",
    "# with (experiment_folder / f'partial_{ds}_batch.json').open('w') as f:\n",
    "#     json.dump(new_partial_batch, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (experiment_folder / f'partial_{ds}_batch-back.json').open('r') as f:\n",
    "    partial_batch = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_store-0 before: n_retrieved = 12 | bos = 12 | 132  --> 60\n",
      "app_store-1 before: n_retrieved = 12 | bos = 12 | 132  --> 60\n",
      "app_store-2 before: n_retrieved = 6 | bos = 6 | 66  --> 30\n",
      "app_store-3 not found: 3, delete\n",
      "app_store-4 not found: 1, delete\n",
      "authors-0 before: n_retrieved = 15 | bos = 15 | 510  --> 255\n",
      "authors-1 before: n_retrieved = 15 | bos = 15 | 510  --> 255\n",
      "authors-2 not found: 15, delete\n",
      "authors-3 not found: 5, delete\n",
      "books-0 before: n_retrieved = 15 | bos = 15 | 585  --> 180\n",
      "books-1 before: n_retrieved = 15 | bos = 15 | 585  --> 180\n",
      "books-2 not found: 15, delete\n",
      "books-3 not found: 5, delete\n",
      "california_schools-0 before: n_retrieved = 15 | bos = 15 | 255  --> 225\n",
      "california_schools-1 before: n_retrieved = 15 | bos = 15 | 255  --> 225\n",
      "california_schools-2 not found: 13, delete\n",
      "california_schools-3 not found: 6, delete\n",
      "california_schools-4 not found: 1, delete\n",
      "college_completion-0 before: n_retrieved = 15 | bos = 15 | 225  --> 225\n",
      "college_completion-1 before: n_retrieved = 15 | bos = 15 | 225  --> 225\n",
      "college_completion-2 not found: 14, delete\n",
      "college_completion-3 not found: 1, delete\n",
      "computer_student-0 before: n_retrieved = 15 | bos = 15 | 210  --> 165\n",
      "computer_student-1 before: n_retrieved = 15 | bos = 15 | 210  --> 165\n",
      "computer_student-2 not found: 11, delete\n",
      "computer_student-3 not found: 2, delete\n",
      "genes-0 before: n_retrieved = 12 | bos = 12 | 48  --> 48\n",
      "genes-1 before: n_retrieved = 1 | bos = 1 | 4  --> 4\n",
      "human_resources-0 before: n_retrieved = 15 | bos = 15 | 165  --> 135\n",
      "human_resources-1 before: n_retrieved = 14 | bos = 14 | 154  --> 126\n",
      "human_resources-2 before: n_retrieved = 6 | bos = 1 | 66  --> 54\n",
      "menu-0 before: n_retrieved = 15 | bos = 15 | 315  --> 210\n",
      "menu-1 before: n_retrieved = 15 | bos = 15 | 315  --> 210\n",
      "menu-2 not found: 14, delete\n",
      "menu-3 not found: 6, delete\n",
      "music_tracker-0 before: n_retrieved = 12 | bos = 12 | 108  --> 48\n",
      "music_tracker-1 before: n_retrieved = 11 | bos = 11 | 99  --> 44\n",
      "music_tracker-2 before: n_retrieved = 4 | bos = 4 | 36  --> 16\n",
      "olympics-0 before: n_retrieved = 15 | bos = 15 | 495  --> 240\n",
      "olympics-1 before: n_retrieved = 15 | bos = 15 | 495  --> 240\n",
      "olympics-2 not found: 15, delete\n",
      "olympics-3 not found: 5, delete\n",
      "regional_sales-0 before: n_retrieved = 15 | bos = 15 | 480  --> 435\n",
      "regional_sales-1 before: n_retrieved = 15 | bos = 15 | 480  --> 435\n",
      "regional_sales-2 not found: 15, delete\n",
      "regional_sales-3 not found: 5, delete\n",
      "retail_world-0 before: n_retrieved = 12 | bos = 12 | 156  --> 48\n",
      "retail_world-1 before: n_retrieved = 11 | bos = 11 | 143  --> 44\n",
      "retail_world-2 before: n_retrieved = 4 | bos = 4 | 52  --> 16\n",
      "retail_world-3 before: n_retrieved = 3 | bos = 3 | 39  --> 12\n",
      "retail_world-4 not found: 3, delete\n",
      "retail_world-5 not found: 3, delete\n",
      "retail_world-6 not found: 3, delete\n",
      "retail_world-7 not found: 1, delete\n",
      "retails-0 before: n_retrieved = 12 | bos = 12 | 588  --> 312\n",
      "retails-1 before: n_retrieved = 12 | bos = 12 | 588  --> 312\n",
      "shakespeare-0 before: n_retrieved = 12 | bos = 12 | 264  --> 192\n",
      "shakespeare-1 before: n_retrieved = 12 | bos = 12 | 264  --> 192\n",
      "shakespeare-2 before: n_retrieved = 12 | bos = 6 | 264  --> 192\n",
      "shakespeare-3 not found: 7, delete\n",
      "shakespeare-4 not found: 6, delete\n",
      "shakespeare-5 not found: 1, delete\n",
      "shipping-0 before: n_retrieved = 15 | bos = 15 | 315  --> 135\n",
      "shipping-1 before: n_retrieved = 15 | bos = 15 | 315  --> 135\n",
      "shipping-2 not found: 14, delete\n",
      "shipping-3 not found: 6, delete\n",
      "student_loan-0 before: n_retrieved = 12 | bos = 12 | 480  --> 168\n",
      "student_loan-1 before: n_retrieved = 12 | bos = 12 | 480  --> 168\n",
      "student_loan-2 before: n_retrieved = 12 | bos = 6 | 480  --> 168\n",
      "student_loan-3 not found: 12, delete\n",
      "student_loan-4 not found: 2, delete\n",
      "talkingdata-0 before: n_retrieved = 15 | bos = 15 | 615  --> 270\n",
      "talkingdata-1 before: n_retrieved = 15 | bos = 15 | 615  --> 270\n",
      "talkingdata-2 not found: 15, delete\n",
      "talkingdata-3 not found: 5, delete\n",
      "trains-0 before: n_retrieved = 15 | bos = 15 | 120  --> 45\n",
      "trains-1 before: n_retrieved = 7 | bos = 7 | 56  --> 21\n",
      "trains-2 before: n_retrieved = 2 | bos = 2 | 16  --> 6\n",
      "world_development_indicators-0 before: n_retrieved = 12 | bos = 12 | 372  --> 324\n",
      "world_development_indicators-1 before: n_retrieved = 12 | bos = 12 | 372  --> 324\n",
      "world_development_indicators-2 before: n_retrieved = 12 | bos = 6 | 372  --> 324\n",
      "world_development_indicators-3 not found: 12, delete\n",
      "world_development_indicators-4 not found: 2, delete\n"
     ]
    }
   ],
   "source": [
    "# 돌려 놓은거 처리\n",
    "ds = 'bird'\n",
    "task = 'valid_bo'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "\n",
    "with (experiment_folder / f'partial_{ds}_batch.json').open('r') as f:\n",
    "    new_partial_batch = json.load(f)\n",
    "\n",
    "df = pd.read_csv(experiment_folder / 'evals' / 'zero_shot' / f'{ds}_dev.csv')\n",
    "df_error = df.loc[df['exec_result'] == 0]\n",
    "error_ids = df_error['sample_id'].tolist()\n",
    "count_db_ids = defaultdict(int)\n",
    "for p in sorted(prediction_path.glob(f'{ds}_dev_*.json')):\n",
    "    name = p.stem.split('_', 2)[-1]\n",
    "    db_id, idx = name.split('-')\n",
    "    # if name == 'app_store-2':\n",
    "    #     break\n",
    "    with p.open() as f:\n",
    "        res = json.load(f)\n",
    "    found = False\n",
    "    for db_id_group, batches in new_partial_batch.items():\n",
    "        if name in batches:\n",
    "            found = True\n",
    "            train_bo_ids = [x['sample_id'] for x in batches[name]['train_bos']]\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        n_retrieved = set([x['retrieved'] for x in res])\n",
    "        p.unlink()\n",
    "        print(f'{name} not found: {len(n_retrieved)}, delete')\n",
    "    else:\n",
    "        n_retrieved = set([x['retrieved'] for x in res])\n",
    "        count_db_ids[name] += len(n_retrieved)\n",
    "        print(f'{name} before: n_retrieved = {len(n_retrieved)} | bos = {len(train_bo_ids)} | {len(res)}', end=' ')\n",
    "        # error_ids 에 있는것만 남기기\n",
    "        res = list(filter(lambda x: x['sample_id'] in error_ids, res))\n",
    "        print(f' --> {len(res)}')\n",
    "        # train_bo_ids 에 해당하는 bos만 남기기\n",
    "        if len(res) < len(list(filter(lambda x: x['retrieved'] in train_bo_ids, res))):\n",
    "            print(f'{name} before reduce bo: {len(res)}')\n",
    "            res = list(filter(lambda x: x['retrieved'] in train_bo_ids, res))\n",
    "            print(f'{name} after reduce bo: {len(res)}')\n",
    "\n",
    "        with p.open('w') as f:\n",
    "            json.dump(res, f, indent=4)\n",
    "\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "db_ids = list(bos.keys())\n",
    "partial_db_ids = {}\n",
    "n = 20\n",
    "for i in range(30):\n",
    "    if db_ids[i*n:(i+1)*n]:\n",
    "        partial_db_ids[i] = db_ids[i*n:(i+1)*n]\n",
    "print(partial_db_ids.keys())\n",
    "\n",
    "with open(experiment_folder / f'partial_{ds}_db_ids.json', 'w') as f:\n",
    "    json.dump(partial_db_ids, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_folder / f'partial_{ds}_db_ids.json') as f:\n",
    "    partial_db_ids = json.load(f)\n",
    "\n",
    "sampler = Sampler(bos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, islice\n",
    "\n",
    "def batched(iterable, n, *, strict=False):\n",
    "    # batched('ABCDEFG', 3) → ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    iterator = iter(iterable)\n",
    "    while batch := tuple(islice(iterator, n)):\n",
    "        if strict and len(batch) != n:\n",
    "            raise ValueError('batched(): incomplete batch')\n",
    "        yield batch\n",
    "\n",
    "sampled_ids = {}\n",
    "for db_id_group in partial_db_ids:\n",
    "    sampled_ids[str(db_id_group)] = defaultdict()\n",
    "    for db_id in partial_db_ids[str(db_id_group)]:\n",
    "        x_samples = list(filter(lambda x: x.db_id == db_id, dev_samples))\n",
    "        for idx_bos, train_bos in enumerate(sampler.sample(db_id, 3, 50, rt_idx=False)):\n",
    "            # print(f'{db_id}-{idx_bos} :', f'{len(train_bos)}', f'{len(list(product(train_bos, x_samples)))}')\n",
    "            sampled_ids[str(db_id_group)][f'{db_id}-{idx_bos}'] = {\n",
    "                'train_bos': train_bos,\n",
    "                'n_iter': len(list(product(train_bos, x_samples))), \n",
    "                'total_bos_in_batch': len(train_bos)\n",
    "            }\n",
    "\n",
    "with (experiment_folder / f'partial_{ds}_batch.json').open('w') as f:\n",
    "    json.dump(sampled_ids, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "n_iter: 8394, iter per file: 102.37\n",
      "50\n",
      "n_iter: 4583, iter per file: 91.66\n",
      "62\n",
      "n_iter: 6707, iter per file: 108.18\n",
      "75\n",
      "n_iter: 10838, iter per file: 144.51\n",
      "68\n",
      "n_iter: 6246, iter per file: 91.85\n",
      "64\n",
      "n_iter: 6218, iter per file: 97.16\n",
      "70\n",
      "n_iter: 7298, iter per file: 104.26\n",
      "70\n",
      "n_iter: 8072, iter per file: 115.31\n"
     ]
    }
   ],
   "source": [
    "for db_id_group in partial_db_ids:\n",
    "    print(len(sampled_ids[str(db_id_group)]))\n",
    "    niters = [x['n_iter'] for x in sampled_ids[str(db_id_group)].values()]\n",
    "    print(f'n_iter: {sum(niters)}, iter per file: {np.mean(niters):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for db_id, bs in bos.items():\n",
    "    for b in bs:\n",
    "        res = {'db_id': db_id, 'gold_complexity': b['gold_complexity']}\n",
    "        df.append(res)\n",
    "\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import pairwise\n",
    "\n",
    "def _format_interval(x: pd.Interval):\n",
    "    return pd.Interval(\n",
    "        left=int(np.floor(x.left)), \n",
    "        right=int(np.floor(x.right)),\n",
    "        closed=x.closed\n",
    "    )\n",
    "\n",
    "def _get_categories(s: pd.Series):\n",
    "    tiles = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "    df = pd.qcut(s, q=tiles, duplicates='drop')\n",
    "    return df\n",
    "\n",
    "def _get_df_from_bos(bos):\n",
    "    df = []\n",
    "    for db_id, bs in bos.items():\n",
    "        for b in bs:\n",
    "            res = {'db_id': db_id}\n",
    "            res.update(b)\n",
    "            df.append(res)\n",
    "    df = pd.DataFrame(df)\n",
    "    df_cates = df.groupby('db_id')['gold_complexity'].apply(_get_categories)\n",
    "    df_cates = df_cates.rename('category').apply(_format_interval)\n",
    "    df = df.merge(df_cates.reset_index('db_id', drop=True), left_index=True, right_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'bird'\n",
    "task = 'zero_shot_hint'\n",
    "typ = 'dev'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "\n",
    "# file_name = f'{ds}_{typ}_parsed.pkl'\n",
    "# with (eval_path / file_name).open('rb') as f:\n",
    "#     target_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/simonjisu/code/BusinessObjects/experiments/bird')"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_path.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sample_id': 5156,\n",
       "  'vt': \"SELECT area_code.area_code, country.county FROM area_code INNER JOIN country AS T2 ON T1.zip_code = T2.zip_code INNER JOIN zip_data AS T3 ON T1.zip_code = T3.zip_code WHERE zip_data.city = '[placeholder-type:string]'\",\n",
       "  'ba': \"The virtual table provides the area code and county information for a specific city based on its zip code. It combines data from the 'area_code', 'country', and 'zip_data' tables, filtering results to match the specified city name.\",\n",
       "  'gold_complexity': 10,\n",
       "  'gold_sql': \"SELECT T1.area_code, T2.county FROM area_code AS T1 INNER JOIN country AS T2 ON T1.zip_code = T2.zip_code INNER JOIN zip_data AS T3 ON T1.zip_code = T3.zip_code WHERE T3.city = 'Savoy'\"},\n",
       " {'sample_id': 5211,\n",
       "  'vt': 'SELECT alias.alias FROM alias INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.population_2020 = (SELECT MAX(zip_data.population_2020) FROM zip_data)',\n",
       "  'ba': \"The virtual table retrieves the aliases of cities from the 'alias' table that correspond to the zip codes with the highest population recorded in 2020 from the 'zip_data' table. The query uses an inner join to connect the 'alias' and 'zip_data' tables based on the zip code, ensuring that only the aliases for the most populated areas are selected.\",\n",
       "  'gold_complexity': 11,\n",
       "  'gold_sql': 'SELECT T1.alias FROM alias AS T1 INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE T2.population_2020 = ( SELECT MAX(population_2020) FROM zip_data )'},\n",
       " {'sample_id': 5227,\n",
       "  'vt': \"SELECT zip_congress.district FROM zip_data INNER JOIN zip_congress AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.city = '[placeholder-type:string]'\",\n",
       "  'ba': \"The virtual table retrieves the district information associated with a specific city from the 'zip_data' table by joining it with the 'zip_congress' table. The placeholder in the WHERE clause represents the name of the city for which the district is being queried.\",\n",
       "  'gold_complexity': 7,\n",
       "  'gold_sql': \"SELECT T2.district FROM zip_data AS T1 INNER JOIN zip_congress AS T2 ON T1.zip_code = T2.zip_code WHERE T1.city = 'East Springfield'\"},\n",
       " {'sample_id': 5091,\n",
       "  'vt': \"SELECT COUNT(zip_data.zip_code) FROM zip_data INNER JOIN avoid AS T2 ON T1.zip_code = T2.zip_code WHERE avoid.bad_alias = '[placeholder-type:string]' AND zip_data.time_zone = '[placeholder-type:string]'\",\n",
       "  'ba': \"The virtual table counts the number of zip codes from the 'zip_data' table that are associated with bad aliases from the 'avoid' table. It filters the results based on a specific bad alias and a specified time zone.\",\n",
       "  'gold_complexity': 8,\n",
       "  'gold_sql': \"SELECT COUNT(T1.zip_code) FROM zip_data AS T1 INNER JOIN avoid AS T2 ON T1.zip_code = T2.zip_code WHERE T2.bad_alias = 'Internal Revenue Service' AND T1.time_zone = 'Eastern'\"}]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos['address'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 85\n",
      "\tPrompt Tokens: 51\n",
      "\tCompletion Tokens: 34\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $2.805e-05\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "\n",
    "class Out(BaseModel):\n",
    "    response: str\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    stream_usage=True,\n",
    ")\n",
    "model = llm.with_structured_output(Out)\n",
    "\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = model.invoke(\"Tell me a joke with JSON format\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.26582278481013 46.229123611557306 11 280\n"
     ]
    }
   ],
   "source": [
    "samples_by_db_id = defaultdict(list)\n",
    "for sample in train_samples:\n",
    "    samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "x = []\n",
    "for db_id, samples in samples_by_db_id.items():\n",
    "    x.append(len(samples))\n",
    "\n",
    "print(np.mean(x), np.std(x), np.min(x), np.max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.468354430379748 15.462355628942769 3 93\n"
     ]
    }
   ],
   "source": [
    "samples_by_db_id = defaultdict(list)\n",
    "for sample in dev_samples:\n",
    "    samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "x = []\n",
    "for db_id, samples in samples_by_db_id.items():\n",
    "    x.append(len(samples))\n",
    "\n",
    "print(np.mean(x), np.std(x), np.min(x), np.max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_parsed.pkl', 'rb') as f:\n",
    "#     train_parsed = pickle.load(f)\n",
    "\n",
    "# # prediction parsed\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'rb') as f:\n",
    "#     dev_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = proj_path / 'experiments' / 'bird' / 'evals' / 'zero_shot'\n",
    "\n",
    "df = []\n",
    "for p in eval_path.glob('bird_dev_*.json'):\n",
    "    with p.open() as f:\n",
    "        for line in f:\n",
    "            eval_data = json.loads(line)\n",
    "            df.append(eval_data)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv(eval_path / 'bird_dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean      0.450361\n",
       "std       0.055482\n",
       "min       0.318118\n",
       "max       0.726155\n",
       "median    0.446118\n",
       "Name: gold_complexity, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gold_complexity'].agg(['mean', 'std', 'min', 'max', 'median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_path = proj_path / 'experiments' / 'bird' / 'predictions' / 'create_bo'\n",
    "bos = defaultdict(list)\n",
    "for p in prediction_path.glob('bird_train_bo_*.json'):\n",
    "    with p.open() as f:\n",
    "        temp = json.load(f)\n",
    "    \n",
    "    bos[p.stem.split('_', 3)[-1]] = temp\n",
    "\n",
    "# with (prediction_path / 'final_bird_train_bo.json').open('w') as f:\n",
    "#     json.dump(bos, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = get_vector_store({'address': bos['address'][:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5156, 5211, 5227, 5091, 5152, 5128, 5200, 5119, 5194, 5141]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b['sample_id'] for b in bos['address'][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_id': 5156,\n",
       " 'vt': \"SELECT area_code.area_code, country.county FROM area_code INNER JOIN country AS T2 ON T1.zip_code = T2.zip_code INNER JOIN zip_data AS T3 ON T1.zip_code = T3.zip_code WHERE zip_data.city = '[placeholder-type:string]'\",\n",
       " 'ba': \"The virtual table provides the area code and county information for a specific city based on its zip code. It combines data from the 'area_code', 'country', and 'zip_data' tables, filtering results to match the specified city name.\"}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos['address'][:10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "base_retriever = vector_store.as_retriever(\n",
    "    search_type='similarity_score_threshold', \n",
    "    search_kwargs={\n",
    "        'k': 3,\n",
    "        'score_threshold': 0.3, 'filter': {'sample_id': {'$nin': []}}\n",
    "    }\n",
    ")\n",
    "\n",
    "# 'lambda_mult': 0.5  'score_threshold': 0.0\n",
    "# 'filter': {'sample_id': {'$in': [5156]}}}\n",
    "model = HuggingFaceCrossEncoder(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "compressor = CrossEncoderReranker(model=model, top_n=1)\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=base_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'what is the aliases of cities along with their elevation?'\n",
    "x = base_retriever.invoke(q)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = vector_store.similarity_search_with_relevance_scores(\n",
    "    q, k=2, filter={'sample_id': {'$nin': [5152, 5211, 5194]}})\n",
    "x\n",
    "# similarity_search_with_relevance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'sample_id': 5152, 'db_id': 'address', 'vt': 'SELECT alias.alias, zip_data.elevation FROM alias INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE alias.zip_code = [placeholder-type:numeric]'}, page_content=\"The virtual table describes the aliases of cities along with their elevation from the 'zip_data' table. The query joins the 'alias' table with the 'zip_data' table based on the zip code, filtering for a specific zip code using a placeholder for numeric values.\"),\n",
       "  0.7825041385389271),\n",
       " (Document(metadata={'sample_id': 5211, 'db_id': 'address', 'vt': 'SELECT alias.alias FROM alias INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.population_2020 = (SELECT MAX(zip_data.population_2020) FROM zip_data)'}, page_content=\"The virtual table retrieves the aliases of cities from the 'alias' table that correspond to the zip codes with the highest population recorded in 2020 from the 'zip_data' table. The query uses an inner join to connect the 'alias' and 'zip_data' tables based on the zip code, ensuring that only the aliases for the most populated areas are selected.\"),\n",
       "  0.7281931194919099),\n",
       " (Document(metadata={'sample_id': 5194, 'db_id': 'address', 'vt': \"SELECT avoid.bad_alias FROM avoid INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.city = '[placeholder-type:string]'\"}, page_content=\"The virtual table retrieves the bad aliases associated with a specific city from the 'avoid' table by joining it with the 'zip_data' table based on the zip code. The placeholder in the WHERE clause represents the name of the city for which we want to find bad aliases.\"),\n",
       "  0.691056772625688),\n",
       " (Document(metadata={'sample_id': 5227, 'db_id': 'address', 'vt': \"SELECT zip_congress.district FROM zip_data INNER JOIN zip_congress AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.city = '[placeholder-type:string]'\"}, page_content=\"The virtual table retrieves the district information associated with a specific city from the 'zip_data' table by joining it with the 'zip_congress' table. The placeholder in the WHERE clause represents the name of the city for which the district is being queried.\"),\n",
       "  0.6538844955711105)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_and_similarities = [\n",
    "    (doc, similarity)\n",
    "    for doc, similarity in x\n",
    "    if similarity >= 0.5\n",
    "]\n",
    "docs_and_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(bos: dict[str, list[dict[str, str]]]):\n",
    "    documents = []\n",
    "    for db_id, samples in bos.items():\n",
    "        for x in samples:\n",
    "            doc = Document(\n",
    "                doc_id=x['sample_id'],\n",
    "                page_content=x['ba'],\n",
    "                metadata={\n",
    "                    'sample_id': x['sample_id'],\n",
    "                    'db_id': db_id,\n",
    "                    'vt': x['vt']\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents, \n",
    "        embedding = embeddings_model,\n",
    "    )\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = get_vector_store(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sql_bo(\n",
    "    to_pred_samples: list[SpiderSample|BirdSample],\n",
    "    tables: dict[DatabaseModel],\n",
    "    vectorstore: FAISS,\n",
    "    chain: RunnableSequence,\n",
    "    prediction_path: Path,\n",
    "    file_name: str = '[args.ds]_[args.type]',\n",
    "    n_retrieval: int = 3,\n",
    "    score_threshold: float = 0.65,\n",
    "):\n",
    "    processed_db_ids = [p.stem.split('_')[-1] for p in prediction_path.glob(f'{file_name}_*')]\n",
    "    # restart from checkpoint\n",
    "    if processed_db_ids:\n",
    "        to_pred_samples = [sample for sample in to_pred_samples if sample.db_id not in processed_db_ids]\n",
    "    \n",
    "    samples_by_db_id = defaultdict(list)\n",
    "    for sample in to_pred_samples:\n",
    "        samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "    for db_id, samples in samples_by_db_id.items():\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_kwargs={'k': n_retrieval, 'score_threshold': score_threshold, 'filter': {'db_id': db_id}}\n",
    "        )\n",
    "        schema_str = get_schema_str(\n",
    "            schema=tables[db_id].db_schema, \n",
    "            foreign_keys=tables[db_id].foreign_keys,\n",
    "            col_explanation=tables[db_id].col_explanation\n",
    "        )\n",
    "        results = []\n",
    "        for sample in tqdm(samples, total=len(samples), desc=f\"{db_id}\"):\n",
    "            question = sample.final.question\n",
    "            docs = retriever.invoke(question)\n",
    "            hint = '\\nDescriptions and Virtual Tables:\\n'\n",
    "            hint += json.dumps({j: {'description': doc.page_content, 'virtual_table': doc.metadata['vt']} for j, doc in enumerate(docs)}, indent=4)\n",
    "            hint += '\\n'\n",
    "            input_data = {'schema': schema_str, 'input_query': question, 'hint': hint}\n",
    "            output = chain.invoke(input=input_data)\n",
    "            \n",
    "            full_sql_output = {}\n",
    "            full_sql_output['sample_id'] = sample.sample_id\n",
    "            full_sql_output['rationale'] = output.rationale\n",
    "            full_sql_output['pred_sql'] = output.full_sql_query\n",
    "            # full_sql_output = 1\n",
    "            results.append(full_sql_output)\n",
    "\n",
    "        with open(prediction_path / f'{file_name}_{db_id}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "movie_platform: 100%|██████████| 10/10 [00:04<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_bo.json', 'r') as f:\n",
    "#     bos = json.load(res, f, indent=4)\n",
    "# vectorstore = get_vector_store(bos)\n",
    "\n",
    "\n",
    "data_path = proj_path / 'data' / 'bird'\n",
    "experiment_folder = proj_path / 'experiments' / 'bird'\n",
    "prediction_path = experiment_folder / 'predictions' / 'zero_shot_hint'\n",
    "eval_path = experiment_folder / 'evals'\n",
    "for p in [prediction_path, eval_path]:\n",
    "    if not p.exists():\n",
    "        p.mkdir(parents=True)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=Prompts.zero_shot_hints_inference,\n",
    "    input_variables=['schema', 'input_query', 'hint'],\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0,\n",
    "    frequency_penalty=0.1,\n",
    ")\n",
    "\n",
    "model = model_openai.with_structured_output(SQLResponse)\n",
    "chain = (prompt | model)\n",
    "\n",
    "n_retrieval = 3\n",
    "score_threshold = 0.65\n",
    "\n",
    "predict_sql_bo(\n",
    "    to_pred_samples=dev_samples[:10],\n",
    "    tables=bird_tables,\n",
    "    vectorstore=vectorstore,\n",
    "    chain=chain,\n",
    "    prediction_path=prediction_path,\n",
    "    n_retrieval=n_retrieval,\n",
    "    score_threshold=score_threshold,\n",
    "    file_name='bird_dev',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptions and Virtual Tables:\n",
      "{\n",
      "    \"0\": {\n",
      "        \"description\": \"The virtual table retrieves the titles of movies that have been rated, filtering by a specific rating timestamp and grouping the results by movie title. The results are ordered by the count of ratings for each movie title, and a limit is applied to restrict the number of returned titles.\",\n",
      "        \"virtual_table\": \"SELECT movies.movie_title FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE ratings.rating_timestamp_utc LIKE '[placeholder-type:string]' GROUP BY movies.movie_title ORDER BY COUNT(movies.movie_title) LIMIT [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"1\": {\n",
      "        \"description\": \"The virtual table provides a count of users who have rated a specific movie, identified by its title, while also filtering for users who were trialists at the time of rating. It combines data from the 'ratings' and 'movies' tables to achieve this.\",\n",
      "        \"virtual_table\": \"SELECT COUNT(ratings.user_id) FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE movies.movie_title = '[placeholder-type:string]' AND ratings.user_trialist = [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"description\": \"The virtual table retrieves the titles of movies that have been rated, ordered by the number of likes received on the critics' comments. The query joins the 'ratings' table with the 'movies' table to access the movie titles associated with each rating. The result is limited to a specified number of entries, allowing users to see the most liked critics' comments for rated movies.\",\n",
      "        \"virtual_table\": \"SELECT movies.movie_title FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id ORDER BY ratings.critic_likes LIMIT [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"description\": \"The virtual table displays the titles of movies from the 'movies' table that have received a certain number of likes on user-written critiques. The query joins the 'ratings' table with the 'movies' table to filter movies based on the number of likes their critiques have received, using a placeholder for the minimum number of likes.\",\n",
      "        \"virtual_table\": \"SELECT movies.movie_title FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE ratings.critic_likes > [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"description\": \"The virtual table counts the number of ratings for a specific movie title from the 'movies' table, filtering by the movie's title and a specified rating timestamp. The placeholders represent the movie title and the date from which to count ratings.\",\n",
      "        \"virtual_table\": \"SELECT COUNT(ratings.rating_id) FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE movies.movie_title = '[placeholder-type:string]' AND ratings.rating_timestamp_utc >= '[placeholder-type:string]'\"\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(sample.final.question)\n",
    "hint = '\\nDescriptions and Virtual Tables:\\n'\n",
    "hint += json.dumps({j: {'description': doc.page_content, 'virtual_table': doc.metadata['vt']} for j, doc in enumerate(docs)}, indent=4)\n",
    "hint += '\\n'\n",
    "input_data = {'schema': db_schema, 'input_query': row['question'], 'hint': hint}\n",
    "output = chain.invoke(input=input_data)\n",
    "\n",
    "print(hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "movie_platform:   0%|          | 0/6341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debit_card_specializing: 100%|██████████| 6341/6341 [00:21<00:00, 299.00it/s]    \n",
      "debit_card_specializing: 100%|██████████| 2091/2091 [00:05<00:00, 408.98it/s]    \n",
      "debit_card_specializing: 100%|██████████| 2193/2193 [00:09<00:00, 225.64it/s]    \n"
     ]
    }
   ],
   "source": [
    "def get_parsed_sql(samples: dict, tables: dict):\n",
    "    error_ids = []\n",
    "    parsed = defaultdict(dict)\n",
    "    iterator = tqdm(samples, total=len(samples))\n",
    "    for sample in iterator:\n",
    "        db_id = sample.db_id\n",
    "        sample_id = sample.sample_id\n",
    "        iterator.set_description(f\"{db_id}\")\n",
    "        schema = Schema(tables[db_id].db_schema)\n",
    "        sql_i = sample.final.sql\n",
    "        try:\n",
    "            ei = extract_all(sql_i, schema)\n",
    "            assert len(ei['sel']) > 0, f'No selection found-{db_id}-{sample_id}'\n",
    "        except Exception as e:\n",
    "            error_ids.append((db_id, sample_id, str(e)))\n",
    "            parsed[db_id].append(None)\n",
    "            continue\n",
    "        parsed[db_id][sample_id] = ei\n",
    "    return parsed, error_ids\n",
    "\n",
    "train_parsed, error_ids = get_parsed_sql(train_samples, bird_tables)\n",
    "dev_parsed, error_ids = get_parsed_sql(dev_samples, bird_tables)\n",
    "test_parsed, error_ids = get_parsed_sql(test_samples, bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_parsed, f)\n",
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(dev_parsed, f)\n",
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_test_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_parsed, f)\n",
    "\n",
    "with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'rb') as f:\n",
    "    dev_parsed = pickle.load(f)\n",
    "\n",
    "with open(proj_path / 'data' / 'pkl_files' / 'bird_test_parsed.pkl', 'rb') as f:\n",
    "    test_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, product\n",
    "from collections import defaultdict\n",
    "from src.eval_utils import get_all_partial_score\n",
    "\n",
    "def measure_inter_score(parsed1: dict[str, tuple], parsed2: dict[str, tuple]):\n",
    "    results = defaultdict()\n",
    "    assert len(parsed1) == len(parsed2), f\"Length mismatch-1: {len(parsed1)} 2:{len(parsed2)}\"\n",
    "    db_ids = list(parsed1.keys())\n",
    "    for db_id in db_ids:\n",
    "        o1 = parsed1[db_id]\n",
    "        o2 = parsed2[db_id]\n",
    "        n1 = len(o1)\n",
    "        n2 = len(o2)\n",
    "        semantic_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "        structural_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "        overall_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "\n",
    "        idxs = list(product(range(n1), range(n2)))\n",
    "        iterator = tqdm(idxs, total=len(idxs), desc=f\"{db_id}\")\n",
    "        for i, j in iterator:\n",
    "            ei = o1[i]\n",
    "            ej = o2[j]\n",
    "\n",
    "            _, final_score = get_all_partial_score(ei, ej, use_bert=True)\n",
    "\n",
    "            structural_sim[i, j] = final_score['structural']\n",
    "            semantic_sim[i, j] = final_score['semantic']\n",
    "            overall_sim[i, j] = final_score['overall']\n",
    "\n",
    "        results[db_id] = {\n",
    "            'semantic': semantic_sim,\n",
    "            'struct': structural_sim,\n",
    "            'overall': overall_sim\n",
    "        }\n",
    "    return results\n",
    "\n",
    "results = measure_inter_score(dev_parsed, test_parsed)\n",
    "with (proj_path / 'data' / 'pkl_files' / 'bird_dev_test_similarity.pkl').open('wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity between datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6341/6341 [00:10<00:00, 631.17it/s]\n",
      "100%|██████████| 2091/2091 [00:03<00:00, 589.55it/s]\n",
      "100%|██████████| 2193/2193 [00:03<00:00, 591.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def measure_complexity(samples, tables):\n",
    "    cs = []\n",
    "    for s in tqdm(samples, total=len(samples)):\n",
    "        schema = Schema(tables[s.db_id].db_schema)\n",
    "        output = extract_all(s.final.sql, schema)\n",
    "        complexity = get_complexity(output)\n",
    "        cs.append(complexity)\n",
    "    return cs\n",
    "\n",
    "train_complexities = measure_complexity(train_samples, bird_tables)\n",
    "dev_complexities = measure_complexity(dev_samples, bird_tables)\n",
    "test_complexities = measure_complexity(test_samples, bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Mean=0.2753 +/-0.0476, Median=0.2710\n",
      "[dev  ] Mean=0.2758 +/-0.0471, Median=0.2710\n",
      "[test ] Mean=0.2760 +/-0.0477, Median=0.2709\n"
     ]
    }
   ],
   "source": [
    "for c, n in zip([train_complexities, dev_complexities, test_complexities], ['train', 'dev  ', 'test ']):\n",
    "    print(f'[{n}] Mean={np.mean(c):.4f} +/-{np.std(c):.4f}, Median={np.median(c):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = defaultdict(list)\n",
    "for s in dev_samples:\n",
    "    stats[s.db_id].append(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
