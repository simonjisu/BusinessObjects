{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simonjisu/code/BusinessObjects/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/simonjisu/code/BusinessObjects/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "proj_path = Path('.').resolve()\n",
    "sys.path.append(str(proj_path))\n",
    "\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from src.db_utils import get_schema_str, get_db_file\n",
    "from src.pymodels import (\n",
    "    DatabaseModel, \n",
    "    SpiderSample, \n",
    "    BirdSample, \n",
    "    BODescription,\n",
    "    SQLResponse,\n",
    "    KeywordExtractionResponse,\n",
    "    GenTemplateResponse\n",
    ")\n",
    "from src.prompts import Prompts\n",
    "from src.database import SqliteDatabase\n",
    "from src.data_preprocess import (\n",
    "    load_raw_data,\n",
    "    process_all_tables,\n",
    "    filter_samples_by_count_spider_bird,\n",
    "    process_samples_bird,\n",
    "    split_train_dev_test,\n",
    "    save_samples_spider_bird,\n",
    "    load_samples_spider_bird,\n",
    ")\n",
    "\n",
    "from src.parsing_sql import (\n",
    "    Schema, extract_all, extract_aliases, _format_expression, STRING_TYPE, NUMERIC_TYPE\n",
    ")\n",
    "from src.eval_utils import get_complexity, result_eq, check_if_exists_orderby\n",
    "from run_bo_sql import get_vector_store\n",
    "from copy import deepcopy\n",
    "\n",
    "from src.eval_utils import get_structural_score, get_all_structural_score, get_all_semantic_score, partial_matching_with_penalty\n",
    "from run_evaluation import get_target_parsed_sql, get_prediction_parsed_sql\n",
    "from run_bo_sql import _get_categories, _format_interval, get_retriever\n",
    "from bert_score import score as bscore\n",
    "from transformers import logging as tfloggings\n",
    "tfloggings.set_verbosity_error()\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.globals import set_llm_cache\n",
    "# from langchain_community.cache import SQLiteCache\n",
    "# set_llm_cache(SQLiteCache(database_path=f\"./cache/valid_bo_bird_dev.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "def get_content(x: dict):\n",
    "    return json.loads(x['kwargs']['message']['kwargs']['content'])\n",
    "task = 'direct'  \n",
    "# task = 'keyword_extraction'\n",
    "db_id = 'movie_3'\n",
    "prefix = 'x-'\n",
    "file_name = f'{task}_{db_id}'\n",
    "db = SqliteDatabase(f\"./cache/{prefix}{file_name}.db\")\n",
    "\n",
    "# df = db.execute(\"SELECT * FROM full_llm_cache WHERE ROWID = (SELECT MAX(ROWID) FROM full_llm_cache);\")\n",
    "df = db.execute(\"SELECT ROWID, response FROM full_llm_cache;\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hint_used': False,\n",
      " 'rationale': ['Identify the customer by name: We need to find the customer '\n",
      "               \"'Marcelia Goering' in the 'customer' table to get the \"\n",
      "               'customer_id.',\n",
      "               'Join the necessary tables: To find the orders placed by this '\n",
      "               \"customer, we need to join the 'customer', 'cust_order', and \"\n",
      "               \"'order_line' tables.\",\n",
      "               'Filter by year: We need to filter the orders that were placed '\n",
      "               'in the year 2021 by checking the order_date in the '\n",
      "               \"'cust_order' table.\",\n",
      "               'Filter by shipping method: We also need to ensure that the '\n",
      "               \"shipping method used is 'Priority Shipping' by joining with \"\n",
      "               \"the 'shipping_method' table.\"],\n",
      " 'sql': 'SELECT COUNT(*) FROM cust_order AS co INNER JOIN customer AS c ON '\n",
      "        'co.customer_id = c.customer_id INNER JOIN shipping_method AS sm ON '\n",
      "        \"co.shipping_method_id = sm.method_id WHERE c.first_name = 'Marcelia' \"\n",
      "        \"AND c.last_name = 'Goering' AND co.order_date LIKE '2021%' AND \"\n",
      "        \"sm.method_name = 'Priority Shipping'\"}\n"
     ]
    }
   ],
   "source": [
    "content = get_content(json.loads(df['response'].iloc[-3]))\n",
    "pprint(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowid</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>601</td>\n",
       "      <td>{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rowid                                           response\n",
       "599    601  {\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langc..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = '''To return latitude and longitude along with station details, I will select the lat and long columns from the station table.\n",
    "'''.strip()\n",
    "df.loc[df['response'].str.contains(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'601'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idxes = df['response'].apply(lambda x: True if get_content(json.loads(x)).get(' ') else False)\n",
    "# idxes = df['response'].str.contains(\"google_entity_id_id\")\n",
    "# idxes = ','.join(df.loc[idxes, 'rowid'].astype(str).tolist())\n",
    "# idxes\n",
    "idxes = ','.join(df.loc[df['response'].str.contains(x)].loc[:, 'rowid'].astype(str).values.tolist())\n",
    "idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'{task}_{db_id}'\n",
    "db = SqliteDatabase(f\"./cache/{prefix}{file_name}.db\")\n",
    "db.start()\n",
    "c = db.con.cursor()\n",
    "c.execute('BEGIN TRANSACTION')\n",
    "\n",
    "# remove the last row record\n",
    "c.execute(f\"\"\"\n",
    "DELETE FROM full_llm_cache\n",
    "WHERE ROWID IN ({idxes});\n",
    "\"\"\")\n",
    "db.con.commit()\n",
    "db.close()\n",
    "# (SELECT MAX(ROWID) FROM full_llm_cache)\n",
    "# JSONDecodeError\n",
    "# ValidationError\n",
    "# c.execute(\"\"\"\n",
    "# DELETE FROM full_llm_cache WHERE response LIKE '%JSONDecodeError%';\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "df = db.execute(\n",
    "'''\n",
    "SELECT * FROM full_llm_cache\n",
    "WHERE prompt LIKE '%Which teams have had a player awarded the Purple Cap and another with the Orange Cap%'\n",
    "''')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_p1 = 'no_bos' if not args.with_bos else 'with_bos'\n",
    "file_p2 = 'test' if args.type else 'dev'\n",
    "\n",
    "file_p1 = 'no_bos'\n",
    "file_p2 = 'dev'\n",
    "file_name = f'{file_p1}-{file_p2}.json'\n",
    "\n",
    "with open(prediction_path / file_name, 'r') as file:\n",
    "    all_results = json.load(file)\n",
    "    \n",
    "for r in all_results:\n",
    "    r['sql'] = r.pop('sql_template')\n",
    "\n",
    "with open(prediction_path / file_name, 'w') as file:\n",
    "    json.dump(all_results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval_utils import get_structural_score, get_all_structural_score, get_all_semantic_score, partial_matching_with_penalty\n",
    "from run_evaluation import get_target_parsed_sql, get_prediction_parsed_sql\n",
    "from run_bo_sql import _get_categories, _format_interval, get_retriever\n",
    "from bert_score import score as bscore\n",
    "from transformers import logging as tfloggings\n",
    "tfloggings.set_verbosity_error()\n",
    "import warnings\n",
    "\n",
    "ds = 'spider'\n",
    "task = 'zero_shot_hint'\n",
    "typ = 'test'\n",
    "scenario = 0\n",
    "description_file = f'description.json' if ds == 'spider' else f'{ds}_description.json'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bird_path = proj_path / 'data' / 'bird'\n",
    "# tables, train_data, dev_data = load_raw_data(bird_path, load_test=False)\n",
    "\n",
    "# with (proj_path / 'data' / 'bird_description.json').open() as f:\n",
    "#     all_descriptions = json.load(f)\n",
    "\n",
    "# bird_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "# train_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_train.json')\n",
    "# dev_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_dev.json')\n",
    "# test_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_post_fix = f'{ds}_{typ}' if scenario < 0 else f'{ds}_{typ}_{scenario}'\n",
    "# final_file = f'final_{file_post_fix}.json'\n",
    "# samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_{typ}.json')\n",
    "\n",
    "# if not (prediction_path / final_file).exists():\n",
    "#     all_results = []\n",
    "#     paths = sorted(list(prediction_path.glob(f'{file_post_fix}_*.json')))\n",
    "#     for p in paths:\n",
    "#         with p.open() as f:\n",
    "#             results = json.load(f)\n",
    "            \n",
    "#         for r in results:\n",
    "#             r.pop('rationale')\n",
    "#             r['db_id'] = p.stem.split('_', 3)[-1]\n",
    "\n",
    "#             found = False\n",
    "#             for s in samples:\n",
    "#                 if r['sample_id'] == s.sample_id:\n",
    "#                     found = True\n",
    "#                     break\n",
    "#             r['gold_sql'] = s.final.sql\n",
    "#             assert found, r['sample_id']\n",
    "\n",
    "#         all_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(prediction_path / final_file, 'r') as f:\n",
    "#     preds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_parsed, _ = get_prediction_parsed_sql(preds, tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'bird'\n",
    "typ = 'dev'\n",
    "task = 'retrieval' # 'zero_shot', 'retrieval', 'valid_bo'\n",
    "description_file = f'description.json' if ds == 'spider' else f'{ds}_description.json'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_{typ}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.json', 'r') as f:\n",
    "#     train_bo = json.load(f)\n",
    "\n",
    "# df = []\n",
    "# for db_id, xs in train_bo.items():\n",
    "#     for x in xs:\n",
    "#         x['db_id'] = db_id\n",
    "#         df.append(x)\n",
    "\n",
    "# df = pd.DataFrame(df)\n",
    "# cates = pd.qcut(df['gold_complexity'], q=5)\n",
    "# df['gold_complexity_cates'] = cates\n",
    "# df['gold_complexity_codes'] = cates.cat.codes\n",
    "# df.to_csv(experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.csv', index=False)\n",
    "# df = pd.read_csv(experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.csv')\n",
    "\n",
    "# df_train = df.groupby('gold_complexity_codes').sample(frac=0.9, random_state=42)\n",
    "# df_dev = df.drop(df_train.index)\n",
    "\n",
    "# print(df_train['gold_complexity_codes'].value_counts().sort_index())\n",
    "# print(df_dev['gold_complexity_codes'].value_counts().sort_index())\n",
    "\n",
    "# df_train.to_csv(experiment_folder / 'predictions' / 'create_bo' / f'{ds}_{task}_trainset.csv', index=False)\n",
    "# df_dev.to_csv(experiment_folder / 'predictions' / 'create_bo' / f'{ds}_{task}_devset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedures\n",
    "\n",
    "## task: valid_bo\n",
    "\n",
    "1. (retrieve) retrieve and rerank for dev dataset to get relevant train BOs\n",
    "2. (gen_template) generate sql template with dev dataset and retrieved BOs --> output: dev sql templates\n",
    "3. (keyword_extraction) extract keywords from dev sql templates --> output: keywords\n",
    "4. (search_value) search for the most helpful values for the dev sql templates --> output: columns with values\n",
    "5. (fill_in) fill-in values with dev sql templates --> output: dev sql\n",
    "6. (evaluate) evaluate the dev sql to get the most helpful train BOs --> output: BOs Pool\n",
    "\n",
    "## task: inference_with_bo\n",
    "\n",
    "1. (retrieve) retrieve and rerank for test dataset from BOs Pool\n",
    "2. (gen_template) generate sql template with test dataset and retrieved BOs --> output: test sql templates\n",
    "3. (keyword_extraction) extract keywords from dev sql templates --> output: keywords\n",
    "4. (search_value) search for the most helpful values for the dev sql templates --> output: columns with values\n",
    "5. (fill_in) fill-in values with test sql templates --> output: test sql\n",
    "6. (evaluate) evaluate the test sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieve analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# 1. modify args.type == 'dev' to evaluate on `n_bos_sample` \n",
    "# 2. run pipeline for `n_bos_sample` x `n_dev_sample` (implement langchain batch)\n",
    "# 3. run test set\n",
    "# 4. implement direct method and run (only with bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS():\n",
    "    exp_name = 'pipeline_exp'\n",
    "    ds = 'bird'\n",
    "    type = 'dev'\n",
    "    task = 'retrieve'\n",
    "    with_bos = True\n",
    "    \n",
    "args = ARGS()\n",
    "\n",
    "description_file = f'description.json' if args.ds == 'spider' else f'{args.ds}_description.json'\n",
    "experiment_folder = proj_path / args.exp_name / args.ds\n",
    "\n",
    "description_file = 'bird_description.json' if args.ds == 'bird' else 'description.json'\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / args.ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{args.ds}_{args.type}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['european_football_1', 'sales_in_weather', 'craftbeer', 'soccer_2016', 'restaurant', 'movie', 'olympics', 'language_corpus', 'app_store', 'sales', 'video_games', 'image_and_language', 'software_company', 'authors', 'movies_4', 'social_media', 'human_resources', 'regional_sales', 'computer_student', 'works_cycles', 'food_inspection_2', 'citeseer', 'bike_share_1', 'law_episode', 'cs_semester', 'legislator', 'world', 'cookbook', 'university', 'books', 'shipping', 'food_inspection', 'movie_platform', 'shakespeare', 'book_publishing_company', 'car_retails', 'mental_health_survey', 'hockey', 'music_platform_2', 'address', 'menu', 'professional_basketball', 'cars', 'synthea', 'genes', 'retails', 'talkingdata', 'beer_factory', 'chicago_crime', 'mondial_geo', 'student_loan', 'codebase_comments', 'retail_world', 'music_tracker', 'disney', 'college_completion', 'ice_hockey_draft', 'world_development_indicators', 'airline', 'retail_complains', 'trains', 'public_review_platform', 'donor', 'coinmarketcap', 'simpson_episodes', 'movie_3', 'shooting', 'superstore', 'movielens', 'debit_card_specializing', 'financial', 'formula_1', 'california_schools', 'card_games', 'european_football_2', 'thrombosis_prediction', 'toxicology', 'student_club', 'superhero', 'codebase_community'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_id = 'language_corpus'\n",
    "\n",
    "db_file = get_db_file(proj_path, 'bird', db_id)\n",
    "db = SqliteDatabase(db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>tbl_name</th>\n",
       "      <th>rootpage</th>\n",
       "      <th>sql</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>table</td>\n",
       "      <td>langs</td>\n",
       "      <td>langs</td>\n",
       "      <td>2</td>\n",
       "      <td>CREATE TABLE langs(lid    INTEGER PRIMARY KEY ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>table</td>\n",
       "      <td>sqlite_sequence</td>\n",
       "      <td>sqlite_sequence</td>\n",
       "      <td>5</td>\n",
       "      <td>CREATE TABLE sqlite_sequence(name,seq)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>table</td>\n",
       "      <td>pages</td>\n",
       "      <td>pages</td>\n",
       "      <td>6</td>\n",
       "      <td>CREATE TABLE pages(pid INTEGER PRIMARY KEY AUT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>table</td>\n",
       "      <td>words</td>\n",
       "      <td>words</td>\n",
       "      <td>8</td>\n",
       "      <td>CREATE TABLE words(wid INTEGER PRIMARY KEY AUT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>table</td>\n",
       "      <td>langs_words</td>\n",
       "      <td>langs_words</td>\n",
       "      <td>10</td>\n",
       "      <td>CREATE TABLE langs_words(lid INTEGER REFERENCE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>table</td>\n",
       "      <td>pages_words</td>\n",
       "      <td>pages_words</td>\n",
       "      <td>11</td>\n",
       "      <td>CREATE TABLE pages_words(pid INTEGER REFERENCE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>table</td>\n",
       "      <td>biwords</td>\n",
       "      <td>biwords</td>\n",
       "      <td>12</td>\n",
       "      <td>CREATE TABLE biwords(lid    INTEGER REFERENCES...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type             name         tbl_name  rootpage  \\\n",
       "0  table            langs            langs         2   \n",
       "1  table  sqlite_sequence  sqlite_sequence         5   \n",
       "2  table            pages            pages         6   \n",
       "3  table            words            words         8   \n",
       "4  table      langs_words      langs_words        10   \n",
       "5  table      pages_words      pages_words        11   \n",
       "6  table          biwords          biwords        12   \n",
       "\n",
       "                                                 sql  \n",
       "0  CREATE TABLE langs(lid    INTEGER PRIMARY KEY ...  \n",
       "1             CREATE TABLE sqlite_sequence(name,seq)  \n",
       "2  CREATE TABLE pages(pid INTEGER PRIMARY KEY AUT...  \n",
       "3  CREATE TABLE words(wid INTEGER PRIMARY KEY AUT...  \n",
       "4  CREATE TABLE langs_words(lid INTEGER REFERENCE...  \n",
       "5  CREATE TABLE pages_words(pid INTEGER REFERENCE...  \n",
       "6  CREATE TABLE biwords(lid    INTEGER REFERENCES...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.execute(\"SELECT * FROM sqlite_master WHERE type='table';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = db.execute(\"SELECT * FROM words;\", rt_pandas=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2764996"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'àbac', 242),\n",
       " (2, 'xinès', 16841),\n",
       " (3, 'llatí', 48700),\n",
       " (4, 'grec', 49897),\n",
       " (5, 'antic', 60220),\n",
       " (6, 'càlcul', 7039),\n",
       " (7, 'aritmètica', 1841),\n",
       " (8, 'nombre', 115938),\n",
       " (9, 'enter', 2174),\n",
       " (10, 'decimal', 1522),\n",
       " (11, 'base', 60713),\n",
       " (12, 'u', 25416),\n",
       " (13, 'desena', 2593),\n",
       " (14, 'suma', 7802),\n",
       " (15, 'resta', 48686),\n",
       " (16, 'multiplicació', 1895),\n",
       " (17, 'divisió', 71303),\n",
       " (18, 'gerbert', 201),\n",
       " (19, \"d'orlhac\", 267),\n",
       " (20, 'vic', 14000),\n",
       " (21, 'ripoll', 5749),\n",
       " (22, 'borrell', 3055),\n",
       " (23, 'ii', 164838),\n",
       " (24, 'barcelona', 272289),\n",
       " (25, 'neperià', 56),\n",
       " (26, 'john', 81745),\n",
       " (27, 'napier', 597),\n",
       " (28, 'amèrica', 47892),\n",
       " (29, 'precolombina', 722),\n",
       " (30, 'yupana', 18),\n",
       " (31, 'quipu', 61),\n",
       " (32, 'espanya', 90071),\n",
       " (33, 'nombres', 13860),\n",
       " (34, 'de', 19877053),\n",
       " (35, 'fibonacci', 483),\n",
       " (36, 'antiga', 32624),\n",
       " (37, 'xina', 50675),\n",
       " (38, 'japó', 49430),\n",
       " (39, 'sumer', 772),\n",
       " (40, 'sistema', 120714),\n",
       " (41, 'sexagesimal', 577),\n",
       " (42, 'heròdot', 2705),\n",
       " (43, 'egipte', 22327),\n",
       " (44, 'dinastia', 40266),\n",
       " (45, 'aquemènida', 1082),\n",
       " (46, 'arsàcides', 168),\n",
       " (47, 'imperi', 46286),\n",
       " (48, 'sassànida', 1837),\n",
       " (49, 'iran', 6467),\n",
       " (50, 'índia', 23318),\n",
       " (51, 'romà', 53465),\n",
       " (52, 'salamina', 1075),\n",
       " (53, 'han', 111805),\n",
       " (54, 'hexadecimal', 392),\n",
       " (55, 'suanpan', 59),\n",
       " (56, 'comunitat', 50781),\n",
       " (57, \"d'estats\", 3596),\n",
       " (58, 'independents', 10040),\n",
       " (59, 'calculadora', 706),\n",
       " (60, 'mecànica', 9970),\n",
       " (61, 'rússia', 58856),\n",
       " (62, 'édouard', 2218),\n",
       " (63, 'lucas', 5878),\n",
       " (64, 'inques', 762),\n",
       " (65, 'ètnia', 2311),\n",
       " (66, 'inca', 2953),\n",
       " (67, 'civilització', 6509),\n",
       " (68, 'maia', 3245),\n",
       " (69, 'vigesimal', 82),\n",
       " (70, 'dacsa', 749),\n",
       " (71, 'coma', 9633),\n",
       " (72, 'flotant', 1479),\n",
       " (73, 'olmeca', 482),\n",
       " (74, 'bastons', 1300),\n",
       " (75, 'recompte', 1221),\n",
       " (76, 'quítxua', 1770),\n",
       " (77, 'successió', 13325),\n",
       " (78, 'regle', 614),\n",
       " (79, 'soroban', 20),\n",
       " (80, 'abacista', 4),\n",
       " (81, 'cut', 1273),\n",
       " (82, 'the', 490795),\n",
       " (83, 'knot', 169),\n",
       " (84, 'del', 3824935),\n",
       " (85, 'abăcus', 1),\n",
       " (86, 'i', 8012535),\n",
       " (87, 'άβαξ', 1),\n",
       " (88, 'ακος', 1),\n",
       " (89, 'que', 3814205),\n",
       " (90, 'significa', 20856),\n",
       " (91, 'taula', 19930),\n",
       " (92, \"l'imperi\", 37433),\n",
       " (93, 'semiunça', 1),\n",
       " (94, 'quart', 17509),\n",
       " (95, \"d'unça\", 11),\n",
       " (96, 'terç', 6131),\n",
       " (97, 'vol', 64144),\n",
       " (98, 'dir', 78501),\n",
       " (99, 'unça', 222),\n",
       " (100, 'unitats', 58264),\n",
       " (101, 'desenes', 2715),\n",
       " (102, 'centenes', 144),\n",
       " (103, 'milers', 9575),\n",
       " (104, 'milions', 44452),\n",
       " (105, '算盤', 2),\n",
       " (106, 'そろばん', 2),\n",
       " (107, 'dalt', 15434),\n",
       " (108, \"l'àbac\", 142),\n",
       " (109, 'a', 7257608),\n",
       " (110, 'baix', 54930),\n",
       " (111, 'счёты', 1),\n",
       " (112, 'aquest', 324806),\n",
       " (113, 'filferro', 976),\n",
       " (114, 'és', 1618497),\n",
       " (115, 'normalment', 24233),\n",
       " (116, 'prop', 68193),\n",
       " (117, \"l'usuari\", 5869),\n",
       " (118, 'el', 6828459),\n",
       " (119, '5', 159986),\n",
       " (120, 'è', 54078),\n",
       " (121, '6', 134887),\n",
       " (122, 'gra', 3752),\n",
       " (123, \"d'un\", 293315),\n",
       " (124, 'milió', 5323),\n",
       " (125, 'si', 166445),\n",
       " (126, 'hi', 659093),\n",
       " (127, 'present', 20298),\n",
       " (128, 'singular', 4928),\n",
       " (129, '210.996', 1),\n",
       " (130, '20', 107659),\n",
       " (131, '1', 486052),\n",
       " (132, '2', 362694),\n",
       " (133, '3', 254978),\n",
       " (134, '4', 211057),\n",
       " (135, 'cada', 168185),\n",
       " (136, 'fila', 2758),\n",
       " (137, '91', 7146),\n",
       " (138, '182', 2369),\n",
       " (139, '273', 1434),\n",
       " (140, '364', 1138),\n",
       " (141, 'es', 1898935),\n",
       " (142, 'queda', 17014),\n",
       " (143, 'curt', 12277),\n",
       " (144, 'en', 4299417),\n",
       " (145, 'dia', 92916),\n",
       " (146, 'paraula', 21960),\n",
       " (147, 'eina', 6079),\n",
       " (148, 'compte', 20603),\n",
       " (149, 'una', 2040810),\n",
       " (150, 'per', 3070299),\n",
       " (151, 'al', 2105255),\n",
       " (152, 'manual', 7155),\n",
       " (153, \"d'operacions\", 3879),\n",
       " (154, 'aritmètiques', 300),\n",
       " (155, 'consisteix', 14231),\n",
       " (156, 'un', 2547145),\n",
       " (157, 'marc', 40347),\n",
       " (158, 'amb', 2195305),\n",
       " (159, 'filferros', 207),\n",
       " (160, 'paral', 24253),\n",
       " (161, 'lels', 2728),\n",
       " (162, 'on', 346312),\n",
       " (163, 'fan', 33803),\n",
       " (164, 'córrer', 4453),\n",
       " (165, 'boles', 2081),\n",
       " (166, \"s'hi\", 62671),\n",
       " (167, 'poden', 116990),\n",
       " (168, 'representar', 12915),\n",
       " (169, 'enters', 1895),\n",
       " (170, 'o', 885596),\n",
       " (171, 'decimals', 509),\n",
       " (172, 'fa', 139889),\n",
       " (173, 'servir', 46841),\n",
       " (174, 'la', 10051808),\n",
       " (175, 'fil', 6903),\n",
       " (176, 'representa', 20300),\n",
       " (177, 'les', 2759892),\n",
       " (178, 'etcètera', 1030),\n",
       " (179, 'generalment', 26309),\n",
       " (180, \"s'utilitza\", 16661),\n",
       " (181, 'efectuar', 2151),\n",
       " (182, 'operacions', 11797),\n",
       " (183, 'bàsiques', 3479),\n",
       " (184, 'més', 1113155),\n",
       " (185, 'dels', 1434371),\n",
       " (186, 'àbacs', 75),\n",
       " (187, 'basats', 4860),\n",
       " (188, 'pedres', 10282),\n",
       " (189, 'també', 577108),\n",
       " (190, \"s'han\", 42006),\n",
       " (191, 'desenvolupat', 12227),\n",
       " (192, 'els', 2188169),\n",
       " (193, 'primer', 233829),\n",
       " (194, 'va', 3431695),\n",
       " (195, 'ser', 964174),\n",
       " (196, 'inventar', 2388),\n",
       " (197, 'voltant', 47177),\n",
       " (198, \"l'any\", 323420),\n",
       " (199, '1000', 7392),\n",
       " (200, 'després', 420174),\n",
       " (201, \"d'haver\", 19949),\n",
       " (202, 'après', 1368),\n",
       " (203, 'sota', 121014),\n",
       " (204, 'protecció', 20374),\n",
       " (205, 'comte', 47566),\n",
       " (206, 'incorpora', 3371),\n",
       " (207, 'unes', 48203),\n",
       " (208, 'regletes', 15),\n",
       " (209, 'contenen', 8192),\n",
       " (210, 'multiplicar', 1198),\n",
       " (211, 'facilitant', 714),\n",
       " (212, 'aquesta', 266285),\n",
       " (213, 'operació', 11444),\n",
       " (214, 'requereix', 7192),\n",
       " (215, 'memorització', 153),\n",
       " (216, 'taules', 6874),\n",
       " (217, 'genaille', 5),\n",
       " (218, 'milloren', 574),\n",
       " (219, 'permeten', 10410),\n",
       " (220, 'llegir', 8274),\n",
       " (221, 'directament', 18296),\n",
       " (222, 'resultat', 31702),\n",
       " (223, 'sense', 159797),\n",
       " (224, 'fer', 228842),\n",
       " (225, 'sumes', 709),\n",
       " (226, 'intermèdies', 600),\n",
       " (227, \"l'amèrica\", 3581),\n",
       " (228, \"s'havien\", 12462),\n",
       " (229, 'instruments', 12482),\n",
       " (230, 'anomenats', 12678),\n",
       " (231, \"d'enregistrament\", 986),\n",
       " (232, 'dades', 116545),\n",
       " (233, 'anomenat', 50919),\n",
       " (234, 'seu', 781123),\n",
       " (235, 'funcionament', 11715),\n",
       " (236, 'desconeix', 3467),\n",
       " (237, 'perquè', 89419),\n",
       " (238, 'espanyols', 14299),\n",
       " (239, 'varen', 16220),\n",
       " (240, 'prohibir', 3605),\n",
       " (241, 'ne', 27178),\n",
       " (242, 'perseguir', 2604),\n",
       " (243, \"l'ús\", 31824),\n",
       " (244, 'ensenyament', 2775),\n",
       " (245, 'investigadors', 5707),\n",
       " (246, 'creuen', 5340),\n",
       " (247, 'feia', 26300),\n",
       " (248, 'emprant', 2686),\n",
       " (249, 'història', 185873),\n",
       " (250, 'èpoques', 4545),\n",
       " (251, 'molt', 221797),\n",
       " (252, 'primerenques', 906),\n",
       " (253, \"l'home\", 14807),\n",
       " (254, 'primitiu', 3633),\n",
       " (255, 'trobar', 56613),\n",
       " (256, 'materials', 20563),\n",
       " (257, 'idear', 1031),\n",
       " (258, 'comptar', 8000),\n",
       " (259, 'probable', 7708),\n",
       " (260, 'inici', 4685),\n",
       " (261, 'fos', 24320),\n",
       " (262, 'superfície', 39537),\n",
       " (263, 'plana', 26159),\n",
       " (264, 'movien', 412),\n",
       " (265, 'sobre', 340691),\n",
       " (266, 'línies', 21272),\n",
       " (267, 'dibuixades', 390),\n",
       " (268, 'pols', 7845),\n",
       " (269, 'avui', 45313),\n",
       " (270, 'tendeix', 2992),\n",
       " (271, 'pensar', 9476),\n",
       " (272, \"l'origen\", 14120),\n",
       " (273, 'troba', 134835),\n",
       " (274, \"d'aquest\", 86406),\n",
       " (275, 'instrument', 12197),\n",
       " (276, 'encara', 151382),\n",
       " (277, 'notable', 10583),\n",
       " (278, 'igual', 23633),\n",
       " (279, 'mesopotàmia', 4070),\n",
       " (280, 'sumeri', 500),\n",
       " (281, 'apareix', 36952),\n",
       " (282, 'període', 71930),\n",
       " (283, 'comprès', 1514),\n",
       " (284, 'entre', 534203),\n",
       " (285, 'anys', 467444),\n",
       " (286, '2700', 315),\n",
       " (287, '2300', 552),\n",
       " (288, 'ac', 111853),\n",
       " (289, 'era', 573420),\n",
       " (290, 'format', 50285),\n",
       " (291, 'successives', 2445),\n",
       " (292, 'columnes', 11155),\n",
       " (293, 'delimitaven', 76),\n",
       " (294, 'ordres', 10769),\n",
       " (295, 'magnitud', 8720),\n",
       " (296, 'empraven', 582),\n",
       " (297, 'no', 807357),\n",
       " (298, 'obstant', 49065),\n",
       " (299, 'diferents', 103052),\n",
       " (300, 'experts', 4168),\n",
       " (301, 'sostenen', 2414),\n",
       " (302, 'sumeris', 646),\n",
       " (303, 'podien', 15574),\n",
       " (304, 'haver', 78725),\n",
       " (305, 'utilitzat', 21909),\n",
       " (306, \"d'addició\", 199),\n",
       " (307, 'sostracció', 111),\n",
       " (308, 'tanmateix', 31981),\n",
       " (309, 'mecanisme', 7885),\n",
       " (310, 'resultava', 1037),\n",
       " (311, 'difícil', 14241),\n",
       " (312, \"d'utilitzar\", 3856),\n",
       " (313, 'càlculs', 2543),\n",
       " (314, 'complexos', 5219),\n",
       " (315, \"l'antic\", 30734),\n",
       " (316, \"l'historiador\", 5915),\n",
       " (317, 'esmenta', 4543),\n",
       " (318, 'descriu', 11474),\n",
       " (319, 'egipcis', 3897),\n",
       " (320, 'manipulaven', 46),\n",
       " (321, 'còdols', 1367),\n",
       " (322, 'dreta', 28375),\n",
       " (323, 'esquerra', 17554),\n",
       " (324, 'contraposició', 1513),\n",
       " (325, 'direcció', 52828),\n",
       " (326, 'emprada', 2811),\n",
       " (327, 'pels', 190348),\n",
       " (328, 'grecs', 10919),\n",
       " (329, 'trobat', 17594),\n",
       " (330, 'diverses', 98211),\n",
       " (331, 'restes', 29886),\n",
       " (332, 'arqueològiques', 4776),\n",
       " (333, 'forma', 164793),\n",
       " (334, 'discs', 3301),\n",
       " (335, 'mides', 4007),\n",
       " (336, 'fet', 140710),\n",
       " (337, 'porta', 58497),\n",
       " (338, 'als', 518879),\n",
       " (339, 'arqueòlegs', 1709),\n",
       " (340, 'foren', 59434),\n",
       " (341, 'emprats', 2545),\n",
       " (342, 'com', 1326249),\n",
       " (343, 'comptes', 11186),\n",
       " (344, 'descripcions', 2375),\n",
       " (345, 'paret', 9944),\n",
       " (346, 'suscita', 165),\n",
       " (347, 'algun', 17647),\n",
       " (348, 'dubte', 7596),\n",
       " (349, \"l'abast\", 2567),\n",
       " (350, 'utilització', 6679),\n",
       " (351, 'pèrsia', 7919),\n",
       " (352, 'durant', 328213),\n",
       " (353, '600', 7507),\n",
       " (354, 'perses', 4941),\n",
       " (355, 'començaren', 3549),\n",
       " (356, 'estendre', 7847),\n",
       " (357, 'imperis', 1162),\n",
       " (358, 'iranians', 671),\n",
       " (359, 'estudiosos', 3666),\n",
       " (360, \"d'aquests\", 31266),\n",
       " (361, 'intercanviarien', 5),\n",
       " (362, 'coneixements', 6339),\n",
       " (363, 'invencions', 506),\n",
       " (364, 'pobles', 30812),\n",
       " (365, 'veïns', 12001),\n",
       " (366, 'considera', 23561),\n",
       " (367, 'intercanvi', 2187),\n",
       " (368, 'coneixement', 16401),\n",
       " (369, 'tecnologia', 19885),\n",
       " (370, 'portà', 3417),\n",
       " (371, \"l'expansió\", 5379),\n",
       " (372, 'països', 83981),\n",
       " (373, 'xifres', 3486),\n",
       " (374, 'representen', 9295),\n",
       " (375, 'posició', 44028),\n",
       " (376, 'conjunt', 56155),\n",
       " (377, 'indicadors', 1132),\n",
       " (378, 'civilitzacions', 2249),\n",
       " (379, 'adaptat', 4773),\n",
       " (380, 'general', 176217),\n",
       " (381, 'numeració', 2490),\n",
       " (382, 'posicional', 383),\n",
       " (383, 'quinària', 7),\n",
       " (384, 'diferències', 10460),\n",
       " (385, 'uns', 100430),\n",
       " (386, 'altres', 300344),\n",
       " (387, 'troben', 38706),\n",
       " (388, 'estan', 65099),\n",
       " (389, 'unides', 45432),\n",
       " (390, 'són', 360235),\n",
       " (391, 'deu', 32491),\n",
       " (392, 'dotze', 11914),\n",
       " (393, 'cap', 321134),\n",
       " (394, 'desplacen', 710),\n",
       " (395, 'adoptar', 8705),\n",
       " (396, 'valors', 15947),\n",
       " (397, 'significatius', 2405),\n",
       " (398, 'cas', 70969),\n",
       " (399, 'permetre', 23218),\n",
       " (400, 'emprar', 4914),\n",
       " (401, '16', 94175),\n",
       " (402, 'aquests', 91389),\n",
       " (403, 'gairebé', 40946),\n",
       " (404, 'immediates', 374),\n",
       " (405, 'afegint', 3720),\n",
       " (406, 'traient', 672),\n",
       " (407, \"l'altre\", 18884),\n",
       " (408, 'arranjant', 51),\n",
       " (409, 'multiplicacions', 117),\n",
       " (410, 'basen', 2654),\n",
       " (411, 'sumar', 2755),\n",
       " (412, 'repetides', 1067),\n",
       " (413, 'vegades', 63169),\n",
       " (414, 'però', 403868),\n",
       " (415, 'aprofitant', 5226),\n",
       " (416, 'propietats', 16031),\n",
       " (417, 'redueix', 3730),\n",
       " (418, 'així', 162822),\n",
       " (419, 'desplaçar', 3560),\n",
       " (420, 'columna', 12323),\n",
       " (421, 'cinc', 81724),\n",
       " (422, 'mitja', 7922),\n",
       " (423, 'arranjar', 565),\n",
       " (424, '8', 129800),\n",
       " (425, '10', 134747),\n",
       " (426, 'restar', 4475),\n",
       " (427, 'doble', 23168),\n",
       " (428, 'pel', 545954),\n",
       " (429, 'divisions', 6783),\n",
       " (430, 'repetidament', 1497),\n",
       " (431, 'dividend', 267),\n",
       " (432, 'divisor', 719),\n",
       " (433, 'quantes', 2312),\n",
       " (434, 'pot', 178222),\n",
       " (435, \"l'operació\", 7644),\n",
       " (436, 'principis', 37705),\n",
       " (437, \"l'anterior\", 10308),\n",
       " (438, 'reduir', 11363),\n",
       " (439, 'moviments', 17419),\n",
       " (440, 'requereixen', 3538),\n",
       " (441, 'empren', 880),\n",
       " (442, 'símbols', 7161),\n",
       " (443, 'cal', 63741),\n",
       " (444, 'aprendre', 6716),\n",
       " (445, 'los', 105512),\n",
       " (446, 'marbre', 6846),\n",
       " (447, 'trobada', 10279),\n",
       " (448, '1846', 6212),\n",
       " (449, 'està', 207211),\n",
       " (450, 'datat', 3204),\n",
       " (451, '300', 13303),\n",
       " (452, '150', 10517),\n",
       " (453, '75', 12505),\n",
       " (454, '4,5', 2656),\n",
       " (455, 'cm', 44884),\n",
       " (456, 'apareixen', 15593),\n",
       " (457, 'grups', 47001),\n",
       " (458, 'marques', 8799),\n",
       " (459, 'tres', 213322),\n",
       " (460, 'conjunts', 6221),\n",
       " (461, 'arranjats', 106),\n",
       " (462, 'llarg', 67936),\n",
       " (463, 'vores', 2949),\n",
       " (464, 'davall', 22687),\n",
       " (465, 'acrofònic', 13),\n",
       " (466, 'centre', 110116),\n",
       " (467, 'leles', 2655),\n",
       " (468, 'dividides', 1499),\n",
       " (469, 'mig', 45669),\n",
       " (470, 'línia', 71531),\n",
       " (471, 'vertical', 7525),\n",
       " (472, 'coberta', 26675),\n",
       " (473, \"d'una\", 353991),\n",
       " (474, 'semicircumferència', 395),\n",
       " (475, 'intersecció', 1663),\n",
       " (476, 'horitzontal', 4554),\n",
       " (477, 'escletxa', 400),\n",
       " (478, 'àmplia', 8917),\n",
       " (479, 'ha', 485624),\n",
       " (480, 'altre', 64023),\n",
       " (481, 'grup', 156725),\n",
       " (482, \"d'onze\", 1421),\n",
       " (483, 'aquestes', 71684),\n",
       " (484, 'divideixen', 2316),\n",
       " (485, 'dues', 191005),\n",
       " (486, 'seccions', 8131),\n",
       " (487, 'perpendicular', 3200),\n",
       " (488, 'elles', 13632),\n",
       " (489, 'part', 340297),\n",
       " (490, 'superior', 56600),\n",
       " (491, 'terceres', 1044),\n",
       " (492, 'sisenes', 55),\n",
       " (493, 'novenes', 42),\n",
       " (494, \"d'aquestes\", 24731),\n",
       " (495, 'marcades', 1353),\n",
       " (496, 'creu', 55639),\n",
       " (497, 'punt', 88396),\n",
       " (498, \"s'encreuen\", 37),\n",
       " (499, 'tauleta', 1169),\n",
       " (500, 'solcs', 486),\n",
       " (501, 'identificats', 1986),\n",
       " (502, 's', 148474),\n",
       " (503, 'z', 15510),\n",
       " (504, 'seguint', 15319),\n",
       " (505, \"l'esquerra\", 15598),\n",
       " (506, 'dos', 279477),\n",
       " (507, 'símbol', 13437),\n",
       " (508, 'θ', 738),\n",
       " (509, 'llavors', 72248),\n",
       " (510, 'venen', 3339),\n",
       " (511, '7', 121754),\n",
       " (512, 'identificades', 866),\n",
       " (513, 'successivament', 3267),\n",
       " (514, 'x', 83641),\n",
       " (515, 'c', 134475),\n",
       " (516, 'ci', 960),\n",
       " (517, 'cci', 105),\n",
       " (518, 'ccci', 3),\n",
       " (519, 'corresponents', 5241),\n",
       " (520, 'des', 358159),\n",
       " (521, 'fins', 374464),\n",
       " (522, 'conté', 27702),\n",
       " (523, 'quatre', 125700),\n",
       " (524, 'pedretes', 74),\n",
       " (525, 'damunt', 15462),\n",
       " (526, 'té', 206086),\n",
       " (527, 'xifra', 4555),\n",
       " (528, \"l'1\", 28082),\n",
       " (529, 'desplaça', 1309),\n",
       " (530, 'amunt', 5872),\n",
       " (531, 'corresponent', 8133),\n",
       " (532, 'solc', 655),\n",
       " (533, 'pedra', 53427),\n",
       " (534, '9', 106512),\n",
       " (535, 'deixant', 11780),\n",
       " (536, 'desplaçant', 697),\n",
       " (537, 'tantes', 1597),\n",
       " (538, 'passen', 5281),\n",
       " (539, 'unces', 260),\n",
       " (540, 'serveix', 9109),\n",
       " (541, 'fraccions', 1379),\n",
       " (542, 'duodecimals', 1),\n",
       " (543, \"n'hi\", 34277),\n",
       " (544, 'dotzena', 2586),\n",
       " (545, 'sis', 49520),\n",
       " (546, 'dotzenes', 924),\n",
       " (547, 'parts', 35295),\n",
       " (548, 'onze', 5974),\n",
       " (549, 'cosa', 71443),\n",
       " (550, 'mateix', 160776),\n",
       " (551, '24', 100525),\n",
       " (552, \"d'unitat\", 2601),\n",
       " (553, '48', 14653),\n",
       " (554, 'permet', 32221),\n",
       " (555, 'terços', 2014),\n",
       " (556, '36', 22007),\n",
       " (557, \"d'origen\", 27141),\n",
       " (558, 'descrit', 8122),\n",
       " (559, 'primera', 221413),\n",
       " (560, 'vegada', 69988),\n",
       " (561, 'llibre', 62403),\n",
       " (562, '190', 3314),\n",
       " (563, 'titulat', 7974),\n",
       " (564, 'notes', 32775),\n",
       " (565, 'suplementàries', 143),\n",
       " (566, \"l'art\", 26063),\n",
       " (567, 'escrit', 22295),\n",
       " (568, 'xu', 1210),\n",
       " (569, 'yue', 616),\n",
       " (570, 'disseny', 30923),\n",
       " (571, 'exacte', 1856),\n",
       " (572, 'coneix', 23004),\n",
       " (573, 'similitud', 1692),\n",
       " (574, 'suggereix', 4982),\n",
       " (575, 'podria', 32289),\n",
       " (576, 'inspirat', 4669),\n",
       " (577, 'ja', 217447),\n",
       " (578, 'alguna', 16330),\n",
       " (579, 'evidència', 3435),\n",
       " (580, 'relació', 47393),\n",
       " (581, 'comercial', 23562),\n",
       " (582, 'demostrar', 10308),\n",
       " (583, 'connexió', 7038),\n",
       " (584, 'directa', 9422),\n",
       " (585, \"l'abaci\", 2),\n",
       " (586, 'fortuïta', 146),\n",
       " (587, 'sorgint', 487),\n",
       " (588, 'dits', 4587),\n",
       " (589, 'mà', 23338),\n",
       " (590, 'model', 39696),\n",
       " (591, 'tenen', 78854),\n",
       " (592, 'grans', 88172),\n",
       " (593, 'lloc', 161301),\n",
       " (594, 'versió', 47713),\n",
       " (595, 'permetent', 4072),\n",
       " (596, 'menys', 94174),\n",
       " (597, 'algoritmes', 520),\n",
       " (598, 'aritmètics', 102),\n",
       " (599, 'utilitzar', 30690),\n",
       " (600, 'lo', 75990),\n",
       " (601, 'models', 16428),\n",
       " (602, 'xinesos', 5516),\n",
       " (603, 'japonesos', 5690),\n",
       " (604, 'corren', 796),\n",
       " (605, 'presumiblement', 896),\n",
       " (606, 'fent', 30695),\n",
       " (607, 'lents', 5186),\n",
       " (608, 'tipus', 104153),\n",
       " (609, 'barra', 5025),\n",
       " (610, 'separadors', 72),\n",
       " (611, \"d'aquesta\", 87870),\n",
       " (612, \"s'anomenen\", 3463),\n",
       " (613, 'terra', 84233),\n",
       " (614, \"d'aigua\", 31597),\n",
       " (615, 'valor', 28937),\n",
       " (616, \"d'1\", 10796),\n",
       " (617, 'seva', 716281),\n",
       " (618, 'cel', 26994),\n",
       " (619, 'llocs', 37001),\n",
       " (620, 'centenars', 6914),\n",
       " (621, 'dècimes', 208),\n",
       " (622, 'centèsimes', 122),\n",
       " (623, 'tals', 6818),\n",
       " (624, 'estrictament', 2958),\n",
       " (625, 'necessita', 5294),\n",
       " (626, 'només', 144441),\n",
       " (627, 'inferiors', 7194),\n",
       " (628, 'extres', 789),\n",
       " (629, 'podrien', 11522),\n",
       " (630, 'hexadecimals', 49),\n",
       " (631, 'alguns', 112222),\n",
       " (632, 'mètodes', 12683),\n",
       " (633, 'antics', 21913),\n",
       " (634, 'dividir', 7915),\n",
       " (635, 'utilitzen', 8403),\n",
       " (636, 'aquells', 18150),\n",
       " (637, 'tècnica', 26321),\n",
       " (638, 'extra', 3277),\n",
       " (639, 'suspès', 1220),\n",
       " (640, 'japonès', 14002),\n",
       " (641, 'evolució', 11450),\n",
       " (642, 'importat', 508),\n",
       " (643, 'segle', 87223),\n",
       " (644, 'xv', 13398),\n",
       " (645, 'muromachi', 182),\n",
       " (646, 'prendre', 38970),\n",
       " (647, 'actual', 31269),\n",
       " (648, '1930', 33903),\n",
       " (649, 'implica', 6262),\n",
       " (650, 'mínim', 10935),\n",
       " (651, 'requerides', 351),\n",
       " (652, 'sola', 17165),\n",
       " (653, \"d'unàries\", 2),\n",
       " (654, 'almenys', 14251),\n",
       " (655, 'quinzena', 1521),\n",
       " (656, 'arribar', 80270),\n",
       " (657, 'tenir', 125468),\n",
       " (658, '21', 71603),\n",
       " (659, '23', 70811),\n",
       " (660, '27', 61693),\n",
       " (661, '31', 45900),\n",
       " (662, 'rus', 39243),\n",
       " (663, 'sxioti', 1),\n",
       " (664, 'inclinada', 917),\n",
       " (665, 'única', 14828),\n",
       " (666, 'excepte', 13322),\n",
       " (667, 'ruble', 439),\n",
       " (668, 'vells', 3523),\n",
       " (669, 'quarts', 5678),\n",
       " (670, 'copec', 20),\n",
       " (671, 'encunyar', 1335),\n",
       " (672, '1916', 16467),\n",
       " (673, 'sovint', 52308),\n",
       " (674, 'verticalment', 1103),\n",
       " (675, \"d'esquerra\", 5486),\n",
       " (676, 'manera', 141114),\n",
       " (677, 'dobleguen', 52),\n",
       " (678, 'bony', 2252),\n",
       " (679, 'tendeixin', 62),\n",
       " (680, 'se', 435903),\n",
       " (681, 'qualsevol', 44258),\n",
       " (682, 'costats', 8478),\n",
       " (683, 'posa', 8857),\n",
       " (684, 'zero', 9057),\n",
       " (685, 'quan', 241004),\n",
       " (686, 'tots', 128514),\n",
       " (687, 'manipulació', 2127),\n",
       " (688, 'mouen', 1733),\n",
       " (689, 'facilitar', 6042),\n",
       " (690, 'visualització', 1617),\n",
       " (691, 'mitjans', 35047),\n",
       " (692, 'color', 60733),\n",
       " (693, 'diferent', 21350),\n",
       " (694, 'vuit', 24773),\n",
       " (695, 'mateixa', 62772),\n",
       " (696, 'esquerre', 6408),\n",
       " (697, 'simple', 13826),\n",
       " (698, 'barat', 1098),\n",
       " (699, 'fiable', 1603),\n",
       " (700, 'ús', 25181),\n",
       " (701, 'totes', 93118),\n",
       " (702, 'botigues', 6663),\n",
       " (703, 'mercats', 4534),\n",
       " (704, 'tota', 62335),\n",
       " (705, 'unió', 88342),\n",
       " (706, 'soviètica', 30809),\n",
       " (707, \"s'ensenyava\", 174),\n",
       " (708, 'moltes', 50975),\n",
       " (709, 'escoles', 17987),\n",
       " (710, '1990', 66202),\n",
       " (711, 'ni', 45034),\n",
       " (712, 'tan', 43421),\n",
       " (713, 'sols', 41425),\n",
       " (714, 'invenció', 3253),\n",
       " (715, '1874', 8408),\n",
       " (716, 'havia', 428905),\n",
       " (717, 'reemplaçat', 2563),\n",
       " (718, 'començar', 85485),\n",
       " (719, 'perdre', 22159),\n",
       " (720, 'popularitat', 8183),\n",
       " (721, 'producció', 53526),\n",
       " (722, 'sèrie', 100202),\n",
       " (723, 'calculadores', 400),\n",
       " (724, 'electròniques', 1014),\n",
       " (725, '1974', 34831),\n",
       " (726, 'arcaic', 1111),\n",
       " (727, 'estat', 218140),\n",
       " (728, 'substituït', 7665),\n",
       " (729, 'mitjançant', 50524),\n",
       " (730, 'sèries', 14573),\n",
       " (731, 'dígits', 1348),\n",
       " (732, 'saber', 13908),\n",
       " (733, 'quina', 4476),\n",
       " (734, 'correspon', 12755),\n",
       " (735, \"s'aconsegueixen\", 363),\n",
       " (736, 'movent', 1162),\n",
       " (737, 'afegir', 12101),\n",
       " (738, \"traure'n\", 4),\n",
       " (739, 'generen', 1887),\n",
       " (740, 'noves', 30363),\n",
       " (741, 'componen', 2837),\n",
       " (742, 'partir', 114333),\n",
       " (743, 'nous', 31038),\n",
       " (744, 'manipulant', 143),\n",
       " (745, 'casos', 28155),\n",
       " (746, 'mecanismes', 4824),\n",
       " (747, 'faciliten', 828),\n",
       " (748, '967', 743),\n",
       " (749, '970', 1077),\n",
       " (750, 'estudiar', 33574),\n",
       " (751, 'persona', 53504),\n",
       " (752, 'llatina', 11001),\n",
       " (753, 'abacus', 98),\n",
       " (754, 'detall', 26632),\n",
       " (755, 'deixeble', 6175),\n",
       " (756, 'bernelinus', 1),\n",
       " (757, \"l'explica\", 62),\n",
       " (758, 'obra', 91450),\n",
       " (759, 'regula', 1832),\n",
       " (760, 'abaco', 48),\n",
       " (761, 'computi', 5),\n",
       " (762, 'què', 90328),\n",
       " (763, 'regles', 6838),\n",
       " (764, 'coincideixen', 1893),\n",
       " (765, 'actuals', 11965),\n",
       " (766, 'mentre', 132180),\n",
       " (767, 'mètode', 20735),\n",
       " (768, 'complementària', 1363),\n",
       " (769, 'trenta', 12377),\n",
       " (770, 'destinades', 2677),\n",
       " (771, 'contenir', 3624),\n",
       " (772, 'reservades', 393),\n",
       " (773, 'agrupades', 1591),\n",
       " (774, 'blocs', 6806),\n",
       " (775, 'bloc', 11524),\n",
       " (776, 'encapçalada', 2188),\n",
       " (777, 'd', 138622),\n",
       " (778, 'ubicant', 78),\n",
       " (779, 'peces', 24812),\n",
       " (780, 'escrites', 4867),\n",
       " (781, 'calia', 4107),\n",
       " (782, \"l'absència\", 3620),\n",
       " (783, 'peça', 15918),\n",
       " (784, 'indica', 12087),\n",
       " (785, 'quantitat', 25860),\n",
       " (786, 'memoritzar', 225),\n",
       " (787, '1617', 1564),\n",
       " (788, 'publicar', 37215),\n",
       " (789, 'basat', 14489),\n",
       " (790, 'varetes', 381),\n",
       " (791, 'productes', 34940),\n",
       " (792, 'redueixen', 1155),\n",
       " (793, 'quocients', 145),\n",
       " (794, 'consta', 20720),\n",
       " (795, 'tauler', 3799),\n",
       " (796, 'vora', 19438),\n",
       " (797, 'col', 180580),\n",
       " (798, 'locaran', 32),\n",
       " (799, 'dividida', 6285),\n",
       " (800, 'caselles', 1345),\n",
       " (801, 'quals', 146503),\n",
       " (802, \"s'escriuen\", 625),\n",
       " (803, 'quadrats', 6032),\n",
       " (804, 'quadrat', 6791),\n",
       " (805, 'dividit', 7473),\n",
       " (806, 'meitats', 807),\n",
       " (807, 'traç', 1199),\n",
       " (808, 'diagonal', 3357),\n",
       " (809, 'casella', 1213),\n",
       " (810, 'vareta', 598),\n",
       " (811, \"s'escriu\", 2234),\n",
       " (812, 'omplint', 478),\n",
       " (813, 'següents', 40317),\n",
       " (814, 'multiplicat', 348),\n",
       " (815, 'nou', 130523),\n",
       " (816, 'resultats', 43261),\n",
       " (817, 'producte', 17725),\n",
       " (818, 'costat', 68200),\n",
       " (819, 'sigui', 32237),\n",
       " (820, 'inferior', 22876),\n",
       " (821, 'escrivint', 3085),\n",
       " (822, 'calcular', 4462),\n",
       " (823, 'exemple', 75118),\n",
       " (824, '46785399', 2),\n",
       " (825, 'posen', 3216),\n",
       " (826, 'tal', 66025),\n",
       " (827, 'mostra', 75015),\n",
       " (828, 'figura', 24361),\n",
       " (829, 'llegeix', 1896),\n",
       " (830, 'faixa', 1190),\n",
       " (831, 'ho', 76961),\n",
       " (832, 'calen', 602),\n",
       " (833, 'senzilles', 2010),\n",
       " (834, 'portant', 4152),\n",
       " (835, 'situats', 7079),\n",
       " (836, 'millora', 7881),\n",
       " (837, 'desenvolupades', 1835),\n",
       " (838, '1885', 10143),\n",
       " (839, 'henri', 11468),\n",
       " (840, 'proposta', 9863),\n",
       " (841, 'teòrica', 2919),\n",
       " (842, 'matemàtic', 9967),\n",
       " (843, 'proporciona', 6630),\n",
       " (844, 'necessitat', 13773),\n",
       " (845, 'diferència', 24257),\n",
       " (846, 'tants', 2077),\n",
       " (847, 'possibles', 9843),\n",
       " (848, 'resultar', 8479),\n",
       " (849, \"d'afegir\", 1011),\n",
       " (850, 'ròssec', 109),\n",
       " (851, 'anterior', 25641),\n",
       " (852, 'triangles', 1230),\n",
       " (853, 'funció', 42384),\n",
       " (854, 'fletxes', 1718),\n",
       " (855, 'indicant', 1735),\n",
       " (856, 'següent', 137531),\n",
       " (857, 'acumulades', 270),\n",
       " (858, \"l'actual\", 25095),\n",
       " (859, '52.749', 2),\n",
       " (860, 'sempre', 47313),\n",
       " (861, 'queden', 8024),\n",
       " (862, 'dígit', 487),\n",
       " (863, 'això', 156313),\n",
       " (864, 'regleta', 29),\n",
       " (865, 'triangle', 5928),\n",
       " (866, 'engloba', 1204),\n",
       " (867, 'apunta', 1393),\n",
       " (868, 'canvi', 50265),\n",
       " (869, 'calgui', 329),\n",
       " (870, 'depenent', 7484),\n",
       " (871, 'tercera', 35063),\n",
       " (872, '0', 48793),\n",
       " (873, \"s'aprecia\", 1221),\n",
       " (874, 'algunes', 72640),\n",
       " (875, 'fonts', 29083),\n",
       " (876, 'esmenten', 1107),\n",
       " (877, 'nepohualtzintzin', 11),\n",
       " (878, 'cultura', 60801),\n",
       " (879, 'mesoamericà', 254),\n",
       " (880, 'utilitzava', 2602),\n",
       " (881, 've', 10692),\n",
       " (882, 'nahuatl', 33),\n",
       " (883, 'arrels', 6747),\n",
       " (884, 'personal', 31700),\n",
       " (885, 'pohual', 1),\n",
       " (886, 'pohualli', 1),\n",
       " (887, 'tzintzin', 1),\n",
       " (888, 'elements', 42500),\n",
       " (889, 'similars', 12914),\n",
       " (890, 'petits', 24931),\n",
       " (891, 'significat', 8782),\n",
       " (892, 'complet', 14031),\n",
       " (893, 'comptant', 1458),\n",
       " (894, 'algú', 4872),\n",
       " (895, 'kalmekak', 1),\n",
       " (896, 'temalpouhkeh', 1),\n",
       " (897, 'eren', 281588),\n",
       " (898, 'estudiants', 12477),\n",
       " (899, 'dedicats', 3758),\n",
       " (900, 'dur', 23647),\n",
       " (901, 'cels', 1521),\n",
       " (902, 'infantesa', 3566),\n",
       " (903, 'malauradament', 1806),\n",
       " (904, 'víctimes', 7977),\n",
       " (905, 'destrucció', 9166),\n",
       " (906, 'conqueridors', 795),\n",
       " (907, 'atribuir', 2519),\n",
       " (908, 'origen', 19792),\n",
       " (909, 'diabòlic', 267),\n",
       " (910, 'observar', 9842),\n",
       " (911, 'enormes', 1921),\n",
       " (912, 'representació', 20079),\n",
       " (913, 'precisió', 5842),\n",
       " (914, 'velocitat', 33755),\n",
       " (915, 'basava', 2142),\n",
       " (916, 'asteques', 655),\n",
       " (917, 'vintenes', 15),\n",
       " (918, 'completament', 16366),\n",
       " (919, 'natural', 61433),\n",
       " (920, 'estava', 86842),\n",
       " (921, 'principals', 76813),\n",
       " (922, 'separades', 4823),\n",
       " (923, 'cordó', 1359),\n",
       " (924, 'intermedi', 2472),\n",
       " (925, 'unitaris', 507),\n",
       " (926, 'dret', 55019),\n",
       " (927, '15', 101063),\n",
       " (928, 'respectivament', 12619),\n",
       " (929, 'respectius', 2355),\n",
       " (930, 'files', 5448),\n",
       " (931, 'superiors', 10930),\n",
       " (932, 'prou', 15660),\n",
       " (933, 'correspondria', 827),\n",
       " (934, 'estiguessin', 1280),\n",
       " (935, 'tot', 281520),\n",
       " (936, 'plegat', 1968),\n",
       " (937, '13', 86321),\n",
       " (938, 'comprenia', 2172),\n",
       " (939, 'bàsic', 4116),\n",
       " (940, 'entendre', 5756),\n",
       " (941, 'estreta', 5128),\n",
       " (942, 'concebuda', 1148),\n",
       " (943, 'fenòmens', 4189),\n",
       " (944, 'naturals', 18160),\n",
       " (945, 'submón', 59),\n",
       " (946, 'cicles', 2984),\n",
       " (947, 'dies', 62685),\n",
       " (948, 'dura', 8398),\n",
       " (949, 'estació', 25854),\n",
       " (950, 'nepohualtzitzin', 1),\n",
       " (951, 'cicle', 14116),\n",
       " (952, 'blat', 9087),\n",
       " (953, 'moro', 6231),\n",
       " (954, 'sembra', 804),\n",
       " (955, 'recol', 4426),\n",
       " (956, 'lecció', 34345),\n",
       " (957, 'gestació', 1444),\n",
       " (958, 'criatura', 1677),\n",
       " (959, 'completa', 13483),\n",
       " (960, \"s'aproxima\", 535),\n",
       " (961, 'any', 121608),\n",
       " (962, 'val', 9768),\n",
       " (963, 'pena', 7721),\n",
       " (964, 'esmentar', 4273),\n",
       " (965, 'permetia', 6585),\n",
       " (966, '18', 169978),\n",
       " (967, 'tant', 113511),\n",
       " (968, 'quantitats', 6594),\n",
       " (969, 'astronòmiques', 991),\n",
       " (970, 'infinitesimals', 161),\n",
       " (971, 'absoluta', 7528),\n",
       " (972, 'traduït', 5199),\n",
       " (973, 'informàtica', 7872),\n",
       " (974, 'moderna', 21742),\n",
       " (975, 'errors', 4430),\n",
       " (976, \"d'arrodoniment\", 76),\n",
       " (977, 'redescoberta', 330),\n",
       " (978, 'deguda', 2602),\n",
       " (979, \"l'enginyer\", 2809),\n",
       " (980, 'mexicà', 6265),\n",
       " (981, 'david', 51355),\n",
       " (982, 'esparza', 337),\n",
       " (983, 'hidalgo', 2142),\n",
       " (984, 'seus', 305813),\n",
       " (985, 'viatges', 7929),\n",
       " (986, 'mèxic', 54565),\n",
       " (987, 'gravats', 4043),\n",
       " (988, 'diversos', 108624),\n",
       " (989, 'pintures', 13389),\n",
       " (990, 'reconstruir', 4039),\n",
       " (991, 'quants', 5636),\n",
       " (992, 'fets', 23739),\n",
       " (993, \"d'or\", 63445),\n",
       " (994, 'jade', 1158),\n",
       " (995, 'incrustacions', 343),\n",
       " (996, 'closca', 2022),\n",
       " (997, 'etc', 41612),\n",
       " (998, 'atribuïts', 792),\n",
       " (999, 'george', 34220),\n",
       " (1000, 'sanchez', 617),\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x[:200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code for dev\n",
    "from itertools import product\n",
    "\n",
    "retrieval_path: Path = experiment_folder / 'predictions' / 'retrieve' / f'with_bos-{args.type}.json'\n",
    "with retrieval_path.open() as f:\n",
    "    retrieved = json.load(f)\n",
    "\n",
    "df = {'db_id': [], 'ss': [], 'bo': []}\n",
    "retrieved_by_db_ids = defaultdict(set)\n",
    "for r in retrieved:\n",
    "    db_id = r['db_id']\n",
    "    sample_id = r['sample_id']\n",
    "    ret = r['retrieved']\n",
    "    for s, bo in product([sample_id], ret):\n",
    "        df['db_id'].append(db_id)\n",
    "        df['ss'].append(s)\n",
    "        df['bo'].append(bo)\n",
    "    retrieved_by_db_ids[db_id].update(ret)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "n_bos_aug = 30\n",
    "sampled_bos = defaultdict(list)\n",
    "for db_id, bos in retrieved_by_db_ids.items():\n",
    "    if len(bos) > n_bos_aug:\n",
    "        n_sample = n_bos_aug\n",
    "    else:\n",
    "        # sample half of the bos\n",
    "        n_sample = len(bos) // 2\n",
    "    sampled = np.random.choice(list(bos), n_sample, replace=False).tolist()\n",
    "    sampled_bos[db_id].extend(sampled)\n",
    "\n",
    "# with (experiment_folder / 'predictions' / 'retrieve' / f'with_bos-sampled.json').open('w') as f:\n",
    "#     json.dump(sampled_bos, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_bos(\n",
    "        bos: dict[str, list[dict[str, str]]]\n",
    "    ) -> dict[str, list[dict[str, str]]]:\n",
    "    hash_map = defaultdict(set)\n",
    "    for db_id, xs in bos.items():\n",
    "        for x in xs:\n",
    "            hash_map[x['vt']].add(x['sample_id'])\n",
    "\n",
    "    ids = list(map(lambda x: list(x)[0], hash_map.values()))\n",
    "    new_bos = {}\n",
    "    for db_id, xs in bos.items():\n",
    "        new_bos[db_id] = [x for x in xs if x['sample_id'] in ids]\n",
    "\n",
    "    return new_bos\n",
    "\n",
    "with (experiment_folder / 'predictions' / 'retrieve' / f'with_bos-sampled.json').open('r') as f:\n",
    "    retrieved = json.load(f)\n",
    "\n",
    "bo_path: Path = experiment_folder / 'predictions' / 'create_bo' / f'final_{args.ds}_train_bo.json'\n",
    "with bo_path.open() as f:\n",
    "    bos = json.load(f)\n",
    "bos = remove_duplicate_bos(bos)\n",
    "\n",
    "\n",
    "if args.with_bos:\n",
    "    prompt_template = Prompts.gen_template_with_bos\n",
    "    input_variables = ['schema', 'input_query', 'hint']\n",
    "    structured_output = GenTemplateResponse\n",
    "else:\n",
    "    prompt_template = Prompts.gen_template_no_bos\n",
    "    input_variables = ['schema', 'input_query']\n",
    "    structured_output = SQLResponse\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=input_variables,\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0,\n",
    "    frequency_penalty=0.1,\n",
    ")\n",
    "model = model_openai.with_structured_output(structured_output, method='json_mode')\n",
    "chain = (prompt | model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, islice\n",
    "def batched(iterable, n, *, strict=False):\n",
    "    # batched('ABCDEFG', 3) → ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    iterator = iter(iterable)\n",
    "    while batch := tuple(islice(iterator, n)):\n",
    "        if strict and len(batch) != n:\n",
    "            raise ValueError('batched(): incomplete batch')\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.callbacks.manager import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hint(id2bo, doc_ids):\n",
    "    docs = [{\n",
    "        'descrption': id2bo[doc_id]['ba'], \n",
    "        'virtual_table': id2bo[doc_id]['vt']\n",
    "    } for doc_id in doc_ids]\n",
    "    \n",
    "    hint = '\\nDescriptions and Virtual Tables:\\n'\n",
    "    hint += json.dumps({j: doc for j, doc in enumerate(docs)}, indent=4)\n",
    "    hint += '\\n'\n",
    "    return hint\n",
    "\n",
    "with_bos = args.with_bos\n",
    "is_test = False\n",
    "if with_bos and is_test:\n",
    "    retrieved_by_db_id = defaultdict(dict)\n",
    "    for sample in retrieved:\n",
    "        db_id = sample['db_id']\n",
    "        sample_id = sample['sample_id']\n",
    "        retrieved_by_db_id[db_id][sample_id] = sample['retrieved']\n",
    "\n",
    "samples_by_db_id = defaultdict(list)\n",
    "for sample in samples:\n",
    "    samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "\n",
    "for db_id, samples in samples_by_db_id.items():    \n",
    "    id2bo = {} \n",
    "    schema_str = get_schema_str(\n",
    "        schema=tables[db_id].db_schema, \n",
    "        foreign_keys=tables[db_id].foreign_keys,\n",
    "        col_explanation=tables[db_id].col_explanation\n",
    "    )\n",
    "    for x in bos[db_id]:\n",
    "        id2bo[x['sample_id']] = x\n",
    "\n",
    "    n_batch = 32 if is_test else 1\n",
    "    results = []\n",
    "    for batch in batched(samples, n_batch):\n",
    "        batch_inputs = []\n",
    "        batch_retrieved = []\n",
    "        batch_sample_ids = []\n",
    "        if with_bos:\n",
    "            if is_test:\n",
    "                for sample in batch:\n",
    "                    question = sample.final.question\n",
    "                    doc_ids = retrieved_by_db_id[db_id][sample.sample_id]\n",
    "                    hint = create_hint(id2bo, doc_ids)\n",
    "                    input_data = {'schema': schema_str, 'input_query': question, 'hint': hint}\n",
    "                    batch_inputs.append(input_data)\n",
    "                    batch_retrieved.append(doc_ids)\n",
    "                    batch_sample_ids.append(sample.sample_id)\n",
    "            else:\n",
    "                for bo_id, sample in product(retrieved[db_id], batch):\n",
    "                    question = sample.final.question\n",
    "                    doc_ids = [bo_id]\n",
    "                    hint = create_hint(id2bo, doc_ids)\n",
    "                    input_data = {'schema': schema_str, 'input_query': question, 'hint': hint}\n",
    "                    batch_inputs.append(input_data)\n",
    "                    batch_retrieved.append(doc_ids)\n",
    "                    batch_sample_ids.append(sample.sample_id)\n",
    "        else:\n",
    "            for sample in batch:\n",
    "                question = sample.final.question\n",
    "                input_data = {'schema': schema_str, 'input_query': question}\n",
    "                batch_inputs.append(input_data)\n",
    "                batch_retrieved.append([])\n",
    "                batch_sample_ids.append(sample.sample_id)\n",
    "        \n",
    "        # with get_openai_callback() as cb:\n",
    "        #     batch_outputs: list[GenTemplateResponse] = chain.batch(inputs=batch_inputs)\n",
    "\n",
    "        # for sample, output in zip(batch, batch_outputs):\n",
    "        #     res = {}\n",
    "        #     res['sample_id'] = sample.sample_id\n",
    "        #     res['rationale'] = output.rationale\n",
    "        #     res['sql_template'] = output.sql\n",
    "        #     if with_bos:\n",
    "        #         res['hint_used'] = output.hint_used\n",
    "        #     res['token_usage'] = {'tokens': cb.total_tokens/len(batch), 'cost': cb.total_cost/len(batch)}\n",
    "        #     results.append(res)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    batch_outputs: list[GenTemplateResponse] = chain.batch(inputs=batch_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_community.cache import SQLiteCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS():\n",
    "    exp_name = 'pipeline_exp'\n",
    "    ds = 'bird'\n",
    "    type = 'dev'\n",
    "    task = 'keyword_extraction'\n",
    "    with_bos = True\n",
    "    \n",
    "args = ARGS()\n",
    "\n",
    "description_file = f'description.json' if args.ds == 'spider' else f'{args.ds}_description.json'\n",
    "experiment_folder = proj_path / args.exp_name / args.ds\n",
    "\n",
    "description_file = 'bird_description.json' if args.ds == 'bird' else 'description.json'\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / args.ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{args.ds}_{args.type}.json')\n",
    "\n",
    "prediction_path = experiment_folder / 'predictions' / args.task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'{\"with_bos\" if args.with_bos else \"no_bos\"}-{args.type}'\n",
    "template_path = experiment_folder / 'predictions' / 'gen_template' / f'{file_name}.json'\n",
    "with template_path.open() as f:\n",
    "    templates = json.load(f)\n",
    "samples_by_id = {s.sample_id: s for s in samples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same template will have one keyword extraction inference\n",
    "input_samples = []\n",
    "template_sample_id2doc_ids = defaultdict(set)  # template, sample_id: {doc_id}\n",
    "for t in templates:\n",
    "    sample_id: int = t['sample_id']\n",
    "    doc_ids: list = t['doc_ids']   # [] for no_bos\n",
    "    sql_template: str = t['sql_template']\n",
    "    key = (sql_template, sample_id)\n",
    "    if key not in template_sample_id2doc_ids:\n",
    "        sample = samples_by_id[sample_id]\n",
    "        x = {\n",
    "            'sample_id': sample_id,\n",
    "            'question': sample.final.question,\n",
    "            'evidence': sample.evidence,\n",
    "            'db_id': sample.db_id,\n",
    "            'sql_template': sql_template,\n",
    "        }\n",
    "        input_samples.append(x)\n",
    "        template_sample_id2doc_ids[key].update(doc_ids)\n",
    "\n",
    "    for doc_id in doc_ids:\n",
    "        template_sample_id2doc_ids[key].add(doc_id)\n",
    "\n",
    "if not (args.with_bos and args.type == 'dev'):\n",
    "    # dev(with_bos): sample_id + doc_ids (need to evaluate for doc_ids)\n",
    "    # dev(no_bos): sample_id\n",
    "    # test(both cases): sample_id (under test retrieval number = 1)\n",
    "    assert len(template_sample_id2doc_ids) == len(templates), f\"{len(template_sample_id2doc_ids)} != {len(templates)}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=Prompts.keyword_extraction,\n",
    "    input_variables=['schema', 'input_query', 'evidence', 'sql_template'],\n",
    ")\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.5,\n",
    "    frequency_penalty=0.1,\n",
    ")\n",
    "model = model_openai.with_structured_output(KeywordExtractionResponse, method='json_mode')\n",
    "chain = (prompt | model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29368, 57902)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_samples), len(templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_by_db_id = defaultdict(list)\n",
    "for sample in input_samples:\n",
    "    # sample: {sample_id, db_id, question, evidence, sql_template}\n",
    "    db_id = sample['db_id']\n",
    "    if db_id not in []:\n",
    "        x = {\n",
    "            'sample_id': sample['sample_id'],\n",
    "            'question': sample['question'],\n",
    "            'evidence': sample['evidence'],\n",
    "            'sql_template': sample['sql_template'],\n",
    "        }\n",
    "        samples_by_db_id[db_id].append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_id = 'legislator'\n",
    "samples = samples_by_db_id[db_id]\n",
    "set_llm_cache(SQLiteCache(database_path=f\"./cache/{prefix}{prediction_path.stem}_{db_id}.db\"))\n",
    "results = []\n",
    "schema_str = get_schema_str(schema=tables[db_id].db_schema)\n",
    "n_batch = 32\n",
    "batched_samples: list[dict] = list(batched(samples, n_batch))\n",
    "batch = batched_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = []\n",
    "batch_iidx2oidx: dict[int, int] = {}  # input_idx --> output_idx for batch_outputs\n",
    "\n",
    "for iidx, sample in enumerate(batch):\n",
    "    # STRING_TYPE = '[PLACEHOLDER-TYPE:STRING]'.lower()\n",
    "    # NUMERIC_TYPE = '[PLACEHOLDER-TYPE:NUMERIC]'.lower()\n",
    "    # only need to inference on the queries that has placeholders\n",
    "    if (STRING_TYPE.lower() in sample['sql_template'].lower()) or (NUMERIC_TYPE.lower() in sample['sql_template'].lower()):\n",
    "        input_data = {\n",
    "            'schema': schema_str,\n",
    "            'input_query': sample['question'],\n",
    "            'evidence': sample['evidence'],\n",
    "            'sql_template': sample['sql_template'] \n",
    "        }\n",
    "        batch_inputs.append(input_data)\n",
    "        batch_iidx2oidx[iidx] = len(batch_inputs) - 1\n",
    "\n",
    "if batch_inputs:\n",
    "    with get_openai_callback() as cb:\n",
    "        batch_outputs: list[KeywordExtractionResponse] = chain.batch(inputs=batch_inputs)\n",
    "else:\n",
    "    batch_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for iidx, sample in enumerate(batch):\n",
    "    sample_id = sample['sample_id']\n",
    "    sql_template = sample['sql_template']\n",
    "    res = {'sample_id': sample_id}\n",
    "    \n",
    "    oidx = batch_iidx2oidx.get(iidx)\n",
    "    if oidx is not None:\n",
    "        res['keywords'] = batch_outputs[oidx].extraction\n",
    "    else:\n",
    "        res['keywords'] = {}\n",
    "    res['token_usage'] = {'tokens': cb.total_tokens/n_batch, 'cost': cb.total_cost/n_batch}\n",
    "    key = (sql_template, sample_id)\n",
    "    doc_ids = template_sample_id2doc_ids.get(key)\n",
    "    if doc_ids:\n",
    "        for doc_id in doc_ids:\n",
    "            new_res = deepcopy(res)\n",
    "            new_res['doc_ids'] = [doc_id]\n",
    "            results.append(new_res)\n",
    "    else:\n",
    "        res['doc_ids'] = []\n",
    "        results.append(res)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS():\n",
    "    exp_name = 'pipeline_exp'\n",
    "    ds = 'bird'\n",
    "    type = 'dev'\n",
    "    task = 'search_value'\n",
    "    with_bos = True\n",
    "    \n",
    "args = ARGS()\n",
    "\n",
    "description_file = f'description.json' if args.ds == 'spider' else f'{args.ds}_description.json'\n",
    "experiment_folder = proj_path / args.exp_name / args.ds\n",
    "\n",
    "description_file = 'bird_description.json' if args.ds == 'bird' else 'description.json'\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / args.ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{args.ds}_{args.type}.json')\n",
    "\n",
    "def _format_column_value(string: str):\n",
    "    operations = [\n",
    "        '>', '<', '=', '>=', '<=', '!=', '<>'\n",
    "    ]\n",
    "    found = False\n",
    "    for op in operations:\n",
    "        if op in string.lower():\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if found:\n",
    "        # split the string by the operator\n",
    "        string = string.split(op, 1)[-1].strip()\n",
    "    \n",
    "    # remove ' or \" from the string\n",
    "    string = string.replace(\"'\", '').replace('\"', '')\n",
    "    return string\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{args.ds}_{args.type}.json')\n",
    "\n",
    "file_name = f'{\"with_bos\" if args.with_bos else \"no_bos\"}-{args.type}'\n",
    "# keyword_path = experiment_folder / 'predictions' / 'keyword_extraction' / f'{file_name}.json'\n",
    "keyword_path = experiment_folder / 'predictions' / 'keyword_extraction' / f'with_bos-dev2.json'\n",
    "assert keyword_path.exists(), f'Run with the `task=keyword_extraction, type={args.type}` first'\n",
    "with keyword_path.open() as f:\n",
    "    keywords = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_by_id = {s.sample_id: s for s in samples}\n",
    "\n",
    "input_samples = []\n",
    "keyword_sample_id2doc_ids = defaultdict(set) # (all_kws, sample_id): {doc_id}\n",
    "for k in keywords:\n",
    "    sample_id: int = k['sample_id']\n",
    "    doc_ids: list = k['doc_ids']\n",
    "    # process keywords\n",
    "    output_kws: dict[str, list[str]] = k['keywords']\n",
    "    # sort keywords first\n",
    "    output_kws = {k: sorted(v) for k, v in output_kws.items()}\n",
    "    all_kws: dict[str, list[str]] = {}\n",
    "    for output_column_name, kws in output_kws.items():\n",
    "        if kws:  # only store non-empty keywords\n",
    "            if ('.' in output_column_name): # rewrite the column_name\n",
    "                _, column_name = output_column_name.split('.')\n",
    "            else:\n",
    "                column_name = output_column_name\n",
    "            all_kws[column_name] = [_format_column_value(kw) for kw in kws if kw]\n",
    "\n",
    "    key = (json.dumps(all_kws), sample_id)\n",
    "    if key not in keyword_sample_id2doc_ids:\n",
    "        sample = samples_by_id[sample_id]\n",
    "        x = {\n",
    "            'sample_id': sample_id,\n",
    "            'db_id': sample.db_id,\n",
    "            'db_file': get_db_file(proj_path, args.ds, sample.db_id),\n",
    "            'search_keywords': all_kws\n",
    "        }\n",
    "        input_samples.append(x)\n",
    "        keyword_sample_id2doc_ids[key].update(doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 680)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_samples), len(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS():\n",
    "    exp_name = 'pipeline_exp'\n",
    "    ds = 'bird'\n",
    "    type = 'dev'\n",
    "    task = 'fill_in'\n",
    "    with_bos = True\n",
    "    \n",
    "args = ARGS()\n",
    "\n",
    "description_file = f'description.json' if args.ds == 'spider' else f'{args.ds}_description.json'\n",
    "experiment_folder = proj_path / args.exp_name / args.ds\n",
    "\n",
    "description_file = 'bird_description.json' if args.ds == 'bird' else 'description.json'\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / args.ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{args.ds}_{args.type}.json')\n",
    "\n",
    "file_name = f'{\"with_bos\" if args.with_bos else \"no_bos\"}-{args.type}'\n",
    "values_path = experiment_folder / 'predictions' / 'search_value' / f'{file_name}.json'\n",
    "assert values_path.exists(), f'Run with the `task=search_value, type={args.type}` first'\n",
    "with values_path.open() as f:\n",
    "    searched_values = json.load(f)\n",
    "\n",
    "template_path = experiment_folder / 'predictions' / 'gen_template' / f'{file_name}.json'\n",
    "assert template_path.exists(), f'Run with the `task=gen_template, type={args.type}` first'\n",
    "with template_path.open() as f:\n",
    "    templates = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare inputs\n",
    "samples_by_id = {s.sample_id: s for s in samples}\n",
    "values_by_id = {v['sample_id']: v for v in searched_values}\n",
    "# same template will have one keyword extraction inference\n",
    "input_samples = []\n",
    "template_values_sample_id2doc_ids = defaultdict(set)  # template, sample_id: {doc_id}\n",
    "for t in templates:\n",
    "    sample_id: int = t['sample_id']\n",
    "    doc_ids: list = t['doc_ids']   # [] for no_bos\n",
    "    sql_template: str = t['sql_template']\n",
    "    values = values_by_id[sample_id]['values'] if values_by_id.get(sample_id) else {}\n",
    "    # sort values\n",
    "    if values:\n",
    "        values = {\n",
    "            table_name: {column_name: sorted(vs) for column_name, vs in column_values.items()}\n",
    "                   for table_name, column_values in values.items()}\n",
    "    key = (sql_template, json.dumps(values), sample_id)\n",
    "    if key not in template_values_sample_id2doc_ids:\n",
    "        sample = samples_by_id[sample_id]\n",
    "        x = {\n",
    "            'sample_id': sample_id,\n",
    "            'db_id': sample.db_id,\n",
    "            'question': sample.final.question,\n",
    "            'values': values,\n",
    "            'sql_template': sql_template,\n",
    "        }\n",
    "        input_samples.append(x)\n",
    "        template_values_sample_id2doc_ids[key].update(doc_ids)\n",
    "\n",
    "    for doc_id in doc_ids:\n",
    "        template_values_sample_id2doc_ids[key].add(doc_id)\n",
    "\n",
    "if not (args.with_bos and args.type == 'dev'):\n",
    "    # dev(with_bos): sample_id + doc_ids (need to evaluate for doc_ids)\n",
    "    # dev(no_bos): sample_id\n",
    "    # test(both cases): sample_id (under test retrieval number = 1)\n",
    "    assert len(template_values_sample_id2doc_ids) == len(templates), f\"{len(template_values_sample_id2doc_ids)} != {len(templates)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29368, 57902)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_samples), len(templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS():\n",
    "    exp_name = 'direct_exp'\n",
    "    ds = 'bird'\n",
    "    type = 'dev'\n",
    "    task = 'evaluate'\n",
    "    eval_target = 'direct'\n",
    "    with_bos = False\n",
    "    \n",
    "args = ARGS()\n",
    "\n",
    "description_file = f'description.json' if args.ds == 'spider' else f'{args.ds}_description.json'\n",
    "experiment_folder = proj_path / args.exp_name / args.ds\n",
    "eval_path = experiment_folder / 'evals'\n",
    "\n",
    "description_file = 'bird_description.json' if args.ds == 'bird' else 'description.json'\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / args.ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "\n",
    "def parse_sql_to_output(sql: str, schema: Schema):\n",
    "    try:\n",
    "        ei = extract_all(sql, schema)\n",
    "        assert len(ei['sel']) > 0, f'No selection found'\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    return ei\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{args.ds}_{args.type}.json')\n",
    "\n",
    "file_name = f'{\"with_bos\" if args.with_bos else \"no_bos\"}-{args.type}'\n",
    "eval_target_path = experiment_folder / 'predictions' / args.eval_target / f'{file_name}.json'\n",
    "assert eval_target_path.exists(), f'Run with the `task={args.eval_target}, type={args.type}` first'\n",
    "with eval_target_path.open() as f:\n",
    "    eval_targets = json.load(f)\n",
    "\n",
    "# TODO: for loop should be start from pred_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preparing eval data: 100%|██████████| 2091/2091 [00:00<00:00, 10097.80it/s]\n"
     ]
    }
   ],
   "source": [
    "parsed_queries_path: Path = eval_path / f'parsed_queries-{file_name}.pkl'\n",
    "parsed_queries_exist = parsed_queries_path.exists()\n",
    "if parsed_queries_exist:\n",
    "    with parsed_queries_path.open('rb') as f:\n",
    "        parsed_queries = pickle.load(f)\n",
    "else:\n",
    "    parsed_queries = defaultdict(dict)\n",
    "    parsed_queries['unable'] = set()\n",
    "\n",
    "samples_by_id = {s.sample_id: s for s in samples}\n",
    "eval_data: dict[str, list] = defaultdict(list)\n",
    "eval_data2doc_ids = defaultdict(set)\n",
    "\n",
    "# from flatten to nested by doc_ids\n",
    "for pred in tqdm(eval_targets, total=len(eval_targets), desc='preparing eval data'):\n",
    "    sample_id = pred['sample_id']\n",
    "    pred_sql = pred['sql']\n",
    "    doc_ids = pred['doc_ids']\n",
    "    target_sample = samples_by_id[sample_id]\n",
    "    target_sql = target_sample.final.sql\n",
    "    db_id = target_sample.db_id\n",
    "    key = hashlib.sha256(f'{sample_id}-{pred_sql}'.encode()).hexdigest()\n",
    "\n",
    "    if not parsed_queries_exist:\n",
    "        schema = Schema(tables[db_id].db_schema)\n",
    "        if parsed_queries['target'].get(target_sql) is None:\n",
    "            target_output = parse_sql_to_output(target_sql, schema)\n",
    "            parsed_queries['target'][target_sql] = target_output\n",
    "        else:\n",
    "            target_output = parsed_queries['target'][target_sql]\n",
    "        if parsed_queries['pred'].get(pred_sql) is None:\n",
    "            pred_output = parse_sql_to_output(pred_sql, schema)\n",
    "            parsed_queries['pred'][pred_sql] = pred_output\n",
    "        else:\n",
    "            pred_output = parsed_queries['pred'][pred_sql]\n",
    "    \n",
    "        if (not pred_output) or (not target_output):\n",
    "            if key not in parsed_queries['unable']:\n",
    "                parsed_queries['unable'].add(key)\n",
    "            continue\n",
    "    else:\n",
    "        if key in parsed_queries['unable']:\n",
    "            continue\n",
    "    \n",
    "    if key not in eval_data2doc_ids:\n",
    "        eval_data['sample_ids'].append(sample_id)\n",
    "        eval_data['target_queries'].append(target_sql)\n",
    "        eval_data['db_paths'].append(get_db_file(proj_path, args.ds, db_id))\n",
    "        eval_data['pred_queries'].append(pred_sql)\n",
    "        eval_data2doc_ids[key].update(doc_ids)\n",
    "\n",
    "    for doc_id in doc_ids:\n",
    "        eval_data2doc_ids[key].add(doc_id)\n",
    "\n",
    "# if not parsed_queries_exist:\n",
    "#     with parsed_queries_path.open('wb') as f:\n",
    "#         pickle.dump(parsed_queries, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, islice\n",
    "def batched(iterable, n, *, strict=False):\n",
    "    # batched('ABCDEFG', 3) → ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    iterator = iter(iterable)\n",
    "    while batch := tuple(islice(iterator, n)):\n",
    "        if strict and len(batch) != n:\n",
    "            raise ValueError('batched(): incomplete batch')\n",
    "        yield batch\n",
    "\n",
    "n_batch = 200\n",
    "n_samples = len(eval_data['sample_ids'])\n",
    "batches = list(batched(range(n_samples), n_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 11\n",
      "200 (0, 1, 2, 3, 4)\n",
      "200 11\n",
      "200 (200, 201, 202, 203, 204)\n",
      "200 11\n",
      "200 (400, 401, 402, 403, 404)\n",
      "200 11\n",
      "200 (600, 601, 602, 603, 604)\n",
      "200 11\n",
      "200 (800, 801, 802, 803, 804)\n",
      "200 11\n",
      "200 (1000, 1001, 1002, 1003, 1004)\n",
      "200 11\n",
      "200 (1200, 1201, 1202, 1203, 1204)\n",
      "200 11\n",
      "200 (1400, 1401, 1402, 1403, 1404)\n",
      "200 11\n",
      "200 (1600, 1601, 1602, 1603, 1604)\n",
      "200 11\n",
      "200 (1800, 1801, 1802, 1803, 1804)\n",
      "29 11\n",
      "29 (2000, 2001, 2002, 2003, 2004)\n"
     ]
    }
   ],
   "source": [
    "for idxes in batches:\n",
    "    print(len(idxes), len(batches))\n",
    "    batch_eval_data = {k: [v[i] for i in idxes] for k, v in eval_data.items()}\n",
    "    print(len(batch_eval_data['sample_ids']), idxes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_eval_data['sample_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'{\"with_bos\" if args.with_bos else \"no_bos\"}-{args.type}'\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{args.ds}_{args.type}.json')\n",
    "db_ids = {s.sample_id: s.db_id for s in samples}\n",
    "\n",
    "if args.with_bos:  # whether to use BOs or not\n",
    "    with open(experiment_folder / 'predictions' / 'retrieve' / f'{file_name}.json', 'r') as f:\n",
    "        retrieved = json.load(f)\n",
    "    retrieved = {r['sample_id']: r for r in retrieved}\n",
    "else:\n",
    "    retrieved = {}\n",
    "\n",
    "if args.eval_target == 'fill_in':  # pipeline method\n",
    "    # gen_template\n",
    "    with open(experiment_folder / 'predictions' / 'gen_template' / f'{file_name}.json', 'r') as f:\n",
    "        sql_templates = json.load(f)\n",
    "    sql_templates = {r['sample_id']: r for r in sql_templates}\n",
    "\n",
    "    # keyword_extraction\n",
    "    with open(experiment_folder / 'predictions' / 'keyword_extraction' / f'{file_name}.json', 'r') as f:\n",
    "        keyword_extraction = json.load(f)\n",
    "    keyword_extraction = {r['sample_id']: r for r in keyword_extraction}\n",
    "\n",
    "    # search_value\n",
    "    with open(experiment_folder / 'predictions' / 'search_value' / f'{file_name}.json', 'r') as f:\n",
    "        search_value = json.load(f)\n",
    "    search_value = {r['sample_id']: r for r in search_value}\n",
    "else:\n",
    "    sql_templates = {}\n",
    "    keyword_extraction = {}\n",
    "    search_value = {}\n",
    "\n",
    "with open(experiment_folder / 'evals' / f'execution_result-{file_name}.json', 'r') as f:\n",
    "    exec_results = json.load(f)\n",
    "exec_results = {r['sample_id']: r for r in exec_results}\n",
    "\n",
    "with open(experiment_folder / 'evals' / f'merit_result-{file_name}.json', 'r') as f:\n",
    "    merit_results = json.load(f)\n",
    "merit_results = {r['sample_id']: r for r in merit_results}\n",
    "\n",
    "all_results = defaultdict(list)\n",
    "\n",
    "for sample_id in merit_results.keys():\n",
    "    all_results['sample_id'].append(sample_id)\n",
    "    all_results['db_id'].append(db_ids[sample_id])\n",
    "    if args.with_bos:\n",
    "        bo_ids = ','.join(map(str, retrieved[sample_id]['retrieved']))\n",
    "        bo_used = int(sql_templates[sample_id]['hint_used'])\n",
    "        all_results['bo_ids'].append(bo_ids)\n",
    "        all_results['bo_used'].append(bo_used)\n",
    "\n",
    "    if args.eval_target == 'fill_in':\n",
    "        keywords = json.dumps({column_name: kws for column_name, kws in keyword_extraction[sample_id]['keywords'].items() if kws})\n",
    "        # keywords = False if keywords == '{}' else True\n",
    "        values = json.dumps(search_value[sample_id]['values'])\n",
    "        # values = False if values == '{}' else True\n",
    "        all_results['keywords'].append(keywords)\n",
    "        all_results['values'].append(values)\n",
    "    all_results['exec_res'].append(exec_results[sample_id]['res'])\n",
    "    all_results['structural_score'].append(merit_results[sample_id]['structural_score'])\n",
    "    all_results['semantic_score'].append(merit_results[sample_id]['semantic_score'])\n",
    "    all_results['f1_score'].append(merit_results[sample_id]['f1_score'])\n",
    "    all_results['target_complexity'].append(merit_results[sample_id]['target_complexity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### valid_bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS():\n",
    "    exp_name = 'pipeline_exp'\n",
    "    ds = 'bird'\n",
    "    type = 'dev'\n",
    "    task = 'valid_bo'\n",
    "    with_bos = False\n",
    "    \n",
    "args = ARGS()\n",
    "\n",
    "description_file = f'description.json' if args.ds == 'spider' else f'{args.ds}_description.json'\n",
    "experiment_folder = proj_path / args.exp_name / args.ds\n",
    "\n",
    "description_file = 'bird_description.json' if args.ds == 'bird' else 'description.json'\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / args.ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{args.ds}_{args.type}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = experiment_folder / 'evals'\n",
    "\n",
    "df_base = pd.read_csv(eval_path / f'result-with_bos-{args.type}.csv')\n",
    "df_base['f1_score'] = np.random.rand(df_base.shape[0])\n",
    "df_base.drop(columns=['bo_ids', 'bo_used', 'keywords', 'values'], inplace=True)\n",
    "df_bo = pd.read_csv(eval_path / f'result-with_bos-{args.type}.csv')\n",
    "# df_no_bo = pd.read_csv(eval_path / f'result-no_bos-{args.type}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cates = df_base.groupby('db_id')['target_complexity'].apply(_get_categories).rename('category').apply(_format_interval)\n",
    "df_base = pd.merge(df_base, df_cates.reset_index('db_id', drop=True), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    left=df_bo,\n",
    "    right=df_base,\n",
    "    how='inner',\n",
    "    on=['db_id', 'sample_id', 'target_complexity'],\n",
    "    suffixes=('_bo', '_base')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>db_id</th>\n",
       "      <th>bo_ids</th>\n",
       "      <th>bo_used</th>\n",
       "      <th>keywords</th>\n",
       "      <th>values</th>\n",
       "      <th>exec_res_bo</th>\n",
       "      <th>structural_score_bo</th>\n",
       "      <th>semantic_score_bo</th>\n",
       "      <th>f1_score_bo</th>\n",
       "      <th>target_complexity</th>\n",
       "      <th>exec_res_base</th>\n",
       "      <th>structural_score_base</th>\n",
       "      <th>semantic_score_base</th>\n",
       "      <th>f1_score_base</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>movie_platform</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5657</td>\n",
       "      <td>0.8888</td>\n",
       "      <td>0.691364</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5657</td>\n",
       "      <td>0.8888</td>\n",
       "      <td>0.357111</td>\n",
       "      <td>(8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>movie_platform</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"movie_title\": [\"When Will I Be Loved\"]}</td>\n",
       "      <td>{\"movies\": {\"movie_title\": [\"When Will I Be Lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.519125</td>\n",
       "      <td>(4, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>movie_platform</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.9713</td>\n",
       "      <td>0.644995</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.9713</td>\n",
       "      <td>0.980458</td>\n",
       "      <td>(4, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>movie_platform</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"rating_date_utc\": [\"2020-04-01\", \"2020-04-30...</td>\n",
       "      <td>{\"ratings_users\": {\"rating_date_utc\": [\"2020-0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6489</td>\n",
       "      <td>0.9158</td>\n",
       "      <td>0.759587</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6489</td>\n",
       "      <td>0.9158</td>\n",
       "      <td>0.710273</td>\n",
       "      <td>(8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13</td>\n",
       "      <td>movie_platform</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"movie_title\": [\"Welcome to the Dollhouse\"]}</td>\n",
       "      <td>{\"movies\": {\"movie_title\": [\"Welcome to the Do...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.534417</td>\n",
       "      <td>(7, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>10924</td>\n",
       "      <td>debit_card_specializing</td>\n",
       "      <td>10923</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"currency\": [\"euro\", \"EUR\"]}</td>\n",
       "      <td>{\"products\": {\"description\": [\"Eurovignette\"]}...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5138</td>\n",
       "      <td>0.7049</td>\n",
       "      <td>0.594367</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5138</td>\n",
       "      <td>0.7049</td>\n",
       "      <td>0.246214</td>\n",
       "      <td>(7, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>10929</td>\n",
       "      <td>debit_card_specializing</td>\n",
       "      <td>10936</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"chainid\": [\"11\"]}</td>\n",
       "      <td>{\"gasstations\": {\"chainid\": [\"11\"]}, \"transact...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8738</td>\n",
       "      <td>0.9658</td>\n",
       "      <td>0.917499</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8738</td>\n",
       "      <td>0.9658</td>\n",
       "      <td>0.715064</td>\n",
       "      <td>(7, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>10930</td>\n",
       "      <td>debit_card_specializing</td>\n",
       "      <td>10931</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"country\": [\"CZE\"], \"price\": [\"1000\"]}</td>\n",
       "      <td>{\"gasstations\": {\"country\": [\"CZE\"]}}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.948353</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.331370</td>\n",
       "      <td>(7, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>10939</td>\n",
       "      <td>debit_card_specializing</td>\n",
       "      <td>10918</td>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.7175</td>\n",
       "      <td>0.712870</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.7175</td>\n",
       "      <td>0.625524</td>\n",
       "      <td>(7, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>10953</td>\n",
       "      <td>debit_card_specializing</td>\n",
       "      <td>10893</td>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3452</td>\n",
       "      <td>0.4155</td>\n",
       "      <td>0.377102</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3452</td>\n",
       "      <td>0.4155</td>\n",
       "      <td>0.989828</td>\n",
       "      <td>(15, 23]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2003 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id                    db_id  bo_ids  bo_used  \\\n",
       "6             3           movie_platform      45        0   \n",
       "4             5           movie_platform      82        1   \n",
       "14            7           movie_platform     136        1   \n",
       "24            8           movie_platform     100        1   \n",
       "27           13           movie_platform      82        1   \n",
       "...         ...                      ...     ...      ...   \n",
       "2002      10924  debit_card_specializing   10923        1   \n",
       "2001      10929  debit_card_specializing   10936        1   \n",
       "1998      10930  debit_card_specializing   10931        1   \n",
       "1999      10939  debit_card_specializing   10918        1   \n",
       "1997      10953  debit_card_specializing   10893        1   \n",
       "\n",
       "                                               keywords  \\\n",
       "6                                                    {}   \n",
       "4             {\"movie_title\": [\"When Will I Be Loved\"]}   \n",
       "14                                                   {}   \n",
       "24    {\"rating_date_utc\": [\"2020-04-01\", \"2020-04-30...   \n",
       "27        {\"movie_title\": [\"Welcome to the Dollhouse\"]}   \n",
       "...                                                 ...   \n",
       "2002                      {\"currency\": [\"euro\", \"EUR\"]}   \n",
       "2001                                {\"chainid\": [\"11\"]}   \n",
       "1998            {\"country\": [\"CZE\"], \"price\": [\"1000\"]}   \n",
       "1999                                                 {}   \n",
       "1997                                                 {}   \n",
       "\n",
       "                                                 values  exec_res_bo  \\\n",
       "6                                                    {}            0   \n",
       "4     {\"movies\": {\"movie_title\": [\"When Will I Be Lo...            1   \n",
       "14                                                   {}            0   \n",
       "24    {\"ratings_users\": {\"rating_date_utc\": [\"2020-0...            0   \n",
       "27    {\"movies\": {\"movie_title\": [\"Welcome to the Do...            1   \n",
       "...                                                 ...          ...   \n",
       "2002  {\"products\": {\"description\": [\"Eurovignette\"]}...            0   \n",
       "2001  {\"gasstations\": {\"chainid\": [\"11\"]}, \"transact...            0   \n",
       "1998              {\"gasstations\": {\"country\": [\"CZE\"]}}            1   \n",
       "1999                                                 {}            1   \n",
       "1997                                                 {}            0   \n",
       "\n",
       "      structural_score_bo  semantic_score_bo  f1_score_bo  target_complexity  \\\n",
       "6                  0.5657             0.8888     0.691364                  9   \n",
       "4                  1.0000             1.0000     1.000000                  7   \n",
       "14                 0.4828             0.9713     0.644995                  5   \n",
       "24                 0.6489             0.9158     0.759587                  9   \n",
       "27                 1.0000             1.0000     1.000000                  8   \n",
       "...                   ...                ...          ...                ...   \n",
       "2002               0.5138             0.7049     0.594367                  9   \n",
       "2001               0.8738             0.9658     0.917499                  8   \n",
       "1998               0.9333             0.9639     0.948353                  9   \n",
       "1999               0.7083             0.7175     0.712870                  8   \n",
       "1997               0.3452             0.4155     0.377102                 23   \n",
       "\n",
       "      exec_res_base  structural_score_base  semantic_score_base  \\\n",
       "6                 0                 0.5657               0.8888   \n",
       "4                 1                 1.0000               1.0000   \n",
       "14                0                 0.4828               0.9713   \n",
       "24                0                 0.6489               0.9158   \n",
       "27                1                 1.0000               1.0000   \n",
       "...             ...                    ...                  ...   \n",
       "2002              0                 0.5138               0.7049   \n",
       "2001              0                 0.8738               0.9658   \n",
       "1998              1                 0.9333               0.9639   \n",
       "1999              1                 0.7083               0.7175   \n",
       "1997              0                 0.3452               0.4155   \n",
       "\n",
       "      f1_score_base  category  \n",
       "6          0.357111    (8, 9]  \n",
       "4          0.519125    (4, 7]  \n",
       "14         0.980458    (4, 7]  \n",
       "24         0.710273    (8, 9]  \n",
       "27         0.534417    (7, 8]  \n",
       "...             ...       ...  \n",
       "2002       0.246214    (7, 9]  \n",
       "2001       0.715064    (7, 9]  \n",
       "1998       0.331370    (7, 9]  \n",
       "1999       0.625524    (7, 9]  \n",
       "1997       0.989828  (15, 23]  \n",
       "\n",
       "[2003 rows x 16 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='sample_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed_sql(samples: dict, tables: dict):\n",
    "    error_ids = []\n",
    "    parsed = defaultdict(dict)\n",
    "    iterator = tqdm(samples, total=len(samples))\n",
    "    for sample in iterator:\n",
    "        db_id = sample.db_id\n",
    "        sample_id = sample.sample_id\n",
    "        iterator.set_description(f\"{db_id}\")\n",
    "        schema = Schema(tables[db_id].db_schema)\n",
    "        sql_i = sample.final.sql\n",
    "        try:\n",
    "            ei = extract_all(sql_i, schema)\n",
    "            assert len(ei['sel']) > 0, f'No selection found-{db_id}-{sample_id}'\n",
    "        except Exception as e:\n",
    "            error_ids.append((db_id, sample_id, str(e)))\n",
    "            parsed[db_id].append(None)\n",
    "            continue\n",
    "        parsed[db_id][sample_id] = ei\n",
    "    return parsed, error_ids\n",
    "\n",
    "train_parsed, error_ids = get_parsed_sql(train_samples, bird_tables)\n",
    "dev_parsed, error_ids = get_parsed_sql(dev_samples, bird_tables)\n",
    "test_parsed, error_ids = get_parsed_sql(test_samples, bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_parsed, f)\n",
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(dev_parsed, f)\n",
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_test_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_parsed, f)\n",
    "\n",
    "with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'rb') as f:\n",
    "    dev_parsed = pickle.load(f)\n",
    "\n",
    "with open(proj_path / 'data' / 'pkl_files' / 'bird_test_parsed.pkl', 'rb') as f:\n",
    "    test_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, product\n",
    "from collections import defaultdict\n",
    "from src.eval_utils import get_all_partial_score\n",
    "\n",
    "def measure_inter_score(parsed1: dict[str, tuple], parsed2: dict[str, tuple]):\n",
    "    results = defaultdict()\n",
    "    assert len(parsed1) == len(parsed2), f\"Length mismatch-1: {len(parsed1)} 2:{len(parsed2)}\"\n",
    "    db_ids = list(parsed1.keys())\n",
    "    for db_id in db_ids:\n",
    "        o1 = parsed1[db_id]\n",
    "        o2 = parsed2[db_id]\n",
    "        n1 = len(o1)\n",
    "        n2 = len(o2)\n",
    "        semantic_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "        structural_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "        overall_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "\n",
    "        idxs = list(product(range(n1), range(n2)))\n",
    "        iterator = tqdm(idxs, total=len(idxs), desc=f\"{db_id}\")\n",
    "        for i, j in iterator:\n",
    "            ei = o1[i]\n",
    "            ej = o2[j]\n",
    "\n",
    "            _, final_score = get_all_partial_score(ei, ej, use_bert=True)\n",
    "\n",
    "            structural_sim[i, j] = final_score['structural']\n",
    "            semantic_sim[i, j] = final_score['semantic']\n",
    "            overall_sim[i, j] = final_score['overall']\n",
    "\n",
    "        results[db_id] = {\n",
    "            'semantic': semantic_sim,\n",
    "            'struct': structural_sim,\n",
    "            'overall': overall_sim\n",
    "        }\n",
    "    return results\n",
    "\n",
    "results = measure_inter_score(dev_parsed, test_parsed)\n",
    "with (proj_path / 'data' / 'pkl_files' / 'bird_dev_test_similarity.pkl').open('wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity between datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_complexity(samples, tables):\n",
    "    cs = []\n",
    "    for s in tqdm(samples, total=len(samples)):\n",
    "        schema = Schema(tables[s.db_id].db_schema)\n",
    "        output = extract_all(s.final.sql, schema)\n",
    "        complexity = get_complexity(output)\n",
    "        cs.append(complexity)\n",
    "    return cs\n",
    "\n",
    "train_complexities = measure_complexity(train_samples, bird_tables)\n",
    "dev_complexities = measure_complexity(dev_samples, bird_tables)\n",
    "test_complexities = measure_complexity(test_samples, bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, n in zip([train_complexities, dev_complexities, test_complexities], ['train', 'dev  ', 'test ']):\n",
    "    print(f'[{n}] Mean={np.mean(c):.4f} +/-{np.std(c):.4f}, Median={np.median(c):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = defaultdict(list)\n",
    "for s in dev_samples:\n",
    "    stats[s.db_id].append(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
