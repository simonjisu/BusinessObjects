{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "proj_path = Path('.').resolve()\n",
    "sys.path.append(str(proj_path))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from src.db_utils import get_schema_str, get_data_dict\n",
    "from src.pymodels import (\n",
    "    DatabaseModel, \n",
    "    SpiderSample, \n",
    "    BirdSample, \n",
    "    BODescription,\n",
    "    SQLResponse\n",
    ")\n",
    "from src.prompts import Prompts\n",
    "from src.database import SqliteDatabase\n",
    "from src.data_preprocess import (\n",
    "    load_raw_data,\n",
    "    process_all_tables,\n",
    "    filter_samples_by_count_spider_bird,\n",
    "    process_samples_bird,\n",
    "    split_train_dev_test,\n",
    "    save_samples_spider_bird,\n",
    "    load_samples_spider_bird,\n",
    ")\n",
    "\n",
    "from src.parsing_sql import Schema, extract_all\n",
    "from src.eval_utils import get_complexity, result_eq, check_if_exists_orderby\n",
    "from run_bo_sql import get_vector_store\n",
    "from copy import deepcopy\n",
    "bird_path = proj_path / 'data' / 'bird'\n",
    "tables, train_data, dev_data = load_raw_data(bird_path, load_test=False)\n",
    "\n",
    "with (proj_path / 'data' / 'bird_description.json').open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "\n",
    "bird_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "train_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_train.json')\n",
    "dev_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_dev.json')\n",
    "test_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.globals import set_llm_cache\n",
    "# from langchain_community.cache import SQLiteCache\n",
    "# set_llm_cache(SQLiteCache(database_path=f\"./cache/valid_bo_bird_dev.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'soccer_2016-0'\n",
    "db = SqliteDatabase(f\"./cache/valid_bo_{file_name}.db\")\n",
    "db.start()\n",
    "c = db.con.cursor()\n",
    "c.execute('BEGIN TRANSACTION')\n",
    "c.execute(\"\"\"\n",
    "DELETE FROM full_llm_cache WHERE response LIKE '%JSONDecodeError%';\n",
    "\"\"\")\n",
    "# c.execute('''\n",
    "# DELETE FROM full_llm_cache\n",
    "# WHERE prompt LIKE '%Which teams have had a player awarded the Purple Cap and another with the Orange Cap%'\n",
    "# ''')\n",
    "db.con.commit()\n",
    "db.close()\n",
    "# JSONDecodeError\n",
    "# ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>llm</th>\n",
       "      <th>idx</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt, llm, idx, response]\n",
       "Index: []"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "df = db.execute(\n",
    "'''\n",
    "SELECT * FROM full_llm_cache\n",
    "WHERE prompt LIKE '%Which teams have had a player awarded the Purple Cap and another with the Orange Cap%'\n",
    "''')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval_utils import get_structural_score, get_all_structural_score, get_all_semantic_score, NLP_SPACY, partial_matching_with_penalty\n",
    "from run_evaluation import get_target_parsed_sql, get_prediction_parsed_sql\n",
    "from run_bo_sql import _get_categories, _format_interval, get_retriever\n",
    "from bert_score import score as bscore\n",
    "from transformers import logging as tfloggings\n",
    "tfloggings.set_verbosity_error()\n",
    "import warnings\n",
    "ds = 'bird'\n",
    "task = 'zero_shot_hint'\n",
    "typ = 'test'\n",
    "scenario = 3\n",
    "description_file = f'{ds}_description.json' # f'{ds}_description.json'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "\n",
    "tables, *_ = load_raw_data(proj_path / 'data' / ds, load_test=False)\n",
    "with (proj_path / 'data' / description_file).open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "tables = process_all_tables(tables, descriptions=all_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_post_fix = f'{ds}_{typ}' if scenario < 0 else f'{ds}_{typ}_{scenario}'\n",
    "final_file = f'final_{file_post_fix}.json'\n",
    "if not (prediction_path / final_file).exists():\n",
    "    all_results = []\n",
    "    paths = sorted(list(prediction_path.glob(f'{file_post_fix}_*.json')))\n",
    "    for p in paths:\n",
    "        with p.open() as f:\n",
    "            results = json.load(f)\n",
    "            for r in results:\n",
    "                r.pop('rationale')\n",
    "                r['db_id'] = p.stem.split('_', 3)[-1]\n",
    "            all_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = all_results\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_{typ}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_db_ids = {r['db_id'] for r in preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [p for p in preds if p['db_id'] in res_db_ids]\n",
    "samples = [s for s in samples if s.db_id in res_db_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "superstore: 100%|██████████| 24/24 [00:00<00:00, 26.37it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_parsed, _ = get_prediction_parsed_sql(preds, tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sample_id for _, sample_id_ast in pred_parsed.items() for sample_id, ast in sample_id_ast.items() if not ast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for p in eval_path.glob(f'{ds}_{typ}_*.json'):\n",
    "    with p.open() as f:\n",
    "        all_results.extend(json.load(f))\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(eval_path / f'{ds}_{typ}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.read_csv(experiment_folder / 'evals' / 'zero_shot' / f'{ds}_{typ}.csv')\n",
    "df_bo = pd.read_csv(experiment_folder / 'evals' / 'valid_bo' / f'{ds}_{typ}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cates = df_base.groupby('db_id')['gold_complexity'].apply(_get_categories).rename('category').apply(_format_interval)\n",
    "df_base = pd.merge(df_base, df_cates.reset_index('db_id', drop=True), left_index=True, right_index=True)\n",
    "\n",
    "df = pd.merge(\n",
    "    left=df_bo,\n",
    "    right=df_base,\n",
    "    how='inner',\n",
    "    on=['db_id', 'sample_id', 'gold_complexity'],\n",
    "    suffixes=('_bo', '_base')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_column = ['db_id', 'retrieved']\n",
    "execution_improvement = df.groupby(group_column)[['exec_result_base', 'exec_result_bo']].sum().diff(axis=1)['exec_result_bo'].rename('execution_improvement')\n",
    "merit_structural = df.groupby(group_column)[['structural_score_base', 'structural_score_bo']].mean().diff(axis=1)['structural_score_bo'].rename('merit_structural')\n",
    "merit_semantic = df.groupby(group_column)[['semantic_score_base', 'semantic_score_bo']].mean().diff(axis=1)['semantic_score_bo'].rename('merit_semantic')\n",
    "merit = df.groupby(group_column)[['f1_score_base', 'f1_score_bo']].mean().diff(axis=1)['f1_score_bo'].rename('merit')\n",
    "\n",
    "ranks = merit.reset_index().groupby(['db_id'])['merit'].rank(method='first', ascending=False).rename('rank').astype(np.int64)\n",
    "merit = pd.concat([merit.reset_index(), ranks], axis=1)\n",
    "merit_by_rank = merit.sort_values(by=['db_id', 'rank'], ascending=True)\n",
    "\n",
    "# merit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "merit\n",
       "True     59\n",
       "False    13\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(merit_by_rank.groupby('db_id')['merit'].mean().sort_values(ascending=False) > 0.0).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bos = defaultdict(list)\n",
    "for x in merit_by_rank.loc[:, ['db_id', 'retrieved']].to_dict(orient='records'):\n",
    "    test_bos[x['db_id']].append(x['retrieved'])\n",
    "\n",
    "n_bos = range(5, 26, 5)\n",
    "test_scenarios = defaultdict(dict)\n",
    "for n_bo in n_bos:\n",
    "    for db_id in test_bos:\n",
    "        test_scenarios[n_bo][db_id] = test_bos[db_id][:n_bo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (experiment_folder / 'test_scenarios.json').open('w') as f:\n",
    "    json.dump(test_scenarios, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.json').open() as f:\n",
    "    all_bos = json.load(f)\n",
    "\n",
    "test_bo_ids = test_scenarios[25]\n",
    "test_bos = defaultdict(list)\n",
    "for db_id, bos in all_bos.items():\n",
    "    if db_id in test_bo_ids:\n",
    "        bo_ids = test_bo_ids[db_id]\n",
    "        test_bos[db_id].extend(list(filter(lambda x: x['sample_id'] in bo_ids, bos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "for db_id, bos in all_bos.items():\n",
    "    for bo in bos:\n",
    "        if not bo.get('question'):\n",
    "            print(db_id, bo['sample_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = get_vector_store(test_bos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [x for x in test_samples if x.db_id in test_bo_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = get_retriever(vectorstore, 'manufactory_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'sample_id': 5318, 'db_id': 'manufactory_1', 'vt': 'SELECT products.name, products.price FROM products'}, page_content=\"The virtual table describes the names and prices of products from the 'products' table.\"),\n",
       " Document(metadata={'sample_id': 5320, 'db_id': 'manufactory_1', 'vt': 'SELECT products.name FROM products WHERE products.price <= [placeholder-type:numeric]'}, page_content=\"The virtual table describes the names of products from the 'products' table that have a retail price less than or equal to a specified value. The placeholder in the WHERE clause represents the maximum price limit for filtering the products.\"),\n",
       " Document(metadata={'sample_id': 5343, 'db_id': 'manufactory_1', 'vt': 'SELECT products.code, products.name, MIN(products.price) FROM products GROUP BY products.name'}, page_content=\"The virtual table displays the unique product codes and names from the 'products' table along with the minimum price for each product. The results are grouped by product name to ensure that each product is listed only once with its lowest price.\")]"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'What are the names and prices of products that cost at least 180, sorted by price decreasing and name ascending?'\n",
    "retriever.invoke(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bos = []\n",
    "n_samples = 10\n",
    "for db_id, group in merit.groupby('db_id'):\n",
    "    # sample by rank, need to check unique ranks\n",
    "    group = group.sort_values('rank').iloc[:n_samples]\n",
    "    final_bos.append(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id</th>\n",
       "      <th>retrieved</th>\n",
       "      <th>merit</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>6776</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>6736</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>6735</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>6784</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>6750</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>6725</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>6786</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>6761</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>6764</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>6774</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         db_id  retrieved     merit  rank\n",
       "32  activity_1       6776 -0.001267     1\n",
       "15  activity_1       6736 -0.001267     1\n",
       "14  activity_1       6735 -0.001267     1\n",
       "37  activity_1       6784 -0.001267     1\n",
       "20  activity_1       6750 -0.001267     1\n",
       "8   activity_1       6725 -0.001267     1\n",
       "39  activity_1       6786 -0.001267     1\n",
       "25  activity_1       6761 -0.001267     1\n",
       "27  activity_1       6764 -0.001267     1\n",
       "30  activity_1       6774 -0.001267     1"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>db_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>epinions_1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>1.142410e-16</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_company</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0.377211</td>\n",
       "      <td>1.635068e-01</td>\n",
       "      <td>0.233700</td>\n",
       "      <td>0.233700</td>\n",
       "      <td>0.233700</td>\n",
       "      <td>0.556600</td>\n",
       "      <td>0.556600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>0.267752</td>\n",
       "      <td>2.932192e-01</td>\n",
       "      <td>-0.113888</td>\n",
       "      <td>-0.068069</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_govt_and_lot</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>small_bank_1</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.224476</td>\n",
       "      <td>1.876357e-01</td>\n",
       "      <td>0.039994</td>\n",
       "      <td>0.039994</td>\n",
       "      <td>0.224476</td>\n",
       "      <td>0.408958</td>\n",
       "      <td>0.408958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poker_player</th>\n",
       "      <td>19.0</td>\n",
       "      <td>-0.082278</td>\n",
       "      <td>1.946600e-03</td>\n",
       "      <td>-0.084500</td>\n",
       "      <td>-0.084500</td>\n",
       "      <td>-0.080663</td>\n",
       "      <td>-0.080663</td>\n",
       "      <td>-0.080663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>browser_web</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.088820</td>\n",
       "      <td>1.922665e-02</td>\n",
       "      <td>-0.094900</td>\n",
       "      <td>-0.094900</td>\n",
       "      <td>-0.094900</td>\n",
       "      <td>-0.094900</td>\n",
       "      <td>-0.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cre_Doc_Template_Mgt</th>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.112200</td>\n",
       "      <td>2.823006e-17</td>\n",
       "      <td>-0.112200</td>\n",
       "      <td>-0.112200</td>\n",
       "      <td>-0.112200</td>\n",
       "      <td>-0.112200</td>\n",
       "      <td>-0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wta_1</th>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.159026</td>\n",
       "      <td>1.591789e-01</td>\n",
       "      <td>-0.294254</td>\n",
       "      <td>-0.294254</td>\n",
       "      <td>-0.170126</td>\n",
       "      <td>-0.170126</td>\n",
       "      <td>0.133629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter_1</th>\n",
       "      <td>16.0</td>\n",
       "      <td>-0.173995</td>\n",
       "      <td>1.617369e-01</td>\n",
       "      <td>-0.283123</td>\n",
       "      <td>-0.283123</td>\n",
       "      <td>-0.283123</td>\n",
       "      <td>0.057822</td>\n",
       "      <td>0.057822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count      mean           std       min       25%  \\\n",
       "db_id                                                                     \n",
       "epinions_1             18.0  0.806900  1.142410e-16  0.806900  0.806900   \n",
       "gas_company            27.0  0.377211  1.635068e-01  0.233700  0.233700   \n",
       "loan_1                 46.0  0.267752  2.932192e-01 -0.113888 -0.068069   \n",
       "local_govt_and_lot     10.0  0.258400  0.000000e+00  0.258400  0.258400   \n",
       "small_bank_1           30.0  0.224476  1.876357e-01  0.039994  0.039994   \n",
       "...                     ...       ...           ...       ...       ...   \n",
       "poker_player           19.0 -0.082278  1.946600e-03 -0.084500 -0.084500   \n",
       "browser_web            10.0 -0.088820  1.922665e-02 -0.094900 -0.094900   \n",
       "cre_Doc_Template_Mgt   30.0 -0.112200  2.823006e-17 -0.112200 -0.112200   \n",
       "wta_1                  30.0 -0.159026  1.591789e-01 -0.294254 -0.294254   \n",
       "twitter_1              16.0 -0.173995  1.617369e-01 -0.283123 -0.283123   \n",
       "\n",
       "                           50%       75%       max  \n",
       "db_id                                               \n",
       "epinions_1            0.806900  0.806900  0.806900  \n",
       "gas_company           0.233700  0.556600  0.556600  \n",
       "loan_1                0.500000  0.500000  0.500000  \n",
       "local_govt_and_lot    0.258400  0.258400  0.258400  \n",
       "small_bank_1          0.224476  0.408958  0.408958  \n",
       "...                        ...       ...       ...  \n",
       "poker_player         -0.080663 -0.080663 -0.080663  \n",
       "browser_web          -0.094900 -0.094900 -0.034100  \n",
       "cre_Doc_Template_Mgt -0.112200 -0.112200 -0.112200  \n",
       "wta_1                -0.170126 -0.170126  0.133629  \n",
       "twitter_1            -0.283123  0.057822  0.057822  \n",
       "\n",
       "[126 rows x 8 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merit.groupby('db_id')['merit'].describe().sort_values('mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank = merit.reset_index().groupby(['db_id'])['merit'].apply(lambda x: rankdata(-x.values, method='min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 31,  1,  1, 16, 16, 16, 31,  1, 31, 16, 31, 31, 31,  1,  1, 16,\n",
       "       16,  1, 16,  1, 31, 16, 16, 16,  1,  1,  1, 31, 31,  1, 16,  1, 31,\n",
       "       31, 31, 16,  1, 16,  1,  1, 16, 16])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank.loc['activity_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'address'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/code/BusinessObjects/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'address'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[226], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_rank\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maddress\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/code/BusinessObjects/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/BusinessObjects/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1431\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/BusinessObjects/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1381\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/BusinessObjects/.venv/lib/python3.11/site-packages/pandas/core/generic.py:4301\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4299\u001b[0m             new_index \u001b[38;5;241m=\u001b[39m index[loc]\n\u001b[1;32m   4300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4301\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   4304\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loc\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_:\n",
      "File \u001b[0;32m~/code/BusinessObjects/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'address'"
     ]
    }
   ],
   "source": [
    "df_rank.loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 4. 3. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2, 1])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2, -1, 0, 3])\n",
    "rank = rankdata(-x, method='average')\n",
    "print(rank)\n",
    "np.argsort(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>sample_id_bo</th>\n",
       "      <th>db_id_bo</th>\n",
       "      <th>retrieved</th>\n",
       "      <th>gold_complexity_bo</th>\n",
       "      <th>structural_score_bo</th>\n",
       "      <th>semantic_score_bo</th>\n",
       "      <th>f1_score_bo</th>\n",
       "      <th>exec_result_bo</th>\n",
       "      <th>sample_id_base</th>\n",
       "      <th>db_id_base</th>\n",
       "      <th>gold_complexity_base</th>\n",
       "      <th>structural_score_base</th>\n",
       "      <th>semantic_score_base</th>\n",
       "      <th>f1_score_base</th>\n",
       "      <th>exec_result_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>student_assessment</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.412207</td>\n",
       "      <td>0</td>\n",
       "      <td>5808</td>\n",
       "      <td>workshop_paper</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>student_assessment</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.666700</td>\n",
       "      <td>0</td>\n",
       "      <td>7247</td>\n",
       "      <td>flight_2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3904</td>\n",
       "      <td>0.3882</td>\n",
       "      <td>0.3892</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>student_assessment</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3067</td>\n",
       "      <td>0.7112</td>\n",
       "      <td>0.428579</td>\n",
       "      <td>0</td>\n",
       "      <td>7322</td>\n",
       "      <td>cre_Doc_Template_Mgt</td>\n",
       "      <td>8</td>\n",
       "      <td>0.3317</td>\n",
       "      <td>0.5843</td>\n",
       "      <td>0.4025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>student_assessment</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3878</td>\n",
       "      <td>0.4971</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>0</td>\n",
       "      <td>6314</td>\n",
       "      <td>e_government</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>student_assessment</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>0.4850</td>\n",
       "      <td>0.5279</td>\n",
       "      <td>0.505542</td>\n",
       "      <td>0</td>\n",
       "      <td>5834</td>\n",
       "      <td>workshop_paper</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18207</th>\n",
       "      <td>857</td>\n",
       "      <td>857</td>\n",
       "      <td>chinook_1</td>\n",
       "      <td>868</td>\n",
       "      <td>8</td>\n",
       "      <td>0.4444</td>\n",
       "      <td>0.4445</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>0</td>\n",
       "      <td>3300</td>\n",
       "      <td>college_1</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18208</th>\n",
       "      <td>312</td>\n",
       "      <td>312</td>\n",
       "      <td>product_catalog</td>\n",
       "      <td>309</td>\n",
       "      <td>9</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.820083</td>\n",
       "      <td>1</td>\n",
       "      <td>5908</td>\n",
       "      <td>cre_Theme_park</td>\n",
       "      <td>7</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.5943</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18209</th>\n",
       "      <td>312</td>\n",
       "      <td>312</td>\n",
       "      <td>product_catalog</td>\n",
       "      <td>329</td>\n",
       "      <td>9</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.820083</td>\n",
       "      <td>0</td>\n",
       "      <td>5908</td>\n",
       "      <td>cre_Theme_park</td>\n",
       "      <td>7</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.5943</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18210</th>\n",
       "      <td>312</td>\n",
       "      <td>312</td>\n",
       "      <td>product_catalog</td>\n",
       "      <td>324</td>\n",
       "      <td>9</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.820083</td>\n",
       "      <td>0</td>\n",
       "      <td>5908</td>\n",
       "      <td>cre_Theme_park</td>\n",
       "      <td>7</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.5943</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18211</th>\n",
       "      <td>312</td>\n",
       "      <td>312</td>\n",
       "      <td>product_catalog</td>\n",
       "      <td>302</td>\n",
       "      <td>9</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.820083</td>\n",
       "      <td>1</td>\n",
       "      <td>5908</td>\n",
       "      <td>cre_Theme_park</td>\n",
       "      <td>7</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.5943</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3537 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id  sample_id_bo            db_id_bo  retrieved  \\\n",
       "326          103           103  student_assessment         60   \n",
       "327           65            65  student_assessment         60   \n",
       "328           89            89  student_assessment         60   \n",
       "329           81            81  student_assessment         60   \n",
       "330          107           107  student_assessment         60   \n",
       "...          ...           ...                 ...        ...   \n",
       "18207        857           857           chinook_1        868   \n",
       "18208        312           312     product_catalog        309   \n",
       "18209        312           312     product_catalog        329   \n",
       "18210        312           312     product_catalog        324   \n",
       "18211        312           312     product_catalog        302   \n",
       "\n",
       "       gold_complexity_bo  structural_score_bo  semantic_score_bo  \\\n",
       "326                    10               0.3391             0.5255   \n",
       "327                     4               0.6667             0.6667   \n",
       "328                     9               0.3067             0.7112   \n",
       "329                    10               0.3878             0.4971   \n",
       "330                     9               0.4850             0.5279   \n",
       "...                   ...                  ...                ...   \n",
       "18207                   8               0.4444             0.4445   \n",
       "18208                   9               0.8000             0.8412   \n",
       "18209                   9               0.8000             0.8412   \n",
       "18210                   9               0.8000             0.8412   \n",
       "18211                   9               0.8000             0.8412   \n",
       "\n",
       "       f1_score_bo  exec_result_bo  sample_id_base            db_id_base  \\\n",
       "326       0.412207               0            5808        workshop_paper   \n",
       "327       0.666700               0            7247              flight_2   \n",
       "328       0.428579               0            7322  cre_Doc_Template_Mgt   \n",
       "329       0.435700               0            6314          e_government   \n",
       "330       0.505542               0            5834        workshop_paper   \n",
       "...            ...             ...             ...                   ...   \n",
       "18207     0.444500               0            3300             college_1   \n",
       "18208     0.820083               1            5908        cre_Theme_park   \n",
       "18209     0.820083               0            5908        cre_Theme_park   \n",
       "18210     0.820083               0            5908        cre_Theme_park   \n",
       "18211     0.820083               1            5908        cre_Theme_park   \n",
       "\n",
       "       gold_complexity_base  structural_score_base  semantic_score_base  \\\n",
       "326                       4                 1.0000               1.0000   \n",
       "327                       7                 0.3904               0.3882   \n",
       "328                       8                 0.3317               0.5843   \n",
       "329                       7                 1.0000               1.0000   \n",
       "330                       8                 1.0000               1.0000   \n",
       "...                     ...                    ...                  ...   \n",
       "18207                    11                 1.0000               1.0000   \n",
       "18208                     7                 0.4975               0.5943   \n",
       "18209                     7                 0.4975               0.5943   \n",
       "18210                     7                 0.4975               0.5943   \n",
       "18211                     7                 0.4975               0.5943   \n",
       "\n",
       "       f1_score_base  exec_result_base  \n",
       "326           1.0000                 1  \n",
       "327           0.3892                 1  \n",
       "328           0.4025                 1  \n",
       "329           1.0000                 1  \n",
       "330           1.0000                 1  \n",
       "...              ...               ...  \n",
       "18207         1.0000                 1  \n",
       "18208         0.5380                 0  \n",
       "18209         0.5380                 0  \n",
       "18210         0.5380                 0  \n",
       "18211         0.5380                 0  \n",
       "\n",
       "[3537 rows x 16 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sample_id', 'db_id', 'gold_complexity', 'structural_score',\n",
       "       'semantic_score', 'f1_score', 'exec_result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "db_id\n",
       "activity_1           0.682253\n",
       "aircraft             0.665956\n",
       "allergy_1            0.756495\n",
       "apartment_rentals    0.793388\n",
       "architecture         0.739333\n",
       "                       ...   \n",
       "wine_1               0.810444\n",
       "workshop_paper       0.806467\n",
       "world_1              0.716850\n",
       "wrestler             0.786637\n",
       "wta_1                0.892736\n",
       "Name: structural_score, Length: 158, dtype: float64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base.groupby(['db_id'])['structural_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retrieved  db_id      \n",
       "16         farm           0.862625\n",
       "18         farm           0.623800\n",
       "19         farm           0.623800\n",
       "20         farm           0.862625\n",
       "21         farm           0.623800\n",
       "                            ...   \n",
       "7978       dog_kennels    0.542933\n",
       "7981       dog_kennels    0.679733\n",
       "7984       dog_kennels    0.679733\n",
       "7986       dog_kennels    0.542933\n",
       "7988       dog_kennels    0.542933\n",
       "Name: structural_score, Length: 3209, dtype: float64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bo.groupby(['retrieved', 'db_id'])['structural_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "db_id       retrieved\n",
       "activity_1  6711         0.665518\n",
       "            6713         0.665518\n",
       "            6714         0.538511\n",
       "            6716         0.538511\n",
       "            6717         0.546622\n",
       "                           ...   \n",
       "wta_1       7472         0.272850\n",
       "            7475         0.272850\n",
       "            7476         0.272850\n",
       "            7477         0.698900\n",
       "            7480         0.272850\n",
       "Name: structural_score, Length: 3209, dtype: float64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['db_id', 'retrieved'])['structural_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sample_id', 'db_id', 'retrieved', 'gold_complexity',\n",
       "       'structural_score', 'semantic_score', 'f1_score', 'exec_result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 83 files\n"
     ]
    }
   ],
   "source": [
    "paths = sorted(list(prediction_path.glob(f'{ds}_{typ}_*.json')))\n",
    "print(f'Found {len(paths)} files')\n",
    "\n",
    "# get target_parsed_sql\n",
    "file_name = f'{ds}_{typ}_parsed.pkl'\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_{typ}.json')\n",
    "if not (eval_path / file_name).exists():\n",
    "    target_parsed, error_ids = get_target_parsed_sql(samples, tables)\n",
    "    with open(eval_path / file_name, 'wb') as f:\n",
    "        pickle.dump(target_parsed, f)\n",
    "    print(f'Error parsing target {ds}_{typ}: {len(error_ids)}')\n",
    "\n",
    "with (eval_path / file_name).open('rb') as f:\n",
    "    target_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in paths:\n",
    "    with p.open() as f:\n",
    "        preds = json.load(f)\n",
    "    output_results = []\n",
    "    db_id = p.stem.split('_', 2)[-1].split('-')[0]\n",
    "    file_name = f'{p.stem}_parsed_pred.pkl'\n",
    "    if not (eval_path / file_name).exists():\n",
    "        pred_parsed, _ = get_prediction_parsed_sql(preds, tables, patch_db_id=db_id)\n",
    "        with open(eval_path / file_name, 'wb') as f:\n",
    "            pickle.dump(pred_parsed, f)\n",
    "        # print(f'Error parsing pred {args.ds}_{args.type}: {len(error_ids)}')\n",
    "\n",
    "    with (eval_path / file_name).open('rb') as f:\n",
    "        pred_parsed = pickle.load(f)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_id': 2528,\n",
       " 'gold_sql': 'SELECT SUM(T1.Installs), T2.Translated_Review FROM playstore AS T1 INNER JOIN user_reviews AS T2 ON T1.App = T2.App WHERE T1.\"Content Rating\" = \\'Adults only 18+\\'',\n",
       " 'retrieved': 2538,\n",
       " 'rationale': [\"Identify the relevant tables: 'playstore' for app details and 'user_reviews' for user reviews.\",\n",
       "  \"Determine the filtering criteria: content rating of 'Adults only 18+'.\",\n",
       "  'Calculate the total installs for apps that meet the content rating criteria.',\n",
       "  \"Join the 'playstore' table with the 'user_reviews' table on the app name to access translated reviews.\",\n",
       "  'Use SUM() to calculate total installs and select translated reviews from the joined tables.',\n",
       "  'Group the results by translated review to get a summary of installs per review.'],\n",
       " 'pred_sql': \"SELECT SUM(playstore.installs) AS total_installs, user_reviews.translated_review FROM playstore INNER JOIN user_reviews ON playstore.app = user_reviews.app WHERE playstore.content_rating = 'Adults only 18+' GROUP BY user_reviews.translated_review;\",\n",
       " 'token_usage': {'tokens': 1171, 'cost': 0.00025079999999999997}}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_id_retrieved = [(db_id, x['retrieved']) for x in preds]\n",
    "len(db_id_retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_parsed_outputs = [target_parsed[db_id][x['sample_id']] for x in preds]\n",
    "pred_parsed_outputs = [pred_parsed[db_id][x['sample_id']] for x in preds]\n",
    "\n",
    "structural_scores = get_all_structural_score(pred_parsed_outputs, target_parsed_outputs)\n",
    "semantic_scores = get_all_semantic_score(pred_parsed_outputs, target_parsed_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score\n",
    "epsilon = 1e-9\n",
    "structural_scores = np.array(structural_scores)\n",
    "semantic_scores = np.array(semantic_scores)\n",
    "f1_scores = 2 * (structural_scores * semantic_scores) / (structural_scores + semantic_scores + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify(source_asts, target_asts):\n",
    "    source_str = [str(ast) if use_bert else NLP_SPACY(str(ast)) for ast in source_asts]\n",
    "    target_str = [str(ast) if use_bert else NLP_SPACY(str(ast)) for ast in target_asts]\n",
    "    return list(zip(source_str, target_str))\n",
    "\n",
    "args = ['table_asts', 'sel_asts', 'cond_asts', 'agg_asts', 'orderby_asts', 'subqueries', 'distinct', 'limit']    \n",
    "use_bert = True\n",
    "rescale_with_baseline = True\n",
    "criteria = 'tsed'\n",
    "penalty = 0.01\n",
    "\n",
    "all_pairs = []\n",
    "all_idxes = defaultdict(dict)\n",
    "all_results = defaultdict(dict)\n",
    "for k, (source_output, target_output) in enumerate(zip(pred_parsed_outputs, target_parsed_outputs)):\n",
    "    for arg in args:\n",
    "        source_exists = bool(source_output[arg]) if arg != 'subqueries' else bool(source_output[arg][1:])\n",
    "        target_exists = bool(target_output[arg]) if arg != 'subqueries' else bool(target_output[arg][1:])\n",
    "        if target_exists and source_exists:\n",
    "            if arg in ['sel_asts', 'cond_asts', 'agg_asts', 'orderby_asts', 'table_asts']:\n",
    "                source = [ast for _, ast, _ in source_output[arg]]\n",
    "                target = [ast for _, ast, _ in target_output[arg]]\n",
    "                pairs = stringify(source, target)\n",
    "                idxes = list(range(len(all_pairs), len(all_pairs)+len(pairs)))\n",
    "                all_pairs.extend(pairs)\n",
    "                all_idxes[k][arg] = idxes\n",
    "                semantic_score = -1\n",
    "            elif arg == 'subqueries':\n",
    "                source = source_output[arg][1:]\n",
    "                target = target_output[arg][1:]\n",
    "                pairs = stringify(source, target)\n",
    "                idxes = list(range(len(all_pairs), len(all_pairs)+len(pairs)))\n",
    "                all_pairs.extend(pairs)\n",
    "                all_idxes[k][arg] = idxes\n",
    "                semantic_score = -1\n",
    "            elif arg in ['distinct', 'limit']:\n",
    "                semantic_score = 1.0 if criteria == 'tsed' else 0.0\n",
    "        elif target_exists ^ source_exists:\n",
    "            semantic_score = 0.0 if criteria == 'tsed' else np.infty\n",
    "        else:\n",
    "            # they don't exist in both so, we can't measure the score\n",
    "            semantic_score = None\n",
    "        all_results[k][arg] = semantic_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('playstore', 'playstore'),\n",
       " ('user_reviews', 'user_reviews'),\n",
       " ('SUM(playstore.installs)', 'SUM(playstore.installs)'),\n",
       " ('user_reviews.translated_review', 'user_reviews.translated_review'),\n",
       " (\"playstore.content_rating = '[placeholder-type:string]'\",\n",
       "  'playstore.\"content rating\" = \\'[placeholder-type:string]\\'')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table_asts': [0, 1], 'sel_asts': [2, 3], 'cond_asts': [4]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_idxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_str, target_str = zip(*all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_str_list = []\n",
    "target_str_list = []\n",
    "sparse_idxes = []\n",
    "idx2arg = defaultdict()\n",
    "for k, key_idxes in all_idxes.items():\n",
    "    for arg, idxes in key_idxes.items():\n",
    "        s, e = idxes[0], idxes[-1]+1\n",
    "        xs = list(product(source_str[s:e], target_str[s:e]))\n",
    "        s_str, t_str = list(zip(*xs))\n",
    "        idx = len(xs) + (sparse_idxes[-1] if sparse_idxes else 0)\n",
    "        sparse_idxes.append(idx)\n",
    "        idx2arg[idx] = (k, arg)\n",
    "        source_str_list.extend(s_str)\n",
    "        target_str_list.extend(t_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'table_asts')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2arg[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, pairwise\n",
    "\n",
    "source_str, target_str = zip(*all_pairs)\n",
    "n, m = len(source_str), len(target_str)\n",
    "if use_bert:\n",
    "    source_str_list = []\n",
    "    target_str_list = []\n",
    "    sparse_idxes = []\n",
    "    idx2arg = defaultdict(dict)\n",
    "    for k, key_idxes in all_idxes.items():\n",
    "        for arg, idxes in key_idxes.items():\n",
    "            s, e = idxes[0], idxes[-1]+1\n",
    "            xs = list(product(source_str[s:e], target_str[s:e]))\n",
    "            s_str, t_str = list(zip(*xs))\n",
    "            idx = len(xs) + (sparse_idxes[-1] if sparse_idxes else 0)\n",
    "            sparse_idxes.append(idx)\n",
    "            idx2arg[idx] = (k, arg)\n",
    "            source_str_list.extend(s_str)\n",
    "            target_str_list.extend(t_str)\n",
    "    with warnings.catch_warnings(action='ignore'):\n",
    "        *_, F1 = bscore(source_str_list, target_str_list, lang='en', verbose=False, rescale_with_baseline=rescale_with_baseline, device='cuda')\n",
    "    scores = F1.numpy()\n",
    "else:\n",
    "    scores = []\n",
    "    for k, key_idxes in all_idxes.items():\n",
    "        for key, idxes in key_idxes.items():\n",
    "            s, e = idxes[0], idxes[-1]+1\n",
    "            s_str, t_str = list(zip(*product(source_str[s:e], target_str[s:e])))\n",
    "            for i, s in enumerate(s_str):\n",
    "                for j, t in enumerate(t_str):\n",
    "                    scores.append(s.similarity(t))\n",
    "    scores = np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0000007 , 0.04659584, 0.04659584, 1.0000014 ], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[i:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in pairwise([0] + sparse_idxes):\n",
    "    n = int(np.sqrt(j - i))\n",
    "    matrix = scores[i:j].reshape(n, n)\n",
    "    k, arg = idx2arg[j]\n",
    "    *_, final_score = partial_matching_with_penalty(matrix, penalty, maximize=True)\n",
    "    all_results[k][arg] = final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_output = pred_parsed_outputs[0]\n",
    "target_output = target_parsed_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = []\n",
    "all_idxes = {}\n",
    "results = {}\n",
    "for arg in args:\n",
    "    source_exists = bool(source_output[arg]) if arg != 'subqueries' else bool(source_output[arg][1:])\n",
    "    target_exists = bool(target_output[arg]) if arg != 'subqueries' else bool(target_output[arg][1:])\n",
    "    if target_exists and source_exists:\n",
    "        if arg in ['sel_asts', 'cond_asts', 'agg_asts', 'orderby_asts', 'table_asts']:\n",
    "            source = [ast for _, ast, _ in source_output[arg]]\n",
    "            target = [ast for _, ast, _ in target_output[arg]]\n",
    "            if target:\n",
    "                pairs = stringify(source, target)\n",
    "                idxes = list(range(len(all_pairs), len(all_pairs)+len(pairs)))\n",
    "                all_pairs.extend(pairs)\n",
    "                all_idxes[arg] = idxes\n",
    "            semantic_score = -1\n",
    "        elif arg == 'subqueries':\n",
    "            source = source_output[arg][1:]\n",
    "            target = target_output[arg][1:]\n",
    "            if target:\n",
    "                pairs = stringify(source, target)\n",
    "                idxes = list(range(len(all_pairs), len(all_pairs)+len(pairs)))\n",
    "                all_pairs.extend(pairs)\n",
    "                all_idxes[arg] = idxes\n",
    "            semantic_score = -1\n",
    "        elif arg in ['distinct', 'limit']:\n",
    "            semantic_score = 1.0 if criteria == 'tsed' else 0.0\n",
    "    elif target_exists ^ source_exists:\n",
    "        semantic_score = 0.0 if criteria == 'tsed' else np.infty\n",
    "    else:\n",
    "        # they don't exist in both so, we can't measure the score\n",
    "        semantic_score = None\n",
    "        # score = 0.0 if criteria == 'tsed' else np.infty\n",
    "    results[arg] = semantic_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "source_str, target_str = zip(*all_pairs)\n",
    "n, m = len(source_str), len(target_str)\n",
    "if use_bert:\n",
    "    source_str_list, target_str_list = list(zip(*product(source_str, target_str)))\n",
    "    with warnings.catch_warnings(action='ignore'):\n",
    "        *_, F1 = bscore(source_str_list, target_str_list, lang='en', verbose=False, rescale_with_baseline=rescale_with_baseline, device='cuda')\n",
    "    matrix = F1.numpy().reshape(n, m)\n",
    "else:\n",
    "    matrix = np.zeros((n, m), dtype=np.float32)\n",
    "    for i, s in enumerate(source_str):\n",
    "        for j, t in enumerate(target_str):\n",
    "            matrix[i, j] = s.similarity(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = 0.01\n",
    "\n",
    "*_, final_score = partial_matching_with_penalty(matrix, penalty, maximize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.947024"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_path = proj_path / 'data' / 'spider'\n",
    "# tables, train_data, dev_data = load_raw_data(spider_path, load_test=False)\n",
    "\n",
    "# with (proj_path / 'data' / 'description.json').open() as f:\n",
    "#     all_descriptions = json.load(f)\n",
    "# seed = 42\n",
    "# all_data = filter_samples_by_count_spider_bird(train_data+dev_data, n=10)\n",
    "\n",
    "# with open(proj_path / 'data' / 'bird_skip.txt') as f:\n",
    "#     skip = [int(line.strip()) for line in f]\n",
    "\n",
    "# bird_samples = process_samples_bird(all_data, bird_tables, skip=skip)\n",
    "# train_samples, dev_samples, test_samples = split_train_dev_test(bird_samples, train_ratio=0.6, dev_ratio=0.2, seed=seed)\n",
    "\n",
    "# save_samples_spider_bird(train_samples, proj_path / 'data' / 'bird_train.json')\n",
    "# save_samples_spider_bird(dev_samples, proj_path / 'data' / 'bird_dev.json')\n",
    "# save_samples_spider_bird(test_samples, proj_path / 'data' / 'bird_test.json')\n",
    "# print(len(train_samples), len(dev_samples), len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_folder = proj_path / 'experiments' / 'bird'\n",
    "# prediction_path = experiment_folder / 'predictions' / 'create_bo'\n",
    "# tables = bird_tables\n",
    "# bos = []\n",
    "# for p in prediction_path.glob('bird_train_bo_*.json'):\n",
    "#     with p.open() as f:\n",
    "#         bos = json.load(f)\n",
    "\n",
    "#     db_id = p.stem.split('_', 3)[-1]\n",
    "#     schema = Schema(tables[db_id].db_schema)\n",
    "#     for bo in bos:\n",
    "#         output = extract_all(bo['gold_sql'], schema)\n",
    "#         bo['gold_complexity'] = get_complexity(output)\n",
    "    \n",
    "#     with p.open('w') as f:\n",
    "#         json.dump(bos, f, indent=4)\n",
    "\n",
    "# bos = {}\n",
    "# for p in prediction_path.glob('bird_train_bo_*.json'):\n",
    "#     db_id = p.stem.split('_', 3)[-1]\n",
    "#     with p.open() as f:\n",
    "#         bos[db_id] = json.load(f)\n",
    "\n",
    "# with (experiment_folder / 'predictions' / 'create_bo' / f'final_bird_train_bo.json').open('w') as f:\n",
    "#     json.dump(bos, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_bo_sql import Sampler, get_vector_store, get_retriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "k_retrieval: int = 5  # for test\n",
    "n_retrieval: int = 1   # for test\n",
    "score_threshold: float = 0.65\n",
    "use_reranker: bool = True\n",
    "# TODO: run spider 4567\n",
    "ds = 'bird'\n",
    "task = 'zero_shot'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "\n",
    "bo_path = experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.json'\n",
    "with bo_path.open() as f:\n",
    "    bos = json.load(f)\n",
    "\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_dev.json')\n",
    "df = pd.read_csv(experiment_folder / 'evals' / 'zero_shot' / f'bird_dev.csv')\n",
    "df_score = df.loc[:, ['sample_id', 'db_id', 'exec_result']]\n",
    "df_error = df_score.loc[df_score['exec_result'] == 0, ['db_id', 'sample_id']]\n",
    "error_ids = df_error['sample_id'].tolist()\n",
    "samples = list(filter(lambda x: x.sample_id in error_ids, samples))\n",
    "\n",
    "\n",
    "samples_by_db_id = defaultdict(list)\n",
    "for sample in samples:\n",
    "    samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "cross_encoder = HuggingFaceCrossEncoder(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "vectorstore = get_vector_store(bos)\n",
    "# dev_samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_dev.json')\n",
    "# pred_res = defaultdict(dict)  # db_id -> train_bo -> list[dict]\n",
    "# for p in prediction_path.glob(f'{ds}_dev_*.json'):\n",
    "#     name = p.stem.split('_', 2)[-1]\n",
    "#     db_id, idx = name.split('-')\n",
    "#     with p.open() as f:\n",
    "# \n",
    "#     for r in res:\n",
    "#         train_bo_id = r['retrieved']\n",
    "#         if not pred_res[db_id].get(train_bo_id):\n",
    "#             pred_res[db_id][train_bo_id] = []\n",
    "#         pred_res[db_id][r['retrieved']].append(r)\n",
    "#     break\n",
    "\n",
    "# save_path = prediction_path / f'final_{ds}_dev.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = []\n",
    "\n",
    "for db_id, samples in samples_by_db_id.items():\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type='similarity_score_threshold',\n",
    "        search_kwargs={\n",
    "            'k': k_retrieval, \n",
    "            'score_threshold': score_threshold, \n",
    "            'filter': {'db_id': db_id, 'sample_id': {'$nin' : sample_ids}},\n",
    "        }\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7ff2c8db3e90>, search_type='similarity_score_threshold', search_kwargs={'k': 5, 'score_threshold': 0.65, 'filter': {'db_id': 'movie_platform', 'sample_id': {'$nin': []}}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3385365853658537"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(eval_path / f'bird_dev.csv')\n",
    "df_score = df.loc[:, ['sample_id', 'db_id', 'exec_result']]\n",
    "df_score['exec_result'].sum() / len(df_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "n_sample = 3\n",
    "n_stop = 50\n",
    "typ = 'dev'\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_{typ}.json')\n",
    "df = pd.read_csv(experiment_folder / 'evals' / 'zero_shot' / f'{ds}_dev.csv')\n",
    "df_score = df.loc[:, ['sample_id', 'db_id', 'exec_result']]\n",
    "df_error = df_score.loc[df_score['exec_result'] == 0, ['db_id', 'sample_id']]\n",
    "error_ids = df_error['sample_id'].tolist()\n",
    "samples = list(filter(lambda x: x.sample_id in error_ids, samples))\n",
    "\n",
    "with open(experiment_folder / f'partial_{ds}_db_ids.json') as f:\n",
    "    partial_db_ids = json.load(f)\n",
    "\n",
    "bo_path = experiment_folder / 'predictions' / 'create_bo' / f'final_{ds}_train_bo.json'\n",
    "assert bo_path.exists(), 'Run with the `task=create_bo, type=train` first'\n",
    "with bo_path.open() as f:\n",
    "    bos = json.load(f)\n",
    "\n",
    "# with open(experiment_folder / f'partial_{ds}_db_ids.json', 'w') as f:\n",
    "#     json.dump(partial_db_ids, f, indent=4)\n",
    "\n",
    "sampler = Sampler(bos)\n",
    "\n",
    "sampled_bos = {}\n",
    "for db_id_group in partial_db_ids:\n",
    "    sampled_bos[str(db_id_group)] = defaultdict()\n",
    "    for db_id in partial_db_ids[str(db_id_group)]:\n",
    "        x_samples = list(filter(lambda x: x.db_id == db_id, samples))\n",
    "        for idx_bos, train_bos in enumerate(sampler.sample(db_id, n_sample, n_stop, rt_idx=False)):\n",
    "            # print(f'{db_id}-{idx_bos} :', f'{len(train_bos)}', f'{len(list(product(train_bos, x_samples)))}')\n",
    "            sampled_bos[str(db_id_group)][f'{db_id}-{idx_bos}'] = {\n",
    "                'train_bos': train_bos,\n",
    "                'n_iter': len(list(product(train_bos, x_samples))), \n",
    "                'total_bos_in_batch': len(train_bos),\n",
    "                'total_samples_in_batch': len(x_samples)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'bird'\n",
    "task = 'zero_shot'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "typ = 'dev'\n",
    "samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_{typ}.json')\n",
    "df = pd.read_csv(experiment_folder / 'evals' / 'zero_shot' / f'{ds}_dev.csv')\n",
    "df_error = df.loc[df['exec_result'] == 0]\n",
    "error_ids = df_error['sample_id'].tolist()\n",
    "samples = list(filter(lambda x: x.sample_id in error_ids, samples))\n",
    "\n",
    "with (experiment_folder / f'partial_{ds}_batch.json').open('r') as f:\n",
    "    partial_batch = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_files = 0\n",
    "for db_group_id, batch in partial_batch.items():\n",
    "    count_files += len(batch)\n",
    "count_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] before: file = 82 bos= 644 n_iter= 8394\n",
      "[0] after: file = 59 bos= 511 n_iter= 1957\n",
      "[1] before: file = 50 bos= 411 n_iter= 4583\n",
      "[1] after: file = 45 bos= 367 n_iter= 881\n",
      "[2] before: file = 62 bos= 517 n_iter= 6707\n",
      "[2] after: file = 46 bos= 396 n_iter= 1184\n",
      "[3] before: file = 75 bos= 690 n_iter= 10838\n",
      "[3] after: file = 52 bos= 505 n_iter= 1900\n",
      "[4] before: file = 68 bos= 561 n_iter= 6246\n",
      "[4] after: file = 53 bos= 470 n_iter= 1264\n",
      "[5] before: file = 64 bos= 508 n_iter= 6218\n",
      "[5] after: file = 50 bos= 409 n_iter= 1073\n",
      "[6] before: file = 70 bos= 573 n_iter= 7298\n",
      "[6] after: file = 52 bos= 436 n_iter= 1577\n",
      "[7] before: file = 70 bos= 583 n_iter= 8072\n",
      "[7] after: file = 50 bos= 434 n_iter= 1442\n"
     ]
    }
   ],
   "source": [
    "new_partial_batch = defaultdict()\n",
    "to_be = 30\n",
    "for db_id_group, batches in partial_batch.items():\n",
    "    new_batch = defaultdict(dict)\n",
    "    db_id_count = defaultdict(int)\n",
    "    for file_name, batch in batches.items():\n",
    "        db_id, idx = file_name.split('-')\n",
    "        x_samples = list(filter(lambda x: x.db_id == db_id, samples))\n",
    "        if db_id_count[db_id] >= to_be:\n",
    "            # print('drop ', file_name)\n",
    "            continue\n",
    "\n",
    "        if db_id_count[db_id] + len(batch['train_bos']) < to_be:\n",
    "            train_bos = batch['train_bos']\n",
    "            new_batch[file_name] = {\n",
    "                'train_bos': train_bos,\n",
    "                'n_iter': len(list(product(train_bos, x_samples))),\n",
    "                'total_bos_in_batch': len(train_bos),\n",
    "                'total_samples_in_batch': len(x_samples)\n",
    "            }\n",
    "            db_id_count[db_id] += len(train_bos)\n",
    "        else: # count + len(batch['train_bos']) > to_be:\n",
    "            n = to_be - db_id_count[db_id]\n",
    "            train_bos = batch['train_bos'][:n]\n",
    "            new_batch[file_name] = {\n",
    "                'train_bos': train_bos,\n",
    "                'n_iter': len(list(product(train_bos, x_samples))),\n",
    "                'total_bos_in_batch': len(train_bos),\n",
    "                'total_samples_in_batch': len(x_samples)\n",
    "            }\n",
    "            db_id_count[db_id] += len(train_bos)\n",
    "    \n",
    "    new_partial_batch[db_id_group] = new_batch\n",
    "    \n",
    "    print(f'[{db_id_group}] before: file = {len(batches)} bos=' , sum([len(v['train_bos']) for v in batches.values()]), 'n_iter=', sum([v['n_iter'] for v in batches.values()]))\n",
    "    print(f'[{db_id_group}] after: file = {len(new_batch)} bos=' , sum([len(v['train_bos']) for v in new_batch.values()]), 'n_iter=', sum([v['n_iter'] for v in new_batch.values()]))\n",
    "    print(f'[{db_id_group}] count = {db_id_count}')\n",
    "# with (experiment_folder / f'partial_{ds}_batch.json').open('w') as f:\n",
    "#     json.dump(new_partial_batch, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (experiment_folder / f'partial_{ds}_batch-back.json').open('r') as f:\n",
    "    partial_batch = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_store-0 before: n_retrieved = 12 | bos = 12 | 132  --> 60\n",
      "app_store-1 before: n_retrieved = 12 | bos = 12 | 132  --> 60\n",
      "app_store-2 before: n_retrieved = 6 | bos = 6 | 66  --> 30\n",
      "app_store-3 not found: 3, delete\n",
      "app_store-4 not found: 1, delete\n",
      "authors-0 before: n_retrieved = 15 | bos = 15 | 510  --> 255\n",
      "authors-1 before: n_retrieved = 15 | bos = 15 | 510  --> 255\n",
      "authors-2 not found: 15, delete\n",
      "authors-3 not found: 5, delete\n",
      "books-0 before: n_retrieved = 15 | bos = 15 | 585  --> 180\n",
      "books-1 before: n_retrieved = 15 | bos = 15 | 585  --> 180\n",
      "books-2 not found: 15, delete\n",
      "books-3 not found: 5, delete\n",
      "california_schools-0 before: n_retrieved = 15 | bos = 15 | 255  --> 225\n",
      "california_schools-1 before: n_retrieved = 15 | bos = 15 | 255  --> 225\n",
      "california_schools-2 not found: 13, delete\n",
      "california_schools-3 not found: 6, delete\n",
      "california_schools-4 not found: 1, delete\n",
      "college_completion-0 before: n_retrieved = 15 | bos = 15 | 225  --> 225\n",
      "college_completion-1 before: n_retrieved = 15 | bos = 15 | 225  --> 225\n",
      "college_completion-2 not found: 14, delete\n",
      "college_completion-3 not found: 1, delete\n",
      "computer_student-0 before: n_retrieved = 15 | bos = 15 | 210  --> 165\n",
      "computer_student-1 before: n_retrieved = 15 | bos = 15 | 210  --> 165\n",
      "computer_student-2 not found: 11, delete\n",
      "computer_student-3 not found: 2, delete\n",
      "genes-0 before: n_retrieved = 12 | bos = 12 | 48  --> 48\n",
      "genes-1 before: n_retrieved = 1 | bos = 1 | 4  --> 4\n",
      "human_resources-0 before: n_retrieved = 15 | bos = 15 | 165  --> 135\n",
      "human_resources-1 before: n_retrieved = 14 | bos = 14 | 154  --> 126\n",
      "human_resources-2 before: n_retrieved = 6 | bos = 1 | 66  --> 54\n",
      "menu-0 before: n_retrieved = 15 | bos = 15 | 315  --> 210\n",
      "menu-1 before: n_retrieved = 15 | bos = 15 | 315  --> 210\n",
      "menu-2 not found: 14, delete\n",
      "menu-3 not found: 6, delete\n",
      "music_tracker-0 before: n_retrieved = 12 | bos = 12 | 108  --> 48\n",
      "music_tracker-1 before: n_retrieved = 11 | bos = 11 | 99  --> 44\n",
      "music_tracker-2 before: n_retrieved = 4 | bos = 4 | 36  --> 16\n",
      "olympics-0 before: n_retrieved = 15 | bos = 15 | 495  --> 240\n",
      "olympics-1 before: n_retrieved = 15 | bos = 15 | 495  --> 240\n",
      "olympics-2 not found: 15, delete\n",
      "olympics-3 not found: 5, delete\n",
      "regional_sales-0 before: n_retrieved = 15 | bos = 15 | 480  --> 435\n",
      "regional_sales-1 before: n_retrieved = 15 | bos = 15 | 480  --> 435\n",
      "regional_sales-2 not found: 15, delete\n",
      "regional_sales-3 not found: 5, delete\n",
      "retail_world-0 before: n_retrieved = 12 | bos = 12 | 156  --> 48\n",
      "retail_world-1 before: n_retrieved = 11 | bos = 11 | 143  --> 44\n",
      "retail_world-2 before: n_retrieved = 4 | bos = 4 | 52  --> 16\n",
      "retail_world-3 before: n_retrieved = 3 | bos = 3 | 39  --> 12\n",
      "retail_world-4 not found: 3, delete\n",
      "retail_world-5 not found: 3, delete\n",
      "retail_world-6 not found: 3, delete\n",
      "retail_world-7 not found: 1, delete\n",
      "retails-0 before: n_retrieved = 12 | bos = 12 | 588  --> 312\n",
      "retails-1 before: n_retrieved = 12 | bos = 12 | 588  --> 312\n",
      "shakespeare-0 before: n_retrieved = 12 | bos = 12 | 264  --> 192\n",
      "shakespeare-1 before: n_retrieved = 12 | bos = 12 | 264  --> 192\n",
      "shakespeare-2 before: n_retrieved = 12 | bos = 6 | 264  --> 192\n",
      "shakespeare-3 not found: 7, delete\n",
      "shakespeare-4 not found: 6, delete\n",
      "shakespeare-5 not found: 1, delete\n",
      "shipping-0 before: n_retrieved = 15 | bos = 15 | 315  --> 135\n",
      "shipping-1 before: n_retrieved = 15 | bos = 15 | 315  --> 135\n",
      "shipping-2 not found: 14, delete\n",
      "shipping-3 not found: 6, delete\n",
      "student_loan-0 before: n_retrieved = 12 | bos = 12 | 480  --> 168\n",
      "student_loan-1 before: n_retrieved = 12 | bos = 12 | 480  --> 168\n",
      "student_loan-2 before: n_retrieved = 12 | bos = 6 | 480  --> 168\n",
      "student_loan-3 not found: 12, delete\n",
      "student_loan-4 not found: 2, delete\n",
      "talkingdata-0 before: n_retrieved = 15 | bos = 15 | 615  --> 270\n",
      "talkingdata-1 before: n_retrieved = 15 | bos = 15 | 615  --> 270\n",
      "talkingdata-2 not found: 15, delete\n",
      "talkingdata-3 not found: 5, delete\n",
      "trains-0 before: n_retrieved = 15 | bos = 15 | 120  --> 45\n",
      "trains-1 before: n_retrieved = 7 | bos = 7 | 56  --> 21\n",
      "trains-2 before: n_retrieved = 2 | bos = 2 | 16  --> 6\n",
      "world_development_indicators-0 before: n_retrieved = 12 | bos = 12 | 372  --> 324\n",
      "world_development_indicators-1 before: n_retrieved = 12 | bos = 12 | 372  --> 324\n",
      "world_development_indicators-2 before: n_retrieved = 12 | bos = 6 | 372  --> 324\n",
      "world_development_indicators-3 not found: 12, delete\n",
      "world_development_indicators-4 not found: 2, delete\n"
     ]
    }
   ],
   "source": [
    "# 돌려 놓은거 처리\n",
    "ds = 'bird'\n",
    "task = 'valid_bo'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "\n",
    "with (experiment_folder / f'partial_{ds}_batch.json').open('r') as f:\n",
    "    new_partial_batch = json.load(f)\n",
    "\n",
    "df = pd.read_csv(experiment_folder / 'evals' / 'zero_shot' / f'{ds}_dev.csv')\n",
    "df_error = df.loc[df['exec_result'] == 0]\n",
    "error_ids = df_error['sample_id'].tolist()\n",
    "count_db_ids = defaultdict(int)\n",
    "for p in sorted(prediction_path.glob(f'{ds}_dev_*.json')):\n",
    "    name = p.stem.split('_', 2)[-1]\n",
    "    db_id, idx = name.split('-')\n",
    "    # if name == 'app_store-2':\n",
    "    #     break\n",
    "    with p.open() as f:\n",
    "        res = json.load(f)\n",
    "    found = False\n",
    "    for db_id_group, batches in new_partial_batch.items():\n",
    "        if name in batches:\n",
    "            found = True\n",
    "            train_bo_ids = [x['sample_id'] for x in batches[name]['train_bos']]\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        n_retrieved = set([x['retrieved'] for x in res])\n",
    "        p.unlink()\n",
    "        print(f'{name} not found: {len(n_retrieved)}, delete')\n",
    "    else:\n",
    "        n_retrieved = set([x['retrieved'] for x in res])\n",
    "        count_db_ids[name] += len(n_retrieved)\n",
    "        print(f'{name} before: n_retrieved = {len(n_retrieved)} | bos = {len(train_bo_ids)} | {len(res)}', end=' ')\n",
    "        # error_ids 에 있는것만 남기기\n",
    "        res = list(filter(lambda x: x['sample_id'] in error_ids, res))\n",
    "        print(f' --> {len(res)}')\n",
    "        # train_bo_ids 에 해당하는 bos만 남기기\n",
    "        if len(res) < len(list(filter(lambda x: x['retrieved'] in train_bo_ids, res))):\n",
    "            print(f'{name} before reduce bo: {len(res)}')\n",
    "            res = list(filter(lambda x: x['retrieved'] in train_bo_ids, res))\n",
    "            print(f'{name} after reduce bo: {len(res)}')\n",
    "\n",
    "        with p.open('w') as f:\n",
    "            json.dump(res, f, indent=4)\n",
    "\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "db_ids = list(bos.keys())\n",
    "partial_db_ids = {}\n",
    "n = 20\n",
    "for i in range(30):\n",
    "    if db_ids[i*n:(i+1)*n]:\n",
    "        partial_db_ids[i] = db_ids[i*n:(i+1)*n]\n",
    "print(partial_db_ids.keys())\n",
    "\n",
    "with open(experiment_folder / f'partial_{ds}_db_ids.json', 'w') as f:\n",
    "    json.dump(partial_db_ids, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_folder / f'partial_{ds}_db_ids.json') as f:\n",
    "    partial_db_ids = json.load(f)\n",
    "\n",
    "sampler = Sampler(bos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, islice\n",
    "\n",
    "def batched(iterable, n, *, strict=False):\n",
    "    # batched('ABCDEFG', 3) → ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    iterator = iter(iterable)\n",
    "    while batch := tuple(islice(iterator, n)):\n",
    "        if strict and len(batch) != n:\n",
    "            raise ValueError('batched(): incomplete batch')\n",
    "        yield batch\n",
    "\n",
    "sampled_ids = {}\n",
    "for db_id_group in partial_db_ids:\n",
    "    sampled_ids[str(db_id_group)] = defaultdict()\n",
    "    for db_id in partial_db_ids[str(db_id_group)]:\n",
    "        x_samples = list(filter(lambda x: x.db_id == db_id, dev_samples))\n",
    "        for idx_bos, train_bos in enumerate(sampler.sample(db_id, 3, 50, rt_idx=False)):\n",
    "            # print(f'{db_id}-{idx_bos} :', f'{len(train_bos)}', f'{len(list(product(train_bos, x_samples)))}')\n",
    "            sampled_ids[str(db_id_group)][f'{db_id}-{idx_bos}'] = {\n",
    "                'train_bos': train_bos,\n",
    "                'n_iter': len(list(product(train_bos, x_samples))), \n",
    "                'total_bos_in_batch': len(train_bos)\n",
    "            }\n",
    "\n",
    "with (experiment_folder / f'partial_{ds}_batch.json').open('w') as f:\n",
    "    json.dump(sampled_ids, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "n_iter: 8394, iter per file: 102.37\n",
      "50\n",
      "n_iter: 4583, iter per file: 91.66\n",
      "62\n",
      "n_iter: 6707, iter per file: 108.18\n",
      "75\n",
      "n_iter: 10838, iter per file: 144.51\n",
      "68\n",
      "n_iter: 6246, iter per file: 91.85\n",
      "64\n",
      "n_iter: 6218, iter per file: 97.16\n",
      "70\n",
      "n_iter: 7298, iter per file: 104.26\n",
      "70\n",
      "n_iter: 8072, iter per file: 115.31\n"
     ]
    }
   ],
   "source": [
    "for db_id_group in partial_db_ids:\n",
    "    print(len(sampled_ids[str(db_id_group)]))\n",
    "    niters = [x['n_iter'] for x in sampled_ids[str(db_id_group)].values()]\n",
    "    print(f'n_iter: {sum(niters)}, iter per file: {np.mean(niters):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for db_id, bs in bos.items():\n",
    "    for b in bs:\n",
    "        res = {'db_id': db_id, 'gold_complexity': b['gold_complexity']}\n",
    "        df.append(res)\n",
    "\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import pairwise\n",
    "\n",
    "def _format_interval(x: pd.Interval):\n",
    "    return pd.Interval(\n",
    "        left=int(np.floor(x.left)), \n",
    "        right=int(np.floor(x.right)),\n",
    "        closed=x.closed\n",
    "    )\n",
    "\n",
    "def _get_categories(s: pd.Series):\n",
    "    tiles = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "    df = pd.qcut(s, q=tiles, duplicates='drop')\n",
    "    return df\n",
    "\n",
    "def _get_df_from_bos(bos):\n",
    "    df = []\n",
    "    for db_id, bs in bos.items():\n",
    "        for b in bs:\n",
    "            res = {'db_id': db_id}\n",
    "            res.update(b)\n",
    "            df.append(res)\n",
    "    df = pd.DataFrame(df)\n",
    "    df_cates = df.groupby('db_id')['gold_complexity'].apply(_get_categories)\n",
    "    df_cates = df_cates.rename('category').apply(_format_interval)\n",
    "    df = df.merge(df_cates.reset_index('db_id', drop=True), left_index=True, right_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'bird'\n",
    "task = 'zero_shot_hint'\n",
    "typ = 'dev'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "\n",
    "# file_name = f'{ds}_{typ}_parsed.pkl'\n",
    "# with (eval_path / file_name).open('rb') as f:\n",
    "#     target_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/simonjisu/code/BusinessObjects/experiments/bird')"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_path.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sample_id': 5156,\n",
       "  'vt': \"SELECT area_code.area_code, country.county FROM area_code INNER JOIN country AS T2 ON T1.zip_code = T2.zip_code INNER JOIN zip_data AS T3 ON T1.zip_code = T3.zip_code WHERE zip_data.city = '[placeholder-type:string]'\",\n",
       "  'ba': \"The virtual table provides the area code and county information for a specific city based on its zip code. It combines data from the 'area_code', 'country', and 'zip_data' tables, filtering results to match the specified city name.\",\n",
       "  'gold_complexity': 10,\n",
       "  'gold_sql': \"SELECT T1.area_code, T2.county FROM area_code AS T1 INNER JOIN country AS T2 ON T1.zip_code = T2.zip_code INNER JOIN zip_data AS T3 ON T1.zip_code = T3.zip_code WHERE T3.city = 'Savoy'\"},\n",
       " {'sample_id': 5211,\n",
       "  'vt': 'SELECT alias.alias FROM alias INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.population_2020 = (SELECT MAX(zip_data.population_2020) FROM zip_data)',\n",
       "  'ba': \"The virtual table retrieves the aliases of cities from the 'alias' table that correspond to the zip codes with the highest population recorded in 2020 from the 'zip_data' table. The query uses an inner join to connect the 'alias' and 'zip_data' tables based on the zip code, ensuring that only the aliases for the most populated areas are selected.\",\n",
       "  'gold_complexity': 11,\n",
       "  'gold_sql': 'SELECT T1.alias FROM alias AS T1 INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE T2.population_2020 = ( SELECT MAX(population_2020) FROM zip_data )'},\n",
       " {'sample_id': 5227,\n",
       "  'vt': \"SELECT zip_congress.district FROM zip_data INNER JOIN zip_congress AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.city = '[placeholder-type:string]'\",\n",
       "  'ba': \"The virtual table retrieves the district information associated with a specific city from the 'zip_data' table by joining it with the 'zip_congress' table. The placeholder in the WHERE clause represents the name of the city for which the district is being queried.\",\n",
       "  'gold_complexity': 7,\n",
       "  'gold_sql': \"SELECT T2.district FROM zip_data AS T1 INNER JOIN zip_congress AS T2 ON T1.zip_code = T2.zip_code WHERE T1.city = 'East Springfield'\"},\n",
       " {'sample_id': 5091,\n",
       "  'vt': \"SELECT COUNT(zip_data.zip_code) FROM zip_data INNER JOIN avoid AS T2 ON T1.zip_code = T2.zip_code WHERE avoid.bad_alias = '[placeholder-type:string]' AND zip_data.time_zone = '[placeholder-type:string]'\",\n",
       "  'ba': \"The virtual table counts the number of zip codes from the 'zip_data' table that are associated with bad aliases from the 'avoid' table. It filters the results based on a specific bad alias and a specified time zone.\",\n",
       "  'gold_complexity': 8,\n",
       "  'gold_sql': \"SELECT COUNT(T1.zip_code) FROM zip_data AS T1 INNER JOIN avoid AS T2 ON T1.zip_code = T2.zip_code WHERE T2.bad_alias = 'Internal Revenue Service' AND T1.time_zone = 'Eastern'\"}]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos['address'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 85\n",
      "\tPrompt Tokens: 51\n",
      "\tCompletion Tokens: 34\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $2.805e-05\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "\n",
    "class Out(BaseModel):\n",
    "    response: str\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    stream_usage=True,\n",
    ")\n",
    "model = llm.with_structured_output(Out)\n",
    "\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = model.invoke(\"Tell me a joke with JSON format\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.26582278481013 46.229123611557306 11 280\n"
     ]
    }
   ],
   "source": [
    "samples_by_db_id = defaultdict(list)\n",
    "for sample in train_samples:\n",
    "    samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "x = []\n",
    "for db_id, samples in samples_by_db_id.items():\n",
    "    x.append(len(samples))\n",
    "\n",
    "print(np.mean(x), np.std(x), np.min(x), np.max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.468354430379748 15.462355628942769 3 93\n"
     ]
    }
   ],
   "source": [
    "samples_by_db_id = defaultdict(list)\n",
    "for sample in dev_samples:\n",
    "    samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "x = []\n",
    "for db_id, samples in samples_by_db_id.items():\n",
    "    x.append(len(samples))\n",
    "\n",
    "print(np.mean(x), np.std(x), np.min(x), np.max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_parsed.pkl', 'rb') as f:\n",
    "#     train_parsed = pickle.load(f)\n",
    "\n",
    "# # prediction parsed\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'rb') as f:\n",
    "#     dev_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = proj_path / 'experiments' / 'bird' / 'evals' / 'zero_shot'\n",
    "\n",
    "df = []\n",
    "for p in eval_path.glob('bird_dev_*.json'):\n",
    "    with p.open() as f:\n",
    "        for line in f:\n",
    "            eval_data = json.loads(line)\n",
    "            df.append(eval_data)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv(eval_path / 'bird_dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean      0.450361\n",
       "std       0.055482\n",
       "min       0.318118\n",
       "max       0.726155\n",
       "median    0.446118\n",
       "Name: gold_complexity, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gold_complexity'].agg(['mean', 'std', 'min', 'max', 'median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_path = proj_path / 'experiments' / 'bird' / 'predictions' / 'create_bo'\n",
    "bos = defaultdict(list)\n",
    "for p in prediction_path.glob('bird_train_bo_*.json'):\n",
    "    with p.open() as f:\n",
    "        temp = json.load(f)\n",
    "    \n",
    "    bos[p.stem.split('_', 3)[-1]] = temp\n",
    "\n",
    "# with (prediction_path / 'final_bird_train_bo.json').open('w') as f:\n",
    "#     json.dump(bos, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = get_vector_store({'address': bos['address'][:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5156, 5211, 5227, 5091, 5152, 5128, 5200, 5119, 5194, 5141]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b['sample_id'] for b in bos['address'][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_id': 5156,\n",
       " 'vt': \"SELECT area_code.area_code, country.county FROM area_code INNER JOIN country AS T2 ON T1.zip_code = T2.zip_code INNER JOIN zip_data AS T3 ON T1.zip_code = T3.zip_code WHERE zip_data.city = '[placeholder-type:string]'\",\n",
       " 'ba': \"The virtual table provides the area code and county information for a specific city based on its zip code. It combines data from the 'area_code', 'country', and 'zip_data' tables, filtering results to match the specified city name.\"}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos['address'][:10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "base_retriever = vector_store.as_retriever(\n",
    "    search_type='similarity_score_threshold', \n",
    "    search_kwargs={\n",
    "        'k': 3,\n",
    "        'score_threshold': 0.3, 'filter': {'sample_id': {'$nin': []}}\n",
    "    }\n",
    ")\n",
    "\n",
    "# 'lambda_mult': 0.5  'score_threshold': 0.0\n",
    "# 'filter': {'sample_id': {'$in': [5156]}}}\n",
    "model = HuggingFaceCrossEncoder(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "compressor = CrossEncoderReranker(model=model, top_n=1)\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=base_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'what is the aliases of cities along with their elevation?'\n",
    "x = base_retriever.invoke(q)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = vector_store.similarity_search_with_relevance_scores(\n",
    "    q, k=2, filter={'sample_id': {'$nin': [5152, 5211, 5194]}})\n",
    "x\n",
    "# similarity_search_with_relevance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'sample_id': 5152, 'db_id': 'address', 'vt': 'SELECT alias.alias, zip_data.elevation FROM alias INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE alias.zip_code = [placeholder-type:numeric]'}, page_content=\"The virtual table describes the aliases of cities along with their elevation from the 'zip_data' table. The query joins the 'alias' table with the 'zip_data' table based on the zip code, filtering for a specific zip code using a placeholder for numeric values.\"),\n",
       "  0.7825041385389271),\n",
       " (Document(metadata={'sample_id': 5211, 'db_id': 'address', 'vt': 'SELECT alias.alias FROM alias INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.population_2020 = (SELECT MAX(zip_data.population_2020) FROM zip_data)'}, page_content=\"The virtual table retrieves the aliases of cities from the 'alias' table that correspond to the zip codes with the highest population recorded in 2020 from the 'zip_data' table. The query uses an inner join to connect the 'alias' and 'zip_data' tables based on the zip code, ensuring that only the aliases for the most populated areas are selected.\"),\n",
       "  0.7281931194919099),\n",
       " (Document(metadata={'sample_id': 5194, 'db_id': 'address', 'vt': \"SELECT avoid.bad_alias FROM avoid INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.city = '[placeholder-type:string]'\"}, page_content=\"The virtual table retrieves the bad aliases associated with a specific city from the 'avoid' table by joining it with the 'zip_data' table based on the zip code. The placeholder in the WHERE clause represents the name of the city for which we want to find bad aliases.\"),\n",
       "  0.691056772625688),\n",
       " (Document(metadata={'sample_id': 5227, 'db_id': 'address', 'vt': \"SELECT zip_congress.district FROM zip_data INNER JOIN zip_congress AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.city = '[placeholder-type:string]'\"}, page_content=\"The virtual table retrieves the district information associated with a specific city from the 'zip_data' table by joining it with the 'zip_congress' table. The placeholder in the WHERE clause represents the name of the city for which the district is being queried.\"),\n",
       "  0.6538844955711105)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_and_similarities = [\n",
    "    (doc, similarity)\n",
    "    for doc, similarity in x\n",
    "    if similarity >= 0.5\n",
    "]\n",
    "docs_and_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(bos: dict[str, list[dict[str, str]]]):\n",
    "    documents = []\n",
    "    for db_id, samples in bos.items():\n",
    "        for x in samples:\n",
    "            doc = Document(\n",
    "                doc_id=x['sample_id'],\n",
    "                page_content=x['ba'],\n",
    "                metadata={\n",
    "                    'sample_id': x['sample_id'],\n",
    "                    'db_id': db_id,\n",
    "                    'vt': x['vt']\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents, \n",
    "        embedding = embeddings_model,\n",
    "    )\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = get_vector_store(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sql_bo(\n",
    "    to_pred_samples: list[SpiderSample|BirdSample],\n",
    "    tables: dict[DatabaseModel],\n",
    "    vectorstore: FAISS,\n",
    "    chain: RunnableSequence,\n",
    "    prediction_path: Path,\n",
    "    file_name: str = '[args.ds]_[args.type]',\n",
    "    n_retrieval: int = 3,\n",
    "    score_threshold: float = 0.65,\n",
    "):\n",
    "    processed_db_ids = [p.stem.split('_')[-1] for p in prediction_path.glob(f'{file_name}_*')]\n",
    "    # restart from checkpoint\n",
    "    if processed_db_ids:\n",
    "        to_pred_samples = [sample for sample in to_pred_samples if sample.db_id not in processed_db_ids]\n",
    "    \n",
    "    samples_by_db_id = defaultdict(list)\n",
    "    for sample in to_pred_samples:\n",
    "        samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "    for db_id, samples in samples_by_db_id.items():\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_kwargs={'k': n_retrieval, 'score_threshold': score_threshold, 'filter': {'db_id': db_id}}\n",
    "        )\n",
    "        schema_str = get_schema_str(\n",
    "            schema=tables[db_id].db_schema, \n",
    "            foreign_keys=tables[db_id].foreign_keys,\n",
    "            col_explanation=tables[db_id].col_explanation\n",
    "        )\n",
    "        results = []\n",
    "        for sample in tqdm(samples, total=len(samples), desc=f\"{db_id}\"):\n",
    "            question = sample.final.question\n",
    "            docs = retriever.invoke(question)\n",
    "            hint = '\\nDescriptions and Virtual Tables:\\n'\n",
    "            hint += json.dumps({j: {'description': doc.page_content, 'virtual_table': doc.metadata['vt']} for j, doc in enumerate(docs)}, indent=4)\n",
    "            hint += '\\n'\n",
    "            input_data = {'schema': schema_str, 'input_query': question, 'hint': hint}\n",
    "            output = chain.invoke(input=input_data)\n",
    "            \n",
    "            full_sql_output = {}\n",
    "            full_sql_output['sample_id'] = sample.sample_id\n",
    "            full_sql_output['rationale'] = output.rationale\n",
    "            full_sql_output['pred_sql'] = output.full_sql_query\n",
    "            # full_sql_output = 1\n",
    "            results.append(full_sql_output)\n",
    "\n",
    "        with open(prediction_path / f'{file_name}_{db_id}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "movie_platform: 100%|██████████| 10/10 [00:04<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_bo.json', 'r') as f:\n",
    "#     bos = json.load(res, f, indent=4)\n",
    "# vectorstore = get_vector_store(bos)\n",
    "\n",
    "\n",
    "data_path = proj_path / 'data' / 'bird'\n",
    "experiment_folder = proj_path / 'experiments' / 'bird'\n",
    "prediction_path = experiment_folder / 'predictions' / 'zero_shot_hint'\n",
    "eval_path = experiment_folder / 'evals'\n",
    "for p in [prediction_path, eval_path]:\n",
    "    if not p.exists():\n",
    "        p.mkdir(parents=True)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=Prompts.zero_shot_hints_inference,\n",
    "    input_variables=['schema', 'input_query', 'hint'],\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0,\n",
    "    frequency_penalty=0.1,\n",
    ")\n",
    "\n",
    "model = model_openai.with_structured_output(SQLResponse)\n",
    "chain = (prompt | model)\n",
    "\n",
    "n_retrieval = 3\n",
    "score_threshold = 0.65\n",
    "\n",
    "predict_sql_bo(\n",
    "    to_pred_samples=dev_samples[:10],\n",
    "    tables=bird_tables,\n",
    "    vectorstore=vectorstore,\n",
    "    chain=chain,\n",
    "    prediction_path=prediction_path,\n",
    "    n_retrieval=n_retrieval,\n",
    "    score_threshold=score_threshold,\n",
    "    file_name='bird_dev',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptions and Virtual Tables:\n",
      "{\n",
      "    \"0\": {\n",
      "        \"description\": \"The virtual table retrieves the titles of movies that have been rated, filtering by a specific rating timestamp and grouping the results by movie title. The results are ordered by the count of ratings for each movie title, and a limit is applied to restrict the number of returned titles.\",\n",
      "        \"virtual_table\": \"SELECT movies.movie_title FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE ratings.rating_timestamp_utc LIKE '[placeholder-type:string]' GROUP BY movies.movie_title ORDER BY COUNT(movies.movie_title) LIMIT [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"1\": {\n",
      "        \"description\": \"The virtual table provides a count of users who have rated a specific movie, identified by its title, while also filtering for users who were trialists at the time of rating. It combines data from the 'ratings' and 'movies' tables to achieve this.\",\n",
      "        \"virtual_table\": \"SELECT COUNT(ratings.user_id) FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE movies.movie_title = '[placeholder-type:string]' AND ratings.user_trialist = [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"description\": \"The virtual table retrieves the titles of movies that have been rated, ordered by the number of likes received on the critics' comments. The query joins the 'ratings' table with the 'movies' table to access the movie titles associated with each rating. The result is limited to a specified number of entries, allowing users to see the most liked critics' comments for rated movies.\",\n",
      "        \"virtual_table\": \"SELECT movies.movie_title FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id ORDER BY ratings.critic_likes LIMIT [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"description\": \"The virtual table displays the titles of movies from the 'movies' table that have received a certain number of likes on user-written critiques. The query joins the 'ratings' table with the 'movies' table to filter movies based on the number of likes their critiques have received, using a placeholder for the minimum number of likes.\",\n",
      "        \"virtual_table\": \"SELECT movies.movie_title FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE ratings.critic_likes > [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"description\": \"The virtual table counts the number of ratings for a specific movie title from the 'movies' table, filtering by the movie's title and a specified rating timestamp. The placeholders represent the movie title and the date from which to count ratings.\",\n",
      "        \"virtual_table\": \"SELECT COUNT(ratings.rating_id) FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE movies.movie_title = '[placeholder-type:string]' AND ratings.rating_timestamp_utc >= '[placeholder-type:string]'\"\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(sample.final.question)\n",
    "hint = '\\nDescriptions and Virtual Tables:\\n'\n",
    "hint += json.dumps({j: {'description': doc.page_content, 'virtual_table': doc.metadata['vt']} for j, doc in enumerate(docs)}, indent=4)\n",
    "hint += '\\n'\n",
    "input_data = {'schema': db_schema, 'input_query': row['question'], 'hint': hint}\n",
    "output = chain.invoke(input=input_data)\n",
    "\n",
    "print(hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "movie_platform:   0%|          | 0/6341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debit_card_specializing: 100%|██████████| 6341/6341 [00:21<00:00, 299.00it/s]    \n",
      "debit_card_specializing: 100%|██████████| 2091/2091 [00:05<00:00, 408.98it/s]    \n",
      "debit_card_specializing: 100%|██████████| 2193/2193 [00:09<00:00, 225.64it/s]    \n"
     ]
    }
   ],
   "source": [
    "def get_parsed_sql(samples: dict, tables: dict):\n",
    "    error_ids = []\n",
    "    parsed = defaultdict(dict)\n",
    "    iterator = tqdm(samples, total=len(samples))\n",
    "    for sample in iterator:\n",
    "        db_id = sample.db_id\n",
    "        sample_id = sample.sample_id\n",
    "        iterator.set_description(f\"{db_id}\")\n",
    "        schema = Schema(tables[db_id].db_schema)\n",
    "        sql_i = sample.final.sql\n",
    "        try:\n",
    "            ei = extract_all(sql_i, schema)\n",
    "            assert len(ei['sel']) > 0, f'No selection found-{db_id}-{sample_id}'\n",
    "        except Exception as e:\n",
    "            error_ids.append((db_id, sample_id, str(e)))\n",
    "            parsed[db_id].append(None)\n",
    "            continue\n",
    "        parsed[db_id][sample_id] = ei\n",
    "    return parsed, error_ids\n",
    "\n",
    "train_parsed, error_ids = get_parsed_sql(train_samples, bird_tables)\n",
    "dev_parsed, error_ids = get_parsed_sql(dev_samples, bird_tables)\n",
    "test_parsed, error_ids = get_parsed_sql(test_samples, bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_parsed, f)\n",
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(dev_parsed, f)\n",
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_test_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_parsed, f)\n",
    "\n",
    "with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'rb') as f:\n",
    "    dev_parsed = pickle.load(f)\n",
    "\n",
    "with open(proj_path / 'data' / 'pkl_files' / 'bird_test_parsed.pkl', 'rb') as f:\n",
    "    test_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, product\n",
    "from collections import defaultdict\n",
    "from src.eval_utils import get_all_partial_score\n",
    "\n",
    "def measure_inter_score(parsed1: dict[str, tuple], parsed2: dict[str, tuple]):\n",
    "    results = defaultdict()\n",
    "    assert len(parsed1) == len(parsed2), f\"Length mismatch-1: {len(parsed1)} 2:{len(parsed2)}\"\n",
    "    db_ids = list(parsed1.keys())\n",
    "    for db_id in db_ids:\n",
    "        o1 = parsed1[db_id]\n",
    "        o2 = parsed2[db_id]\n",
    "        n1 = len(o1)\n",
    "        n2 = len(o2)\n",
    "        semantic_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "        structural_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "        overall_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "\n",
    "        idxs = list(product(range(n1), range(n2)))\n",
    "        iterator = tqdm(idxs, total=len(idxs), desc=f\"{db_id}\")\n",
    "        for i, j in iterator:\n",
    "            ei = o1[i]\n",
    "            ej = o2[j]\n",
    "\n",
    "            _, final_score = get_all_partial_score(ei, ej, use_bert=True)\n",
    "\n",
    "            structural_sim[i, j] = final_score['structural']\n",
    "            semantic_sim[i, j] = final_score['semantic']\n",
    "            overall_sim[i, j] = final_score['overall']\n",
    "\n",
    "        results[db_id] = {\n",
    "            'semantic': semantic_sim,\n",
    "            'struct': structural_sim,\n",
    "            'overall': overall_sim\n",
    "        }\n",
    "    return results\n",
    "\n",
    "results = measure_inter_score(dev_parsed, test_parsed)\n",
    "with (proj_path / 'data' / 'pkl_files' / 'bird_dev_test_similarity.pkl').open('wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity between datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6341/6341 [00:10<00:00, 631.17it/s]\n",
      "100%|██████████| 2091/2091 [00:03<00:00, 589.55it/s]\n",
      "100%|██████████| 2193/2193 [00:03<00:00, 591.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def measure_complexity(samples, tables):\n",
    "    cs = []\n",
    "    for s in tqdm(samples, total=len(samples)):\n",
    "        schema = Schema(tables[s.db_id].db_schema)\n",
    "        output = extract_all(s.final.sql, schema)\n",
    "        complexity = get_complexity(output)\n",
    "        cs.append(complexity)\n",
    "    return cs\n",
    "\n",
    "train_complexities = measure_complexity(train_samples, bird_tables)\n",
    "dev_complexities = measure_complexity(dev_samples, bird_tables)\n",
    "test_complexities = measure_complexity(test_samples, bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Mean=0.2753 +/-0.0476, Median=0.2710\n",
      "[dev  ] Mean=0.2758 +/-0.0471, Median=0.2710\n",
      "[test ] Mean=0.2760 +/-0.0477, Median=0.2709\n"
     ]
    }
   ],
   "source": [
    "for c, n in zip([train_complexities, dev_complexities, test_complexities], ['train', 'dev  ', 'test ']):\n",
    "    print(f'[{n}] Mean={np.mean(c):.4f} +/-{np.std(c):.4f}, Median={np.median(c):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = defaultdict(list)\n",
    "for s in dev_samples:\n",
    "    stats[s.db_id].append(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
