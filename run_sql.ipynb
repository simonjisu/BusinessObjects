{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "proj_path = Path('.').resolve()\n",
    "sys.path.append(str(proj_path))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from src.db_utils import get_schema_str, get_data_dict\n",
    "from src.pymodels import (\n",
    "    DatabaseModel, \n",
    "    SpiderSample, \n",
    "    BirdSample, \n",
    "    BODescription,\n",
    "    SQLResponse\n",
    ")\n",
    "from src.prompts import Prompts\n",
    "from src.database import SqliteDatabase\n",
    "from src.data_preprocess import (\n",
    "    load_raw_data,\n",
    "    process_all_tables,\n",
    "    filter_samples_by_count_spider_bird,\n",
    "    process_samples_bird,\n",
    "    split_train_dev_test,\n",
    "    save_samples_spider_bird,\n",
    "    load_samples_spider_bird,\n",
    ")\n",
    "\n",
    "from src.parsing_sql import Schema, extract_all\n",
    "from src.eval_utils import get_complexity, result_eq, check_if_exists_orderby\n",
    "from run_bo_sql import get_vector_store\n",
    "from copy import deepcopy\n",
    "bird_path = proj_path / 'data' / 'bird'\n",
    "tables, train_data, dev_data = load_raw_data(bird_path, load_test=False)\n",
    "\n",
    "with (proj_path / 'data' / 'bird_description.json').open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "\n",
    "bird_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "train_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_train.json')\n",
    "dev_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_dev.json')\n",
    "test_samples = load_samples_spider_bird(proj_path / 'data' / 'bird_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_path = proj_path / 'data' / 'spider'\n",
    "# tables, train_data, dev_data = load_raw_data(spider_path, load_test=False)\n",
    "\n",
    "# with (proj_path / 'data' / 'description.json').open() as f:\n",
    "#     all_descriptions = json.load(f)\n",
    "# seed = 42\n",
    "# all_data = filter_samples_by_count_spider_bird(train_data+dev_data, n=10)\n",
    "\n",
    "# with open(proj_path / 'data' / 'bird_skip.txt') as f:\n",
    "#     skip = [int(line.strip()) for line in f]\n",
    "\n",
    "# bird_samples = process_samples_bird(all_data, bird_tables, skip=skip)\n",
    "# train_samples, dev_samples, test_samples = split_train_dev_test(bird_samples, train_ratio=0.6, dev_ratio=0.2, seed=seed)\n",
    "\n",
    "# save_samples_spider_bird(train_samples, proj_path / 'data' / 'bird_train.json')\n",
    "# save_samples_spider_bird(dev_samples, proj_path / 'data' / 'bird_dev.json')\n",
    "# save_samples_spider_bird(test_samples, proj_path / 'data' / 'bird_test.json')\n",
    "# print(len(train_samples), len(dev_samples), len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_folder = proj_path / 'experiments' / 'bird'\n",
    "# prediction_path = experiment_folder / 'predictions' / 'create_bo'\n",
    "# tables = bird_tables\n",
    "# bos = []\n",
    "# for p in prediction_path.glob('bird_train_bo_*.json'):\n",
    "#     with p.open() as f:\n",
    "#         bos = json.load(f)\n",
    "\n",
    "#     db_id = p.stem.split('_', 3)[-1]\n",
    "#     schema = Schema(tables[db_id].db_schema)\n",
    "#     for bo in bos:\n",
    "#         output = extract_all(bo['gold_sql'], schema)\n",
    "#         bo['gold_complexity'] = get_complexity(output)\n",
    "    \n",
    "#     with p.open('w') as f:\n",
    "#         json.dump(bos, f, indent=4)\n",
    "\n",
    "# bos = {}\n",
    "# for p in prediction_path.glob('bird_train_bo_*.json'):\n",
    "#     db_id = p.stem.split('_', 3)[-1]\n",
    "#     with p.open() as f:\n",
    "#         bos[db_id] = json.load(f)\n",
    "\n",
    "# with (experiment_folder / 'predictions' / 'create_bo' / f'final_bird_train_bo.json').open('w') as f:\n",
    "#     json.dump(bos, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_bo_sql import Sampler\n",
    "# TODO: run spider 4567\n",
    "ds = 'bird'\n",
    "task = 'valid_bo'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "\n",
    "# dev_samples = load_samples_spider_bird(proj_path / 'data' / f'{ds}_dev.json')\n",
    "# pred_res = defaultdict(dict)  # db_id -> train_bo -> list[dict]\n",
    "# for p in prediction_path.glob(f'{ds}_dev_*.json'):\n",
    "#     name = p.stem.split('_', 2)[-1]\n",
    "#     db_id, idx = name.split('-')\n",
    "#     with p.open() as f:\n",
    "#         res = json.load(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for r in res:\n",
    "#         train_bo_id = r['retrieved']\n",
    "#         if not pred_res[db_id].get(train_bo_id):\n",
    "#             pred_res[db_id][train_bo_id] = []\n",
    "#         pred_res[db_id][r['retrieved']].append(r)\n",
    "#     break\n",
    "\n",
    "# save_path = prediction_path / f'final_{ds}_dev.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/simonjisu/code/BusinessObjects/experiments/bird/predictions/valid_bo')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(prediction_path.glob(f'{ds}_dev_*.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1022, 992, 1013, 1023, 1005, 973, 998, 1028, 994, 987, 974, 1021, 1012, 1000, 1032])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_res['computer_student'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "db_ids = list(bos.keys())\n",
    "partial_db_ids = {}\n",
    "n = 20\n",
    "for i in range(30):\n",
    "    if db_ids[i*n:(i+1)*n]:\n",
    "        partial_db_ids[i] = db_ids[i*n:(i+1)*n]\n",
    "print(partial_db_ids.keys())\n",
    "\n",
    "with open(experiment_folder / f'partial_{ds}_db_ids.json', 'w') as f:\n",
    "    json.dump(partial_db_ids, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_folder / f'partial_{ds}_db_ids.json') as f:\n",
    "    partial_db_ids = json.load(f)\n",
    "\n",
    "sampler = Sampler(bos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh ./scripts/valid_bo/valid_bo_bird_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, islice\n",
    "\n",
    "def batched(iterable, n, *, strict=False):\n",
    "    # batched('ABCDEFG', 3) → ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    iterator = iter(iterable)\n",
    "    while batch := tuple(islice(iterator, n)):\n",
    "        if strict and len(batch) != n:\n",
    "            raise ValueError('batched(): incomplete batch')\n",
    "        yield batch\n",
    "\n",
    "sampled_ids = {}\n",
    "for db_id_group in partial_db_ids:\n",
    "    sampled_ids[str(db_id_group)] = defaultdict()\n",
    "    for db_id in partial_db_ids[str(db_id_group)]:\n",
    "        x_samples = list(filter(lambda x: x.db_id == db_id, dev_samples))\n",
    "        for idx_bos, train_bos in enumerate(sampler.sample(db_id, 3, 50, rt_idx=False)):\n",
    "            # print(f'{db_id}-{idx_bos} :', f'{len(train_bos)}', f'{len(list(product(train_bos, x_samples)))}')\n",
    "            sampled_ids[str(db_id_group)][f'{db_id}-{idx_bos}'] = {\n",
    "                'train_bos': train_bos,\n",
    "                'n_iter': len(list(product(train_bos, x_samples))), \n",
    "                'total_bos_in_batch': len(train_bos)\n",
    "            }\n",
    "\n",
    "with (experiment_folder / f'partial_{ds}_batch.json').open('w') as f:\n",
    "    json.dump(sampled_ids, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "n_iter: 8394, iter per file: 102.37\n",
      "50\n",
      "n_iter: 4583, iter per file: 91.66\n",
      "62\n",
      "n_iter: 6707, iter per file: 108.18\n",
      "75\n",
      "n_iter: 10838, iter per file: 144.51\n",
      "68\n",
      "n_iter: 6246, iter per file: 91.85\n",
      "64\n",
      "n_iter: 6218, iter per file: 97.16\n",
      "70\n",
      "n_iter: 7298, iter per file: 104.26\n",
      "70\n",
      "n_iter: 8072, iter per file: 115.31\n"
     ]
    }
   ],
   "source": [
    "for db_id_group in partial_db_ids:\n",
    "    print(len(sampled_ids[str(db_id_group)]))\n",
    "    niters = [x['n_iter'] for x in sampled_ids[str(db_id_group)].values()]\n",
    "    print(f'n_iter: {sum(niters)}, iter per file: {np.mean(niters):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for db_id, bs in bos.items():\n",
    "    for b in bs:\n",
    "        res = {'db_id': db_id, 'gold_complexity': b['gold_complexity']}\n",
    "        df.append(res)\n",
    "\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import pairwise\n",
    "\n",
    "def _format_interval(x: pd.Interval):\n",
    "    return pd.Interval(\n",
    "        left=int(np.floor(x.left)), \n",
    "        right=int(np.floor(x.right)),\n",
    "        closed=x.closed\n",
    "    )\n",
    "\n",
    "def _get_categories(s: pd.Series):\n",
    "    tiles = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "    df = pd.qcut(s, q=tiles, duplicates='drop')\n",
    "    return df\n",
    "\n",
    "def _get_df_from_bos(bos):\n",
    "    df = []\n",
    "    for db_id, bs in bos.items():\n",
    "        for b in bs:\n",
    "            res = {'db_id': db_id}\n",
    "            res.update(b)\n",
    "            df.append(res)\n",
    "    df = pd.DataFrame(df)\n",
    "    df_cates = df.groupby('db_id')['gold_complexity'].apply(_get_categories)\n",
    "    df_cates = df_cates.rename('category').apply(_format_interval)\n",
    "    df = df.merge(df_cates.reset_index('db_id', drop=True), left_index=True, right_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sampler with gold_complexity\n",
    "from typing import Iterator\n",
    "class Sampler():\n",
    "    def __init__(self, bos: dict[str, list[dict]]):\n",
    "        self.bos = bos\n",
    "        self.df = _get_df_from_bos(bos)\n",
    "\n",
    "    def _get_sample_batch(self, db_id: str, n_sample: int=1, n_stop: int=20):\n",
    "        sampled = []\n",
    "        sample_batch = []\n",
    "        n_sampled = 0\n",
    "        df_db_id = self.df.loc[(self.df['db_id'] == db_id) & ~self.df['sample_id'].isin(sampled)]\n",
    "        while df_db_id['category'].nunique() > 0 and n_sampled < n_stop:\n",
    "            groupby_statement = df_db_id.groupby('category')['sample_id']\n",
    "            # n_minima, n_maxima = groupby_statement.size().agg(['min', 'max']).tolist()\n",
    "            # n = min([n_sample, n_minima, n_maxima])  # actual sample size    \n",
    "            sample_ids = groupby_statement.apply(lambda x: x.sample(min(len(x), n_sample))).tolist()\n",
    "            sampled.extend(sample_ids)\n",
    "            sample_batch.append(sample_ids)\n",
    "            n_sampled += len(sample_ids)\n",
    "            df_db_id = self.df.loc[(self.df['db_id'] == db_id) & ~self.df['sample_id'].isin(sampled)]\n",
    "\n",
    "        return sample_batch\n",
    "    \n",
    "    def sample(self, db_id: str, n_sample: int=1, n_stop: int=20) -> Iterator:\n",
    "        sample_batch = self._get_sample_batch(db_id, n_sample, n_stop)\n",
    "        for sample_ids in sample_batch:\n",
    "            s = self.df.loc[(self.df['db_id'] == db_id) & self.df['sample_id'].isin(sample_ids)]\n",
    "            s = s.to_dict(orient='records')\n",
    "            yield s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of run_bo_sql failed: Traceback (most recent call last):\n",
      "  File \"/home/simonjisu/code/BusinessObjects/.venv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/simonjisu/code/BusinessObjects/.venv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/simonjisu/code/BusinessObjects/.venv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/simonjisu/code/BusinessObjects/.venv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 365, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/home/simonjisu/code/BusinessObjects/.venv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 319, in update_instances\n",
      "    refs = gc.get_referrers(old)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "sampler = Sampler(bos)\n",
    "\n",
    "for train_bos in sampler.sample('address', n_sample=1, n_stop=20):\n",
    "    print(len(train_bos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_by_db_id = defaultdict(list)\n",
    "for sample in samples:\n",
    "    samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "sampler = Sampler(bos)\n",
    "\n",
    "for db_id, samples in samples_by_db_id.items():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BirdSample(sample_id=165, db_id='movie_platform', final=QuestionSQL(question='What is the name of the movie that was rated recently by user 57756708?', sql='SELECT T2.movie_title FROM ratings AS T1 INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE T1.user_id = 57756708 ORDER BY T1.rating_timestamp_utc DESC LIMIT 1', source_tables=['movies', 'ratings']), evidence='user 57756708 refers to user_id = 57756708; rated recently refers to MAX(rating_timestamp_utc)', bo=None),\n",
       " BirdSample(sample_id=137, db_id='movie_platform', final=QuestionSQL(question='For all the users who gave \"A Shot in the Dark\" a rating, how many percent of them is a paying subscriber?', sql=\"SELECT CAST(SUM(CASE WHEN T1.user_has_payment_method = 1 THEN 1 ELSE 0 END) AS REAL) * 100 / COUNT(*) FROM ratings AS T1 INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id INNER JOIN lists_users AS T3 ON T1.user_id = T3.user_id WHERE T2.movie_title = 'A Shot in the Dark'\", source_tables=['movies', 'ratings', 'lists_users']), evidence='\"A Shot in the Dark\" refers to movie_title = \\'A Shot in the Dark\\'; paying subscriber refers to user_has_payment_method = 1; percentage refers to DIVIDE(COUNT(user_has_payment_method = 1),COUNT(user_has_payment_method))*100', bo=None),\n",
       " BirdSample(sample_id=163, db_id='movie_platform', final=QuestionSQL(question='What is the average number of number of movies added to the lists of user 8516503? Indicate how many movies did he/she give a rating score of 5.', sql='SELECT AVG(T3.list_movie_number) , SUM(CASE WHEN T1.rating_score = 5 THEN 1 ELSE 0 END) FROM ratings AS T1 INNER JOIN lists_users AS T2 ON T1.user_id = T2.user_id INNER JOIN lists AS T3 ON T2.user_id = T3.user_id WHERE T1.user_id = 8516503', source_tables=['lists_users', 'ratings', 'lists']), evidence='average number of number of movies refers to AVG(list_movie_number); user 8516503 refers to user_id = 8516503; rating score of 5 refers to rating_score = 5', bo=None)]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_samples[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"description\": \"The virtual table counts the number of cities associated with a specific congress representative based on their first and last names, while also filtering for cities that have a certain number of employees. It joins the \\'congress\\' table with the \\'state\\' table to match the state abbreviation and then joins with the \\'zip_data\\' table to access city information.\",\\n  \"virtual_table\": \"SELECT COUNT(zip_data.city) FROM congress INNER JOIN state AS T2 ON T1.abbreviation = T2.abbreviation INNER JOIN zip_data AS T3 ON T2.abbreviation = T3.state WHERE congress.first_name = \\'[placeholder-type:string]\\' AND congress.last_name = \\'[placeholder-type:string]\\' AND zip_data.employees = [placeholder-type:numeric]\"\\n}'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps({'description': bo['ba'], 'virtual_table': bo['vt']}, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'bird'\n",
    "task = 'zero_shot_hint'\n",
    "typ = 'dev'\n",
    "experiment_folder = proj_path / 'experiments' / ds\n",
    "prediction_path = experiment_folder / 'predictions' / task\n",
    "eval_path = experiment_folder / 'evals' / task\n",
    "\n",
    "# file_name = f'{ds}_{typ}_parsed.pkl'\n",
    "# with (eval_path / file_name).open('rb') as f:\n",
    "#     target_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/simonjisu/code/BusinessObjects/experiments/bird')"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_path.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sample_id': 5156,\n",
       "  'vt': \"SELECT area_code.area_code, country.county FROM area_code INNER JOIN country AS T2 ON T1.zip_code = T2.zip_code INNER JOIN zip_data AS T3 ON T1.zip_code = T3.zip_code WHERE zip_data.city = '[placeholder-type:string]'\",\n",
       "  'ba': \"The virtual table provides the area code and county information for a specific city based on its zip code. It combines data from the 'area_code', 'country', and 'zip_data' tables, filtering results to match the specified city name.\",\n",
       "  'gold_complexity': 10,\n",
       "  'gold_sql': \"SELECT T1.area_code, T2.county FROM area_code AS T1 INNER JOIN country AS T2 ON T1.zip_code = T2.zip_code INNER JOIN zip_data AS T3 ON T1.zip_code = T3.zip_code WHERE T3.city = 'Savoy'\"},\n",
       " {'sample_id': 5211,\n",
       "  'vt': 'SELECT alias.alias FROM alias INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.population_2020 = (SELECT MAX(zip_data.population_2020) FROM zip_data)',\n",
       "  'ba': \"The virtual table retrieves the aliases of cities from the 'alias' table that correspond to the zip codes with the highest population recorded in 2020 from the 'zip_data' table. The query uses an inner join to connect the 'alias' and 'zip_data' tables based on the zip code, ensuring that only the aliases for the most populated areas are selected.\",\n",
       "  'gold_complexity': 11,\n",
       "  'gold_sql': 'SELECT T1.alias FROM alias AS T1 INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE T2.population_2020 = ( SELECT MAX(population_2020) FROM zip_data )'},\n",
       " {'sample_id': 5227,\n",
       "  'vt': \"SELECT zip_congress.district FROM zip_data INNER JOIN zip_congress AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.city = '[placeholder-type:string]'\",\n",
       "  'ba': \"The virtual table retrieves the district information associated with a specific city from the 'zip_data' table by joining it with the 'zip_congress' table. The placeholder in the WHERE clause represents the name of the city for which the district is being queried.\",\n",
       "  'gold_complexity': 7,\n",
       "  'gold_sql': \"SELECT T2.district FROM zip_data AS T1 INNER JOIN zip_congress AS T2 ON T1.zip_code = T2.zip_code WHERE T1.city = 'East Springfield'\"},\n",
       " {'sample_id': 5091,\n",
       "  'vt': \"SELECT COUNT(zip_data.zip_code) FROM zip_data INNER JOIN avoid AS T2 ON T1.zip_code = T2.zip_code WHERE avoid.bad_alias = '[placeholder-type:string]' AND zip_data.time_zone = '[placeholder-type:string]'\",\n",
       "  'ba': \"The virtual table counts the number of zip codes from the 'zip_data' table that are associated with bad aliases from the 'avoid' table. It filters the results based on a specific bad alias and a specified time zone.\",\n",
       "  'gold_complexity': 8,\n",
       "  'gold_sql': \"SELECT COUNT(T1.zip_code) FROM zip_data AS T1 INNER JOIN avoid AS T2 ON T1.zip_code = T2.zip_code WHERE T2.bad_alias = 'Internal Revenue Service' AND T1.time_zone = 'Eastern'\"}]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos['address'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 85\n",
      "\tPrompt Tokens: 51\n",
      "\tCompletion Tokens: 34\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $2.805e-05\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "\n",
    "class Out(BaseModel):\n",
    "    response: str\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    stream_usage=True,\n",
    ")\n",
    "model = llm.with_structured_output(Out)\n",
    "\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = model.invoke(\"Tell me a joke with JSON format\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.26582278481013 46.229123611557306 11 280\n"
     ]
    }
   ],
   "source": [
    "samples_by_db_id = defaultdict(list)\n",
    "for sample in train_samples:\n",
    "    samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "x = []\n",
    "for db_id, samples in samples_by_db_id.items():\n",
    "    x.append(len(samples))\n",
    "\n",
    "print(np.mean(x), np.std(x), np.min(x), np.max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.468354430379748 15.462355628942769 3 93\n"
     ]
    }
   ],
   "source": [
    "samples_by_db_id = defaultdict(list)\n",
    "for sample in dev_samples:\n",
    "    samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "x = []\n",
    "for db_id, samples in samples_by_db_id.items():\n",
    "    x.append(len(samples))\n",
    "\n",
    "print(np.mean(x), np.std(x), np.min(x), np.max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_parsed.pkl', 'rb') as f:\n",
    "#     train_parsed = pickle.load(f)\n",
    "\n",
    "# # prediction parsed\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'rb') as f:\n",
    "#     dev_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = proj_path / 'experiments' / 'bird' / 'evals' / 'zero_shot'\n",
    "\n",
    "df = []\n",
    "for p in eval_path.glob('bird_dev_*.json'):\n",
    "    with p.open() as f:\n",
    "        for line in f:\n",
    "            eval_data = json.loads(line)\n",
    "            df.append(eval_data)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv(eval_path / 'bird_dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean      0.450361\n",
       "std       0.055482\n",
       "min       0.318118\n",
       "max       0.726155\n",
       "median    0.446118\n",
       "Name: gold_complexity, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gold_complexity'].agg(['mean', 'std', 'min', 'max', 'median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_path = proj_path / 'experiments' / 'bird' / 'predictions' / 'create_bo'\n",
    "bos = defaultdict(list)\n",
    "for p in prediction_path.glob('bird_train_bo_*.json'):\n",
    "    with p.open() as f:\n",
    "        temp = json.load(f)\n",
    "    \n",
    "    bos[p.stem.split('_', 3)[-1]] = temp\n",
    "\n",
    "# with (prediction_path / 'final_bird_train_bo.json').open('w') as f:\n",
    "#     json.dump(bos, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = get_vector_store({'address': bos['address'][:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5156, 5211, 5227, 5091, 5152, 5128, 5200, 5119, 5194, 5141]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b['sample_id'] for b in bos['address'][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_id': 5156,\n",
       " 'vt': \"SELECT area_code.area_code, country.county FROM area_code INNER JOIN country AS T2 ON T1.zip_code = T2.zip_code INNER JOIN zip_data AS T3 ON T1.zip_code = T3.zip_code WHERE zip_data.city = '[placeholder-type:string]'\",\n",
       " 'ba': \"The virtual table provides the area code and county information for a specific city based on its zip code. It combines data from the 'area_code', 'country', and 'zip_data' tables, filtering results to match the specified city name.\"}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos['address'][:10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "base_retriever = vector_store.as_retriever(\n",
    "    search_type='similarity_score_threshold', \n",
    "    search_kwargs={\n",
    "        'k': 3,\n",
    "        'score_threshold': 0.3, 'filter': {'sample_id': {'$nin': []}}\n",
    "    }\n",
    ")\n",
    "\n",
    "# 'lambda_mult': 0.5  'score_threshold': 0.0\n",
    "# 'filter': {'sample_id': {'$in': [5156]}}}\n",
    "model = HuggingFaceCrossEncoder(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "compressor = CrossEncoderReranker(model=model, top_n=1)\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=base_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'what is the aliases of cities along with their elevation?'\n",
    "x = base_retriever.invoke(q)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = vector_store.similarity_search_with_relevance_scores(\n",
    "    q, k=2, filter={'sample_id': {'$nin': [5152, 5211, 5194]}})\n",
    "x\n",
    "# similarity_search_with_relevance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'sample_id': 5152, 'db_id': 'address', 'vt': 'SELECT alias.alias, zip_data.elevation FROM alias INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE alias.zip_code = [placeholder-type:numeric]'}, page_content=\"The virtual table describes the aliases of cities along with their elevation from the 'zip_data' table. The query joins the 'alias' table with the 'zip_data' table based on the zip code, filtering for a specific zip code using a placeholder for numeric values.\"),\n",
       "  0.7825041385389271),\n",
       " (Document(metadata={'sample_id': 5211, 'db_id': 'address', 'vt': 'SELECT alias.alias FROM alias INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.population_2020 = (SELECT MAX(zip_data.population_2020) FROM zip_data)'}, page_content=\"The virtual table retrieves the aliases of cities from the 'alias' table that correspond to the zip codes with the highest population recorded in 2020 from the 'zip_data' table. The query uses an inner join to connect the 'alias' and 'zip_data' tables based on the zip code, ensuring that only the aliases for the most populated areas are selected.\"),\n",
       "  0.7281931194919099),\n",
       " (Document(metadata={'sample_id': 5194, 'db_id': 'address', 'vt': \"SELECT avoid.bad_alias FROM avoid INNER JOIN zip_data AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.city = '[placeholder-type:string]'\"}, page_content=\"The virtual table retrieves the bad aliases associated with a specific city from the 'avoid' table by joining it with the 'zip_data' table based on the zip code. The placeholder in the WHERE clause represents the name of the city for which we want to find bad aliases.\"),\n",
       "  0.691056772625688),\n",
       " (Document(metadata={'sample_id': 5227, 'db_id': 'address', 'vt': \"SELECT zip_congress.district FROM zip_data INNER JOIN zip_congress AS T2 ON T1.zip_code = T2.zip_code WHERE zip_data.city = '[placeholder-type:string]'\"}, page_content=\"The virtual table retrieves the district information associated with a specific city from the 'zip_data' table by joining it with the 'zip_congress' table. The placeholder in the WHERE clause represents the name of the city for which the district is being queried.\"),\n",
       "  0.6538844955711105)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_and_similarities = [\n",
    "    (doc, similarity)\n",
    "    for doc, similarity in x\n",
    "    if similarity >= 0.5\n",
    "]\n",
    "docs_and_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CrossEncoderReranker' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'CrossEncoderReranker' object is not callable"
     ]
    }
   ],
   "source": [
    "compressor([y[0] for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(langchain_core.retrievers.BaseRetriever,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContextualCompressionRetriever.__bases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2091"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_path = proj_path / 'experiments' / 'bird' / 'predictions' / 'zero_shot'\n",
    "predictions = []\n",
    "for p in prediction_path.glob('bird_dev_*.json'):\n",
    "    with open(p) as f:\n",
    "        pred = json.load(f)\n",
    "        new_pred = []\n",
    "        for x in pred:\n",
    "            x.pop('rationale')\n",
    "            new_pred.append(x)\n",
    "        predictions.extend(new_pred)\n",
    "\n",
    "with open(prediction_path / 'final_bird_dev.jsonl', 'w') as f:\n",
    "    for p in predictions:\n",
    "        f.write(json.dumps(p) + '\\n')\n",
    "\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_id': 8773,\n",
       " 'db_id': 'food_inspection',\n",
       " 'gold_sql': \"SELECT COUNT(owner_state) FROM businesses WHERE owner_state = 'CA'\",\n",
       " 'pred_sql': \"SELECT COUNT(DISTINCT owner_name) AS owner_count FROM businesses WHERE owner_state = 'California';\"}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typ = 'test'\n",
    "# samples = test_samples\n",
    "# predictions = []\n",
    "# with open(prediction_path / f'final_bird_{typ}.jsonl', 'r') as f:\n",
    "#     preds = f.readlines()\n",
    "#     for p in preds:\n",
    "#         pred = json.loads(p)\n",
    "#         found = False\n",
    "#         for sample in samples:\n",
    "#             if sample.sample_id == pred['sample_id']:\n",
    "#                 pred['gold_sql'] = sample.final.sql\n",
    "#                 found = True\n",
    "#                 break\n",
    "#         if not found:\n",
    "#             raise ValueError(f\"Sample ID {pred['sample_id']} not found\")\n",
    "        \n",
    "#         predictions.append(pred)\n",
    "\n",
    "# with open(prediction_path / f'final_bird_{typ}.jsonl', 'w') as f:\n",
    "#     for p in predictions:\n",
    "#         f.write(json.dumps(p) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "import sqlglot.expressions as exp\n",
    "from src.eval_utils import result_eq, check_if_exists_orderby\n",
    "\n",
    "def get_pred_results(\n",
    "        proj_path: Path,\n",
    "        predictions: list[dict],\n",
    "        tables: dict[str, DatabaseModel],\n",
    "        ds: str = 'bird' # spider or bird\n",
    "    ):\n",
    "    output_results = []\n",
    "    error_infos = {\n",
    "        'pred_exec': [],\n",
    "        'gold_exec': [],\n",
    "        'python_script': [],\n",
    "        'result': []\n",
    "    }\n",
    "    predictions_by_db_id = defaultdict(list)\n",
    "    for pred in predictions:\n",
    "        predictions_by_db_id[pred['db_id']].append(pred)\n",
    "    \n",
    "    for db_id, preds in predictions_by_db_id.items():\n",
    "        schema = Schema(tables[db_id].db_schema)\n",
    "        if ds == 'bird':\n",
    "            try:\n",
    "                database = SqliteDatabase(\n",
    "                    db_file=str(proj_path / 'data' / ds / 'train' / 'train_databases' / db_id / f'{db_id}.sqlite'),\n",
    "                    foreign_keys=tables[db_id].foreign_keys\n",
    "                )\n",
    "            except:\n",
    "                database = SqliteDatabase(\n",
    "                    db_file=str(proj_path / 'data' / ds / 'dev' / 'dev_databases' / db_id / f'{db_id}.sqlite'),\n",
    "                    foreign_keys=tables[db_id].foreign_keys\n",
    "                )\n",
    "        else:\n",
    "            database = SqliteDatabase(\n",
    "                db_file=str(proj_path / 'data' / 'spider' / 'database' / db_id / f'{db_id}.sqlite'), \n",
    "                foreign_keys=tables[db_id].foreign_keys\n",
    "            )\n",
    "        iterator = tqdm(preds, total=len(preds))\n",
    "        for pred in iterator:\n",
    "            iterator.set_description(f'{db_id} | pred_exec: {len(error_infos[\"pred_exec\"])} | gold_exec: {len(error_infos[\"gold_exec\"])} | python_script: {len(error_infos[\"python_script\"])} | result: {len(error_infos[\"result\"])}')\n",
    "\n",
    "            pred_sql = pred['pred_sql'] \n",
    "            gold_sql = pred['gold_sql']\n",
    "            \n",
    "            error_info = ''\n",
    "            try:\n",
    "                pred_result = database.execute(pred_sql, rt_pandas=False)\n",
    "            except Exception as e:\n",
    "                pred_result = []\n",
    "                error_infos['pred_exec'].append((db_id, pred['sample_id']))\n",
    "                error_info = 'Predction Execution Error:' + str(e)\n",
    "                score = 0\n",
    "\n",
    "            try:\n",
    "                gold_result = database.execute(gold_sql, rt_pandas=False)\n",
    "            except Exception as e:\n",
    "                error_infos['gold_exec'].append((db_id, pred['sample_id']))\n",
    "                error_info = 'Gold Execution Error:' + str(e)\n",
    "            \n",
    "            if 'Gold Execution Error' in error_info:\n",
    "                continue\n",
    "            elif 'Predction Execution Error' in error_info:\n",
    "                output_results.append(\n",
    "                    {\n",
    "                        'sample_id': pred['sample_id'], \n",
    "                        'db_id': db_id,\n",
    "                        'score': score,\n",
    "                        'gold_sql': gold_sql,\n",
    "                        'pred_sql': pred_sql,\n",
    "                    }\n",
    "                )\n",
    "                continue\n",
    "            else:\n",
    "                exists_orderby = check_if_exists_orderby(gold_sql)\n",
    "                \n",
    "                try:\n",
    "                    score = int(result_eq(pred_result, gold_result, order_matters=exists_orderby))\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    score = 0\n",
    "                    error_info = 'Python Script Error:' + str(e)\n",
    "                    error_infos['python_script'].append((db_id, pred['sample_id']))\n",
    "\n",
    "                if score == 0 and error_info == '':\n",
    "                    error_info = 'Result not equal'\n",
    "                    error_infos['result'].append((db_id, pred['sample_id']))\n",
    "                output_results.append(\n",
    "                    {\n",
    "                        'sample_id': pred['sample_id'], \n",
    "                        'db_id': db_id,\n",
    "                        'score': score,\n",
    "                        'gold_sql': gold_sql,\n",
    "                        'pred_sql': pred_sql,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return output_results, error_infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_path = proj_path / 'experiments' / 'bird' / 'predictions' / 'zero_shot'\n",
    "\n",
    "def load_predictions(prediction_path: Path, filename: str):\n",
    "    predictions = []\n",
    "    with open(prediction_path / filename, 'r') as f:\n",
    "        preds = f.readlines()\n",
    "        for p in preds:\n",
    "            predictions.append(json.loads(p))\n",
    "    return predictions\n",
    "predictions = []\n",
    "with open(prediction_path / 'final_bird_dev.jsonl', 'r') as f:\n",
    "    preds = f.readlines()\n",
    "    for p in preds:\n",
    "        predictions.append(json.loads(p))\n",
    "\n",
    "# output_results, error_infos = get_pred_results(proj_path, predictions, tables, ds='bird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_evaluation import get_prediction_parsed_sql, get_target_parsed_sql\n",
    "from src.eval_utils import get_complexity, get_all_partial_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "california_schools: 100%|██████████| 3/3 [00:00<00:00, 456.88it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "california_schools: 100%|██████████| 3/3 [00:00<00:00, 444.52it/s]\n"
     ]
    }
   ],
   "source": [
    "target_parsed, error_ids = get_target_parsed_sql([x for x in dev_samples if x.sample_id in [9503, 9468, 9494]], tables=bird_tables)\n",
    "pred_parsed, error_ids = get_prediction_parsed_sql(predictions[:3], tables=bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_id = 'california_schools'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ID: 9503\n",
      "f1=0.7798 | 0.7657 | 0.8017\n",
      "Sample ID: 9468\n",
      "f1=0.4271 | 0.4170 | 0.5096\n"
     ]
    }
   ],
   "source": [
    "for pred in predictions[:3]:\n",
    "    sample_id = pred['sample_id']\n",
    "    target_o = target_parsed[db_id][sample_id]\n",
    "    pred_o = pred_parsed[db_id][sample_id]\n",
    "    if pred_o:\n",
    "        _, all_score = get_all_partial_score(pred_o, target_o)\n",
    "\n",
    "        print(f\"Sample ID: {sample_id}\")\n",
    "        print(f'f1={all_score[\"overall\"]} | {all_score[\"structural\"]:.4f} | {all_score[\"semantic\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_id': 9494,\n",
       " 'rationale': ['Identify the relevant table: frpm contains the enrollment data.',\n",
       "  \"Determine the columns needed: 'enrollment (ages 5-17)' for the number of students, 'academic year' to filter by the specific year, and 'district type' to specify the type of school.\",\n",
       "  'Filter the records for the academic year 2014-2015.',\n",
       "  \"Filter for the district type 'State Special Schools'.\",\n",
       "  \"Filter for the county name 'Fremont'.\",\n",
       "  'Use SUM() to aggregate the total enrollment for the specified filters.'],\n",
       " 'pred_sql': \"SELECT SUM(enrollment (ages 5-17)) AS total_enrollment\\nFROM frpm\\nWHERE academic year = '2014-2015' AND district type = 'State Special Schools' AND county name = 'Fremont';\",\n",
       " 'db_id': 'california_schools',\n",
       " 'gold_sql': 'SELECT T1.\"Enrollment (Ages 5-17)\" FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.EdOpsCode = \\'SSS\\' AND T2.City = \\'Fremont\\' AND T1.\"Academic Year\" BETWEEN 2014 AND 2015'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "california_schools: 100%|██████████| 1/1 [00:00<00:00, 465.31it/s]\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT SUM(\"enrollment (ages 5-17)\") AS total_enrollment\n",
    "FROM frpm\n",
    "WHERE \"academic year\" = '2014-2015' AND \"district type\" = 'State Special Schools' AND \"county name\" = 'Fremont';\n",
    "\"\"\"\n",
    "x = {'sample_id': 9494,\n",
    " 'rationale': ['Identify the relevant table: frpm contains the enrollment data.',\n",
    "  \"Determine the columns needed: 'enrollment (ages 5-17)' for the number of students, 'academic year' to filter by the specific year, and 'district type' to specify the type of school.\",\n",
    "  'Filter the records for the academic year 2014-2015.',\n",
    "  \"Filter for the district type 'State Special Schools'.\",\n",
    "  \"Filter for the county name 'Fremont'.\",\n",
    "  'Use SUM() to aggregate the total enrollment for the specified filters.'],\n",
    " 'pred_sql': sql,\n",
    " 'db_id': 'california_schools',\n",
    " 'gold_sql': 'SELECT T1.\"Enrollment (Ages 5-17)\" FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.EdOpsCode = \\'SSS\\' AND T2.City = \\'Fremont\\' AND T1.\"Academic Year\" BETWEEN 2014 AND 2015'}\n",
    "\n",
    "parsed, error_ids = get_prediction_parsed_sql([x], tables=bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_id = 'california_schools'\n",
    "tables = bird_tables\n",
    "db_schema = get_schema_str(\n",
    "    schema=tables[db_id].db_schema, \n",
    "    foreign_keys=tables[db_id].foreign_keys,\n",
    "    col_explanation=tables[db_id].col_explanation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = SqliteDatabase(\n",
    "    db_file=str(proj_path / 'data' / 'bird' / 'dev' / 'dev_databases' / 'california_schools' / 'california_schools.sqlite'),\n",
    "    foreign_keys=bird_tables['california_schools'].foreign_keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Enrollment (Ages 5-17)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1070.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Enrollment (Ages 5-17)\n",
       "0                  1070.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.execute(\"\"\"\n",
    "SELECT \"enrollment (ages 5-17)\"\n",
    "FROM frpm\n",
    "LIMIT 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('california_schools',\n",
       "  9494,\n",
       "  'Invalid expression / Unexpected token. Line 4, Col: 19.\\n  \\nSELECT SUM(\"enrollment (ages 5-17)\") AS total_enrollment\\nFROM frpm\\nWHERE academic \\x1b[4myear\\x1b[0m = \\'2014-2015\\' AND district type = \\'State Special Schools\\' AND county name = \\'Fremont\\';\\n')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_evaluation import get_parsed_sql\n",
    "\n",
    "parsed_path = proj_path / 'data' / 'pkl_files' \n",
    "file_name = f'bird_dev_parsed_pred.pkl'\n",
    "if not (parsed_path / file_name).exists():\n",
    "    pred_parsed, error_ids = get_parsed_sql(predictions, tables)\n",
    "    with open(parsed_path / file_name, 'wb') as f:\n",
    "        pickle.dump(pred_parsed, f)\n",
    "    print(f'Error parsing pred bird_dev: {len(error_ids)}')\n",
    "\n",
    "with (parsed_path / file_name).open('rb') as f:\n",
    "    pred_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(bos: dict[str, list[dict[str, str]]]):\n",
    "    documents = []\n",
    "    for db_id, samples in bos.items():\n",
    "        for x in samples:\n",
    "            doc = Document(\n",
    "                doc_id=x['sample_id'],\n",
    "                page_content=x['ba'],\n",
    "                metadata={\n",
    "                    'sample_id': x['sample_id'],\n",
    "                    'db_id': db_id,\n",
    "                    'vt': x['vt']\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents, \n",
    "        embedding = embeddings_model,\n",
    "    )\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = get_vector_store(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sql_bo(\n",
    "    to_pred_samples: list[SpiderSample|BirdSample],\n",
    "    tables: dict[DatabaseModel],\n",
    "    vectorstore: FAISS,\n",
    "    chain: RunnableSequence,\n",
    "    prediction_path: Path,\n",
    "    file_name: str = '[args.ds]_[args.type]',\n",
    "    n_retrieval: int = 3,\n",
    "    score_threshold: float = 0.65,\n",
    "):\n",
    "    processed_db_ids = [p.stem.split('_')[-1] for p in prediction_path.glob(f'{file_name}_*')]\n",
    "    # restart from checkpoint\n",
    "    if processed_db_ids:\n",
    "        to_pred_samples = [sample for sample in to_pred_samples if sample.db_id not in processed_db_ids]\n",
    "    \n",
    "    samples_by_db_id = defaultdict(list)\n",
    "    for sample in to_pred_samples:\n",
    "        samples_by_db_id[sample.db_id].append(sample)\n",
    "\n",
    "    for db_id, samples in samples_by_db_id.items():\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_kwargs={'k': n_retrieval, 'score_threshold': score_threshold, 'filter': {'db_id': db_id}}\n",
    "        )\n",
    "        schema_str = get_schema_str(\n",
    "            schema=tables[db_id].db_schema, \n",
    "            foreign_keys=tables[db_id].foreign_keys,\n",
    "            col_explanation=tables[db_id].col_explanation\n",
    "        )\n",
    "        results = []\n",
    "        for sample in tqdm(samples, total=len(samples), desc=f\"{db_id}\"):\n",
    "            question = sample.final.question\n",
    "            docs = retriever.invoke(question)\n",
    "            hint = '\\nDescriptions and Virtual Tables:\\n'\n",
    "            hint += json.dumps({j: {'description': doc.page_content, 'virtual_table': doc.metadata['vt']} for j, doc in enumerate(docs)}, indent=4)\n",
    "            hint += '\\n'\n",
    "            input_data = {'schema': schema_str, 'input_query': question, 'hint': hint}\n",
    "            output = chain.invoke(input=input_data)\n",
    "            \n",
    "            full_sql_output = {}\n",
    "            full_sql_output['sample_id'] = sample.sample_id\n",
    "            full_sql_output['rationale'] = output.rationale\n",
    "            full_sql_output['pred_sql'] = output.full_sql_query\n",
    "            # full_sql_output = 1\n",
    "            results.append(full_sql_output)\n",
    "\n",
    "        with open(prediction_path / f'{file_name}_{db_id}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "movie_platform: 100%|██████████| 10/10 [00:04<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_bo.json', 'r') as f:\n",
    "#     bos = json.load(res, f, indent=4)\n",
    "# vectorstore = get_vector_store(bos)\n",
    "\n",
    "\n",
    "data_path = proj_path / 'data' / 'bird'\n",
    "experiment_folder = proj_path / 'experiments' / 'bird'\n",
    "prediction_path = experiment_folder / 'predictions' / 'zero_shot_hint'\n",
    "eval_path = experiment_folder / 'evals'\n",
    "for p in [prediction_path, eval_path]:\n",
    "    if not p.exists():\n",
    "        p.mkdir(parents=True)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=Prompts.zero_shot_hints_inference,\n",
    "    input_variables=['schema', 'input_query', 'hint'],\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0,\n",
    "    frequency_penalty=0.1,\n",
    ")\n",
    "\n",
    "model = model_openai.with_structured_output(SQLResponse)\n",
    "chain = (prompt | model)\n",
    "\n",
    "n_retrieval = 3\n",
    "score_threshold = 0.65\n",
    "\n",
    "predict_sql_bo(\n",
    "    to_pred_samples=dev_samples[:10],\n",
    "    tables=bird_tables,\n",
    "    vectorstore=vectorstore,\n",
    "    chain=chain,\n",
    "    prediction_path=prediction_path,\n",
    "    n_retrieval=n_retrieval,\n",
    "    score_threshold=score_threshold,\n",
    "    file_name='bird_dev',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptions and Virtual Tables:\n",
      "{\n",
      "    \"0\": {\n",
      "        \"description\": \"The virtual table retrieves the titles of movies that have been rated, filtering by a specific rating timestamp and grouping the results by movie title. The results are ordered by the count of ratings for each movie title, and a limit is applied to restrict the number of returned titles.\",\n",
      "        \"virtual_table\": \"SELECT movies.movie_title FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE ratings.rating_timestamp_utc LIKE '[placeholder-type:string]' GROUP BY movies.movie_title ORDER BY COUNT(movies.movie_title) LIMIT [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"1\": {\n",
      "        \"description\": \"The virtual table provides a count of users who have rated a specific movie, identified by its title, while also filtering for users who were trialists at the time of rating. It combines data from the 'ratings' and 'movies' tables to achieve this.\",\n",
      "        \"virtual_table\": \"SELECT COUNT(ratings.user_id) FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE movies.movie_title = '[placeholder-type:string]' AND ratings.user_trialist = [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"description\": \"The virtual table retrieves the titles of movies that have been rated, ordered by the number of likes received on the critics' comments. The query joins the 'ratings' table with the 'movies' table to access the movie titles associated with each rating. The result is limited to a specified number of entries, allowing users to see the most liked critics' comments for rated movies.\",\n",
      "        \"virtual_table\": \"SELECT movies.movie_title FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id ORDER BY ratings.critic_likes LIMIT [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"description\": \"The virtual table displays the titles of movies from the 'movies' table that have received a certain number of likes on user-written critiques. The query joins the 'ratings' table with the 'movies' table to filter movies based on the number of likes their critiques have received, using a placeholder for the minimum number of likes.\",\n",
      "        \"virtual_table\": \"SELECT movies.movie_title FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE ratings.critic_likes > [placeholder-type:numeric]\"\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"description\": \"The virtual table counts the number of ratings for a specific movie title from the 'movies' table, filtering by the movie's title and a specified rating timestamp. The placeholders represent the movie title and the date from which to count ratings.\",\n",
      "        \"virtual_table\": \"SELECT COUNT(ratings.rating_id) FROM ratings INNER JOIN movies AS T2 ON T1.movie_id = T2.movie_id WHERE movies.movie_title = '[placeholder-type:string]' AND ratings.rating_timestamp_utc >= '[placeholder-type:string]'\"\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(sample.final.question)\n",
    "hint = '\\nDescriptions and Virtual Tables:\\n'\n",
    "hint += json.dumps({j: {'description': doc.page_content, 'virtual_table': doc.metadata['vt']} for j, doc in enumerate(docs)}, indent=4)\n",
    "hint += '\\n'\n",
    "input_data = {'schema': db_schema, 'input_query': row['question'], 'hint': hint}\n",
    "output = chain.invoke(input=input_data)\n",
    "\n",
    "print(hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "movie_platform:   0%|          | 0/6341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debit_card_specializing: 100%|██████████| 6341/6341 [00:21<00:00, 299.00it/s]    \n",
      "debit_card_specializing: 100%|██████████| 2091/2091 [00:05<00:00, 408.98it/s]    \n",
      "debit_card_specializing: 100%|██████████| 2193/2193 [00:09<00:00, 225.64it/s]    \n"
     ]
    }
   ],
   "source": [
    "def get_parsed_sql(samples: dict, tables: dict):\n",
    "    error_ids = []\n",
    "    parsed = defaultdict(dict)\n",
    "    iterator = tqdm(samples, total=len(samples))\n",
    "    for sample in iterator:\n",
    "        db_id = sample.db_id\n",
    "        sample_id = sample.sample_id\n",
    "        iterator.set_description(f\"{db_id}\")\n",
    "        schema = Schema(tables[db_id].db_schema)\n",
    "        sql_i = sample.final.sql\n",
    "        try:\n",
    "            ei = extract_all(sql_i, schema)\n",
    "            assert len(ei['sel']) > 0, f'No selection found-{db_id}-{sample_id}'\n",
    "        except Exception as e:\n",
    "            error_ids.append((db_id, sample_id, str(e)))\n",
    "            parsed[db_id].append(None)\n",
    "            continue\n",
    "        parsed[db_id][sample_id] = ei\n",
    "    return parsed, error_ids\n",
    "\n",
    "train_parsed, error_ids = get_parsed_sql(train_samples, bird_tables)\n",
    "dev_parsed, error_ids = get_parsed_sql(dev_samples, bird_tables)\n",
    "test_parsed, error_ids = get_parsed_sql(test_samples, bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_train_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_parsed, f)\n",
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(dev_parsed, f)\n",
    "\n",
    "# with open(proj_path / 'data' / 'pkl_files' / 'bird_test_parsed.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_parsed, f)\n",
    "\n",
    "with open(proj_path / 'data' / 'pkl_files' / 'bird_dev_parsed.pkl', 'rb') as f:\n",
    "    dev_parsed = pickle.load(f)\n",
    "\n",
    "with open(proj_path / 'data' / 'pkl_files' / 'bird_test_parsed.pkl', 'rb') as f:\n",
    "    test_parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, product\n",
    "from collections import defaultdict\n",
    "from src.eval_utils import get_all_partial_score\n",
    "\n",
    "def measure_inter_score(parsed1: dict[str, tuple], parsed2: dict[str, tuple]):\n",
    "    results = defaultdict()\n",
    "    assert len(parsed1) == len(parsed2), f\"Length mismatch-1: {len(parsed1)} 2:{len(parsed2)}\"\n",
    "    db_ids = list(parsed1.keys())\n",
    "    for db_id in db_ids:\n",
    "        o1 = parsed1[db_id]\n",
    "        o2 = parsed2[db_id]\n",
    "        n1 = len(o1)\n",
    "        n2 = len(o2)\n",
    "        semantic_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "        structural_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "        overall_sim = np.zeros((n1, n2), dtype=np.float32)\n",
    "\n",
    "        idxs = list(product(range(n1), range(n2)))\n",
    "        iterator = tqdm(idxs, total=len(idxs), desc=f\"{db_id}\")\n",
    "        for i, j in iterator:\n",
    "            ei = o1[i]\n",
    "            ej = o2[j]\n",
    "\n",
    "            _, final_score = get_all_partial_score(ei, ej, use_bert=True)\n",
    "\n",
    "            structural_sim[i, j] = final_score['structural']\n",
    "            semantic_sim[i, j] = final_score['semantic']\n",
    "            overall_sim[i, j] = final_score['overall']\n",
    "\n",
    "        results[db_id] = {\n",
    "            'semantic': semantic_sim,\n",
    "            'struct': structural_sim,\n",
    "            'overall': overall_sim\n",
    "        }\n",
    "    return results\n",
    "\n",
    "results = measure_inter_score(dev_parsed, test_parsed)\n",
    "with (proj_path / 'data' / 'pkl_files' / 'bird_dev_test_similarity.pkl').open('wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity between datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6341/6341 [00:10<00:00, 631.17it/s]\n",
      "100%|██████████| 2091/2091 [00:03<00:00, 589.55it/s]\n",
      "100%|██████████| 2193/2193 [00:03<00:00, 591.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def measure_complexity(samples, tables):\n",
    "    cs = []\n",
    "    for s in tqdm(samples, total=len(samples)):\n",
    "        schema = Schema(tables[s.db_id].db_schema)\n",
    "        output = extract_all(s.final.sql, schema)\n",
    "        complexity = get_complexity(output)\n",
    "        cs.append(complexity)\n",
    "    return cs\n",
    "\n",
    "train_complexities = measure_complexity(train_samples, bird_tables)\n",
    "dev_complexities = measure_complexity(dev_samples, bird_tables)\n",
    "test_complexities = measure_complexity(test_samples, bird_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Mean=0.2753 +/-0.0476, Median=0.2710\n",
      "[dev  ] Mean=0.2758 +/-0.0471, Median=0.2710\n",
      "[test ] Mean=0.2760 +/-0.0477, Median=0.2709\n"
     ]
    }
   ],
   "source": [
    "for c, n in zip([train_complexities, dev_complexities, test_complexities], ['train', 'dev  ', 'test ']):\n",
    "    print(f'[{n}] Mean={np.mean(c):.4f} +/-{np.std(c):.4f}, Median={np.median(c):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = defaultdict(list)\n",
    "for s in dev_samples:\n",
    "    stats[s.db_id].append(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
