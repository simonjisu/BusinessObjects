{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from src.db_utils import get_schema_str\n",
    "from src.database import SqliteDatabase\n",
    "from src.spider_sparc_preprocess import (\n",
    "    load_spider_sparc_data,\n",
    "    process_all_tables, \n",
    "    load_samples_spider,\n",
    "    load_samples_sparc,\n",
    "    filter_samples_by_count_sparc,\n",
    "    filter_samples_by_count_spider, \n",
    "    process_samples_sparc,\n",
    "    process_samples_spider, \n",
    "    split_train_dev_test,\n",
    "    save_samples_spider\n",
    ")\n",
    "\n",
    "proj_path = Path('.').resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_path = proj_path / 'data' / 'spider'\n",
    "# tables, train_data, dev_data = load_spider_sparc_data(spider_path, load_test=False)\n",
    "\n",
    "# with (proj_path / 'data' / 'description.json').open() as f:\n",
    "#     all_descriptions = json.load(f)\n",
    "# spider_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "# # # --------------- split_in_domain ----------------\n",
    "# # all_data = filter_samples_by_count_spider(train_data+dev_data, n=10)\n",
    "# # # process samples -> {db_id: list of samples}\n",
    "# # spider_samples = process_samples_spider(all_data, spider_tables, skip=[])\n",
    "# # # change train/dev by sample\n",
    "# # train_samples, dev_samples, test_samples = split_train_dev_test(spider_samples, train_ratio=0.8, dev_ratio=0.1)\n",
    "\n",
    "# # ------------------ split_by_domain ----------------\n",
    "# def values_flatten(x: dict[str, list]) -> list:\n",
    "#     return [item for sublist in x.values() for item in sublist]\n",
    "\n",
    "# train_samples = process_samples_spider(train_data, spider_tables, skip=[])\n",
    "# train_samples = values_flatten(train_samples)\n",
    "\n",
    "# dev_samples = process_samples_spider(dev_data, spider_tables, skip=[])\n",
    "# dev_samples = values_flatten(dev_samples)\n",
    "# # test_samples = process_samples_spider(test_data, spider_tables, skip=[])\n",
    "# test_samples = []\n",
    "# print(f'Number of train: {len(train_samples)} | Number of dev: {len(dev_samples)} | Number of test: {len(test_samples)}')\n",
    "\n",
    "# save_samples_spider(train_samples, proj_path / 'data' / 'spider_train.json')\n",
    "# save_samples_spider(dev_samples, proj_path / 'data' / 'spider_dev.json')\n",
    "# # save_samples_spider(test_samples, proj_path / 'data' / 'spider_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train: 7000 | Number of dev: 1034 | Number of test: 0\n"
     ]
    }
   ],
   "source": [
    "with (proj_path / 'data' / 'spider' / f'tables.json').open() as f:\n",
    "    tables = json.load(f)\n",
    "\n",
    "with (proj_path / 'data' / 'description.json').open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "spider_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "train_samples = load_samples_spider(proj_path / 'data' / 'spider_train.json')\n",
    "dev_samples = load_samples_spider(proj_path / 'data' / 'spider_dev.json')\n",
    "# test_samples = load_samples_spider(proj_path / 'data' / 'spider_test.json')\n",
    "test_samples = []\n",
    "print(f'Number of train: {len(train_samples)} | Number of dev: {len(dev_samples)} | Number of test: {len(test_samples)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SQL generation\n",
    "\n",
    "* gpt-4o-mini\n",
    "* gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from src.spider_sparc_preprocess import DatabaseModel, SpiderSample\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "class Response(BaseModel):\n",
    "    full_sql_query: str = Field(description='The full SQL query.')\n",
    "    rationale: list[str] = Field(description='The step-by-step reasoning to generate the SQL query. Each step has ')\n",
    "\n",
    "# class Response(BaseModel):\n",
    "#     output: list[OutputFormat]\n",
    "\n",
    "template = '''### TASK\n",
    "You are tasked with generating a SQL query(in a SQLite Database) according to a user input request.\n",
    "You should work in step-by-step reasoning before coming to the full SQL query.\n",
    "\n",
    "You will be provided an input NL query.\n",
    "\n",
    "### SCHEMA\n",
    "You are working with the following schema in a SQLite Database:\n",
    "{schema}\n",
    "\n",
    "### FORMATTING\n",
    "Your output should be of the following JSON format:\n",
    "{{\n",
    "    \"rationale\": \"<list[str]: the step-by-step reasoning to generate the SQL query>\",\n",
    "    \"full_sql_query\": \"<str: the full SQL query>\"\n",
    "}}\n",
    "\n",
    "### OUTPUT\n",
    "<INPUT QUERY>: {input_query}\n",
    "<OUTPUT>: \n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['schema', 'input_query']\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "model = model_openai.with_structured_output(Response)\n",
    "chain = (prompt | model)\n",
    "\n",
    "def predict_sql(samples: list[SpiderSample], spider_tables: dict[str, DatabaseModel], chain: RunnableSequence, k: int = 500, file_name: str = 'full_sql_output') -> list[dict]:\n",
    "    all_full_sql = list()\n",
    "    for i, data in tqdm(enumerate(samples), total=len(samples)):\n",
    "        db_schema = get_schema_str(\n",
    "            schema=spider_tables[data.db_id].db_schema, \n",
    "            foreign_keys=spider_tables[data.db_id].foreign_keys,\n",
    "            col_explanation=spider_tables[data.db_id].col_explanation\n",
    "        )\n",
    "        input_data = {'schema': db_schema, 'input_query': data.final.question}\n",
    "        output = chain.invoke(input=input_data)\n",
    "\n",
    "        full_sql_output = {}\n",
    "        full_sql_output['sample_id'] = data.sample_id\n",
    "        full_sql_output['db_id'] = data.db_id\n",
    "        full_sql_output['question'] = data.final.question\n",
    "        full_sql_output['rationale'] = output.rationale\n",
    "        full_sql_output['pred_sql'] = output.full_sql_query\n",
    "        full_sql_output['gold_sql'] = data.final.sql\n",
    "        full_sql_output['source_tables'] = data.final.source_tables\n",
    "        all_full_sql.append(full_sql_output)\n",
    "\n",
    "        if len(all_full_sql) == k:\n",
    "            with open(proj_path / 'experiments' / f'{file_name}_{i//k}.jsonl', 'w') as f:\n",
    "                for d in all_full_sql:\n",
    "                    f.write(json.dumps(d) + '\\n')\n",
    "            all_full_sql = list()\n",
    "\n",
    "    if len(all_full_sql) > 0:\n",
    "        with open(proj_path / 'experiments' / f'{file_name}_{i//k}.jsonl', 'w') as f:\n",
    "            for d in all_full_sql:\n",
    "                f.write(json.dumps(d) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_predictions = predict_sql(train_samples, spider_tables, chain, k=500, file_name='spider_train')\n",
    "# dev_predictions = predict_sql(dev_samples, spider_tables, chain, k=500, file_name='spider_dev')\n",
    "# test_predictions = predict_sql(test_samples, spider_tables, chain, k=500, file_name='spider_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(file_pattern: str) -> list[dict]:\n",
    "    predictions = []\n",
    "    for p in sorted((proj_path / 'experiments').glob(file_pattern), key=lambda x: int(x.stem.split('_')[-1])):\n",
    "        with p.open() as f:\n",
    "            for line in f:\n",
    "                predictions.append(json.loads(line))\n",
    "    return predictions\n",
    "\n",
    "train_predictions = load_predictions('spider_train_*')\n",
    "dev_predictions = load_predictions('spider_dev_*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pred_exec: 0 | gold_exec: 0 | python_script: 0 | result: 0:   0%|          | 0/7000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pred_exec: 115 | gold_exec: 3 | python_script: 0 | result: 1702: 100%|██████████| 7000/7000 [00:41<00:00, 169.96it/s]\n",
      "pred_exec: 13 | gold_exec: 2 | python_script: 0 | result: 243: 100%|██████████| 1034/1034 [00:07<00:00, 133.54it/s]\n"
     ]
    }
   ],
   "source": [
    "## database execution evaluation\n",
    "import sqlglot\n",
    "from src.eval import result_eq, check_if_exists_orderby\n",
    "\n",
    "def get_output_results(predictions: list[dict], spider_tables: dict[str, DatabaseModel]) -> tuple[list[dict], dict[str, list]]:\n",
    "    output_results = []\n",
    "    error_infos = {\n",
    "        'pred_exec': [],\n",
    "        'gold_exec': [],\n",
    "        'python_script': [],\n",
    "        'result': []\n",
    "    }\n",
    "\n",
    "    iterator = tqdm(predictions, total=len(predictions))\n",
    "    for data in iterator:\n",
    "        iterator.set_description(f'pred_exec: {len(error_infos[\"pred_exec\"])} | gold_exec: {len(error_infos[\"gold_exec\"])} | python_script: {len(error_infos[\"python_script\"])} | result: {len(error_infos[\"result\"])}')\n",
    "        sample_id = data['sample_id']\n",
    "        db_id = data['db_id']\n",
    "        table = spider_tables[db_id]\n",
    "        database = SqliteDatabase(str(proj_path / 'data' / 'spider' / 'database' / db_id / f'{db_id}.sqlite'), foreign_keys=table.foreign_keys)\n",
    "        pred_sql = data['pred_sql'] # sqlglot.parse_one(, read='sqlite').sql()\n",
    "        gold_sql = data['gold_sql']\n",
    "        \n",
    "        error_info = ''\n",
    "        try:\n",
    "            pred_result = database.execute(pred_sql, rt_pandas=False)\n",
    "        except Exception as e:\n",
    "            pred_result = []\n",
    "            error_infos['pred_exec'].append(sample_id)\n",
    "            error_info = 'Predction Execution Error:' + str(e)\n",
    "            score = 0\n",
    "        try:\n",
    "            gold_result = database.execute(gold_sql, rt_pandas=False)\n",
    "        except Exception as e:\n",
    "            error_infos['gold_exec'].append(sample_id)\n",
    "            error_info = 'Gold Execution Error:' + str(e)\n",
    "\n",
    "        if 'Gold Execution Error' in error_info:\n",
    "            continue\n",
    "        elif 'Predction Execution Error' in error_info:\n",
    "            output_results.append(\n",
    "                {\n",
    "                    'sample_id': sample_id, \n",
    "                    'db_id': db_id,\n",
    "                    'score': score,\n",
    "                    'gold_sql': gold_sql,\n",
    "                    'pred_sql': pred_sql,\n",
    "                    'source_tables': data['source_tables'],\n",
    "                    'error_info': error_info\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "        else:\n",
    "            exists_orderby = check_if_exists_orderby(gold_sql)\n",
    "            \n",
    "            try:\n",
    "                score = int(result_eq(pred_result, gold_result, order_matters=exists_orderby))\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                score = 0\n",
    "                error_info = 'Python Script Error:' + str(e)\n",
    "                error_infos['python_script'].append(sample_id)\n",
    "\n",
    "            if score == 0 and error_info == '':\n",
    "                error_info = 'Result not equal'\n",
    "                error_infos['result'].append(sample_id)\n",
    "            output_results.append(\n",
    "                {\n",
    "                    'sample_id': sample_id, \n",
    "                    'db_id': db_id,\n",
    "                    'score': score,\n",
    "                    'gold_sql': gold_sql,\n",
    "                    'pred_sql': pred_sql,\n",
    "                    'source_tables': data['source_tables'],\n",
    "                    'error_info': error_info\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return output_results, error_infos\n",
    "\n",
    "# train_output_results, train_errors = get_output_results(train_predictions, spider_tables)\n",
    "# dev_output_results, dev_errors = get_output_results(dev_predictions, spider_tables)\n",
    "\n",
    "# eval_path = proj_path / 'experiments' / 'evals'\n",
    "# if not eval_path.exists():\n",
    "#     eval_path.mkdir()\n",
    "\n",
    "# with open(eval_path / 'spider_train_eval.json', 'w') as f:\n",
    "#     json.dump(train_output_results, f)\n",
    "\n",
    "# with open(eval_path / 'spider_train_errors.json', 'w') as f:\n",
    "#     json.dump(train_errors, f)\n",
    "\n",
    "# with open(eval_path / 'spider_dev_eval.json', 'w') as f:\n",
    "#     json.dump(dev_output_results, f)\n",
    "\n",
    "# with open(eval_path / 'spider_dev_errors.json', 'w') as f:\n",
    "#     json.dump(dev_errors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complesity of the SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = proj_path / 'experiments' / 'evals'\n",
    "\n",
    "with open(eval_path / 'spider_train_eval.json') as f:\n",
    "    train_output_results = json.load(f)\n",
    "\n",
    "with open(eval_path / 'spider_train_errors.json') as f:\n",
    "    train_errors = json.load(f)\n",
    "\n",
    "with open(eval_path / 'spider_dev_eval.json') as f:\n",
    "    dev_output_results = json.load(f)\n",
    "\n",
    "with open(eval_path / 'spider_dev_errors.json') as f:\n",
    "    dev_errors = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_exec: 115 | gold_exec: 3 | python_script: 0 | result: 1702\n",
      "pred_exec: 13 | gold_exec: 2 | python_script: 0 | result: 244\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df_train_results = pd.DataFrame(train_output_results)\n",
    "df_dev_results = pd.DataFrame(dev_output_results)\n",
    "\n",
    "df_train_results['tbls'] = df_train_results['source_tables'].str.join(',')\n",
    "df_dev_results['tbls'] = df_dev_results['source_tables'].str.join(',')\n",
    "df_train_results['len_tbls'] = df_train_results['source_tables'].apply(len)\n",
    "df_dev_results['len_tbls'] = df_dev_results['source_tables'].apply(len)\n",
    "\n",
    "print(f'pred_exec: {len(train_errors[\"pred_exec\"])} | gold_exec: {len(train_errors[\"gold_exec\"])} | python_script: {len(train_errors[\"python_script\"])} | result: {len(train_errors[\"result\"])}')\n",
    "print(f'pred_exec: {len(dev_errors[\"pred_exec\"])} | gold_exec: {len(dev_errors[\"gold_exec\"])} | python_script: {len(dev_errors[\"python_script\"])} | result: {len(dev_errors[\"result\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.process_sql import get_sql, get_schema, Schema\n",
    "from src.eval import Evaluator\n",
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "import sqlglot\n",
    "import sqlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "\n",
    "db_id = 'department_management'\n",
    "db_path = str(proj_path / 'data' / 'spider' / 'database' / db_id / f'{db_id}.sqlite')\n",
    "schema = Schema(schema=get_schema(db_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_train_results.loc[df_train_results['db_id'] == 'department_management', 'gold_sql'].values:\n",
    "    print(x)\n",
    "    sql = get_sql(schema, x)\n",
    "    print(evaluator.eval_hardness(sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error_info\n",
       "Predction Execution Error:no such column: duration               12\n",
       "Predction Execution Error:no such column: training_hours         10\n",
       "Predction Execution Error:no such function: TIME_TO_STR           8\n",
       "Predction Execution Error:no such column: l.fastestLapSpeed       4\n",
       "Predction Execution Error:no such column: files.resolution        4\n",
       "                                                                 ..\n",
       "Predction Execution Error:no such column: p.w                     1\n",
       "Predction Execution Error:ambiguous column name: attendance       1\n",
       "Predction Execution Error:no such column: student_id              1\n",
       "Predction Execution Error:no such column: S.date_of_enrolment     1\n",
       "Predction Execution Error:no such column: vehicle_details         1\n",
       "Name: count, Length: 67, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_exec: usually the error is due to the understanding of the schema\n",
    "df_train_pred_exec_errors = df_train_results.loc[df_train_results['sample_id'].isin(train_errors['pred_exec']), 'error_info'].value_counts()\n",
    "df_dev_pred_exec_errors = df_dev_results.loc[df_dev_results['sample_id'].isin(dev_errors['pred_exec']), 'error_info'].value_counts()\n",
    "df_train_pred_exec_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 74.03% (5180/6997) | Dev Accuracy: 75.29% (777/1032)\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "n_train = len(df_train_results)\n",
    "n_dev = len(df_dev_results)\n",
    "n_train_correct = df_train_results['score'].sum()\n",
    "n_dev_correct = df_dev_results['score'].sum()\n",
    "acc_train = n_train_correct / n_train\n",
    "acc_dev = n_dev_correct / n_dev\n",
    "print(f'Train Accuracy: {acc_train*100:.2f}% ({n_train_correct}/{n_train}) | Dev Accuracy: {acc_dev*100:.2f}% ({n_dev_correct}/{n_dev})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy by number of tables:\n",
      "1 tables: 79.00% (3104/3929)\n",
      "2 tables: 65.78% (1536/2335)\n",
      "3 tables: 73.91% (493/667)\n",
      "4 tables: 75.86% (44/58)\n",
      "5 tables: 37.50% (3/8)\n",
      "Dev Accuracy by number of tables:\n",
      "1 tables: 82.20% (471/573)\n",
      "2 tables: 69.47% (273/393)\n",
      "3 tables: 46.67% (28/60)\n",
      "4 tables: 83.33% (5/6)\n"
     ]
    }
   ],
   "source": [
    "# group by number of tables\n",
    "len_tbls_train = df_train_results.groupby('len_tbls')['score'].agg(['sum', 'count', 'mean'])\n",
    "len_tbls_dev = df_dev_results.groupby('len_tbls')['score'].agg(['sum', 'count', 'mean'])\n",
    "\n",
    "print('Train Accuracy by number of tables:')\n",
    "for i, x in len_tbls_train.iterrows():\n",
    "    s = int(x['sum'])\n",
    "    c = int(x['count'])\n",
    "    m = x['mean']\n",
    "    print(f'{i} tables: {m*100:.2f}% ({s}/{c})')\n",
    "\n",
    "print('Dev Accuracy by number of tables:')\n",
    "for i, x in len_tbls_dev.iterrows():\n",
    "    s = int(x['sum'])\n",
    "    c = int(x['count'])\n",
    "    m = x['mean']\n",
    "    print(f'{i} tables: {m*100:.2f}% ({s}/{c})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only result not equal errors\n",
    "df_train_result_errors = df_train_results.loc[df_train_results['sample_id'].isin(train_errors['result'])]\n",
    "df_dev_result_errors = df_dev_results.loc[df_dev_results['sample_id'].isin(dev_errors['result'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id</th>\n",
       "      <th>tbls</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>activity</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>activity,faculty,faculty_participates_in</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>activity,faculty_participates_in</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>activity,participates_in</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>activity_1</td>\n",
       "      <td>faculty</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>workshop_paper</td>\n",
       "      <td>submission</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>workshop_paper</td>\n",
       "      <td>workshop</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>wrestler</td>\n",
       "      <td>elimination</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>wrestler</td>\n",
       "      <td>elimination,wrestler</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>wrestler</td>\n",
       "      <td>wrestler</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1111 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               db_id                                      tbls  count\n",
       "0         activity_1                                  activity      4\n",
       "1         activity_1  activity,faculty,faculty_participates_in      8\n",
       "2         activity_1          activity,faculty_participates_in      4\n",
       "3         activity_1                  activity,participates_in      4\n",
       "4         activity_1                                   faculty     34\n",
       "...              ...                                       ...    ...\n",
       "1106  workshop_paper                                submission     18\n",
       "1107  workshop_paper                                  workshop      2\n",
       "1108        wrestler                               elimination     12\n",
       "1109        wrestler                      elimination,wrestler     10\n",
       "1110        wrestler                                  wrestler     18\n",
       "\n",
       "[1111 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group by true-positive, false-negative, false-positive, true-negative\n",
    "\n",
    "df_count = df_train_results.groupby(['db_id', 'tbls'])['tbls'].count()\n",
    "df_count = df_count.rename('count').reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "1. Single table query performance vs multiple table query performance\n",
    "2. Cofusion matrix for each db_id, calculate the average precision, recall, AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which department has the largest number of employees?\n",
      "SELECT name FROM department GROUP BY departmentID ORDER BY count(departmentID) DESC LIMIT 1;\n",
      "[Table and Columns]\n",
      "Table Name: Physician\n",
      "  - EmployeeID(text): Unique identifier for each physician.\n",
      "  - Name(text): Full name of the physician.\n",
      "  - Position(text): Job title or role of the physician.\n",
      "  - SSN(text): Social Security Number of the physician.\n",
      "Table Name: Department\n",
      "  - DepartmentID(number): Unique identifier for each department.\n",
      "  - Name(number): Name of the department.\n",
      "  - Head(number): Identifier for the head of the department.\n",
      "Table Name: Affiliated_With\n",
      "  - Physician(text): Identifier for the physician.\n",
      "  - Department(text): Identifier for the department.\n",
      "  - PrimaryAffiliation(text): Indicates if this is the primary affiliation.\n",
      "Table Name: Procedures\n",
      "  - Code(text): Unique code for each medical procedure.\n",
      "  - Name(text): Name of the medical procedure.\n",
      "  - Cost(text): Cost associated with the procedure.\n",
      "Table Name: Trained_In\n",
      "  - Physician(number): Identifier for the physician.\n",
      "  - Treatment(number): Identifier for the treatment.\n",
      "  - CertificationDate(number): Date when the physician was certified.\n",
      "  - CertificationExpires(number): Date when the certification expires.\n",
      "Table Name: Patient\n",
      "  - SSN(number): Social Security Number of the patient.\n",
      "  - Name(number): Full name of the patient.\n",
      "  - Address(number): Residential address of the patient.\n",
      "  - Phone(number): Contact phone number of the patient.\n",
      "  - InsuranceID(number): Identifier for the patient's insurance.\n",
      "  - PCP(number): Identifier for the patient's primary care physician.\n",
      "Table Name: Nurse\n",
      "  - EmployeeID(text): Unique identifier for each nurse.\n",
      "  - Name(text): Full name of the nurse.\n",
      "  - Position(text): Job title or role of the nurse.\n",
      "  - Registered(text): Indicates if the nurse is registered.\n",
      "  - SSN(text): Social Security Number of the nurse.\n",
      "Table Name: Appointment\n",
      "  - AppointmentID(number): Unique identifier for each appointment.\n",
      "  - Patient(number): Identifier for the patient.\n",
      "  - PrepNurse(number): Identifier for the nurse preparing the patient.\n",
      "  - Physician(number): Identifier for the physician conducting the appointment.\n",
      "  - Start(number): Start time of the appointment.\n",
      "  - End(number): End time of the appointment.\n",
      "  - ExaminationRoom(number): Identifier for the examination room.\n",
      "Table Name: Medication\n",
      "  - Code(number): Unique code for each medication.\n",
      "  - Name(number): Name of the medication.\n",
      "  - Brand(number): Brand name of the medication.\n",
      "  - Description(number): Description of the medication.\n",
      "Table Name: Prescribes\n",
      "  - Physician(number): Identifier for the physician prescribing the medication.\n",
      "  - Patient(number): Identifier for the patient receiving the prescription.\n",
      "  - Medication(number): Identifier for the prescribed medication.\n",
      "  - Date(number): Date when the medication was prescribed.\n",
      "  - Appointment(number): Identifier for the appointment related to the prescription.\n",
      "  - Dose(number): Dosage of the medication prescribed.\n",
      "Table Name: Block\n",
      "  - BlockFloor(boolean): Indicates if the floor is blocked.\n",
      "  - BlockCode(boolean): Indicates if the block code is active.\n",
      "Table Name: Room\n",
      "  - RoomNumber(number): Unique identifier for each room.\n",
      "  - RoomType(number): Type of the room (e.g., patient room, examination room).\n",
      "  - BlockFloor(number): Identifier for the blocked floor status.\n",
      "  - BlockCode(number): Identifier for the blocked code status.\n",
      "  - Unavailable(number): Indicates if the room is unavailable.\n",
      "Table Name: On_Call\n",
      "  - Nurse(text): Identifier for the nurse on call.\n",
      "  - BlockFloor(text): Identifier for the blocked floor status.\n",
      "  - BlockCode(text): Identifier for the blocked code status.\n",
      "  - OnCallStart(text): Start time of the on-call period.\n",
      "  - OnCallEnd(text): End time of the on-call period.\n",
      "Table Name: Stay\n",
      "  - StayID(number): Unique identifier for each hospital stay.\n",
      "  - Patient(number): Identifier for the patient staying in the hospital.\n",
      "  - Room(number): Identifier for the room where the patient is staying.\n",
      "  - StayStart(number): Start date and time of the hospital stay.\n",
      "  - StayEnd(number): End date and time of the hospital stay.\n",
      "Table Name: Undergoes\n",
      "  - Patient(number): Identifier for the patient undergoing procedures.\n",
      "  - Procedures(number): Identifier for the procedures being undergone.\n",
      "  - Stay(number): Identifier for the stay associated with the procedures.\n",
      "  - DateUndergoes(number): Date when the procedures are performed.\n",
      "  - Physician(number): Identifier for the physician performing the procedures.\n",
      "  - AssistingNurse(number): Identifier for the nurse assisting during the procedures.\n",
      "\n",
      "[Foreign Keys]\n",
      "Department.Head = Physician.EmployeeID\n",
      "Affiliated_With.Department = Department.DepartmentID\n",
      "Affiliated_With.Physician = Physician.EmployeeID\n",
      "Trained_In.Treatment = Procedures.Code\n",
      "Trained_In.Physician = Physician.EmployeeID\n",
      "Patient.PCP = Physician.EmployeeID\n",
      "Appointment.Physician = Physician.EmployeeID\n",
      "Appointment.PrepNurse = Nurse.EmployeeID\n",
      "Appointment.Patient = Patient.SSN\n",
      "Prescribes.Appointment = Appointment.AppointmentID\n",
      "Prescribes.Medication = Medication.Code\n",
      "Prescribes.Patient = Patient.SSN\n",
      "Prescribes.Physician = Physician.EmployeeID\n",
      "Room.BlockFloor = Block.BlockFloor\n",
      "Room.BlockCode = Block.BlockCode\n",
      "On_Call.BlockFloor = Block.BlockFloor\n",
      "On_Call.BlockCode = Block.BlockCode\n",
      "On_Call.Nurse = Nurse.EmployeeID\n",
      "Stay.Room = Room.RoomNumber\n",
      "Stay.Patient = Patient.SSN\n",
      "Undergoes.AssistingNurse = Nurse.EmployeeID\n",
      "Undergoes.Physician = Physician.EmployeeID\n",
      "Undergoes.Stay = Stay.StayID\n",
      "Undergoes.Procedures = Procedures.Code\n",
      "Undergoes.Patient = Patient.SSN\n"
     ]
    }
   ],
   "source": [
    "db_id = 'hospital_1'\n",
    "train_subsamples = list(filter(lambda x: x.db_id == db_id, train_samples))\n",
    "dev_subsamples = list(filter(lambda x: x.db_id == db_id, dev_samples))\n",
    "s = get_schema_str(\n",
    "    schema=spider_tables[db_id].db_schema, \n",
    "    foreign_keys=spider_tables[db_id].foreign_keys,\n",
    "    col_explanation=spider_tables[db_id].col_explanation,\n",
    "    col_fmt='', skip_type=False)\n",
    "print(train_subsamples[0].final.question)\n",
    "print(train_subsamples[0].final.sql)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Interest Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Dataset for training a cross-encoder\n",
    "\n",
    "* if two questions share the same source table, they are considered as a co-related pair\n",
    "* using jaccard similarity to label the common interest: \n",
    "    * e.g., $q_1$ has three tables $t_1, t_2, t_3$, $q_2$ has two tables $t_1, t_2$, then the jaccard similarity is $2/3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import groupby, combinations, product\n",
    "from src.spider_sparc_preprocess import SpiderSample, SparcSample\n",
    "\n",
    "def jaccard_similarity(i_tables: str|set, j_tables: str|set):\n",
    "    def preprocess(tables: str):\n",
    "        return set([t.strip() for t in tables.split(',')])\n",
    "    # Get the number of common tables\n",
    "    i_set = preprocess(i_tables) if isinstance(i_tables, str) else i_tables\n",
    "    j_set = preprocess(j_tables) if isinstance(j_tables, str) else j_tables\n",
    "\n",
    "    common_tables = i_set.intersection(j_set)\n",
    "    union_tables = i_set.union(j_set)\n",
    "    return len(common_tables) / len(union_tables)\n",
    "\n",
    "def curate_samples(samples: list) -> list[dict]:\n",
    "\n",
    "    dataset = []\n",
    "    for db_id, group_samples in groupby(samples, key=lambda x: x.db_id):\n",
    "        # schema_str = get_schema_str(spider_tables[db_id].db_schema, col_fmt='', skip_type=True, remove_meta=True)\n",
    "        data_dict = defaultdict(list)\n",
    "        for tbls, samples in groupby(group_samples, key=lambda x: x.final.source_tables):\n",
    "            tbls = ', '.join(tbls)\n",
    "            for s in samples:\n",
    "                data_dict[tbls].append(s.final.question)\n",
    "        \n",
    "        for i_tables, j_tables in combinations(data_dict.keys(), 2):\n",
    "            similarity = jaccard_similarity(i_tables, j_tables)\n",
    "            i_data = data_dict[i_tables]\n",
    "            j_data = data_dict[j_tables]\n",
    "            for i, j in product(i_data, j_data):\n",
    "                dataset.append(\n",
    "                    {\n",
    "                        'db_id': db_id,\n",
    "                        'sentence1': i,\n",
    "                        'sentence2': j,\n",
    "                        'label': similarity,\n",
    "                        'tables1': i_tables,\n",
    "                        'tables2': j_tables\n",
    "                    }\n",
    "                )\n",
    "    return dataset\n",
    "\n",
    "train_dataset = curate_samples(train_samples)\n",
    "dev_dataset = curate_samples(dev_samples)\n",
    "test_dataset = curate_samples(test_samples)\n",
    "\n",
    "with (proj_path / 'data' / 'spider_common_interest_train.json').open('w') as f:\n",
    "    json.dump(train_dataset, f, indent=4)\n",
    "\n",
    "with (proj_path / 'data' / 'spider_common_interest_dev.json').open('w') as f:\n",
    "    json.dump(dev_dataset, f, indent=4)\n",
    "\n",
    "with (proj_path / 'data' / 'spider_common_interest_test.json').open('w') as f:\n",
    "    json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137797, 1278, 1800)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(dev_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('json', \n",
    "    data_files={'train': str(proj_path / 'data' / 'spider_common_interest_train.json'), \n",
    "                'validation': str(proj_path / 'data' / 'spider_common_interest_dev.json'),\n",
    "                'test': str(proj_path / 'data' / 'spider_common_interest_test.json')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, losses, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.similarity_functions import SimilarityFunction\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import BatchSamplers, SentenceTransformerTrainingArguments\n",
    "\n",
    "\n",
    "def load_dataset(path: Path):\n",
    "    with path.open('r') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "with (proj_path / 'data' / 'spider_common_interest_train.json').open() as f:\n",
    "    train_dataset = json.load(f)\n",
    "\n",
    "with (proj_path / 'data' / 'spider_common_interest_dev.json').open() as f:\n",
    "    dev_dataset = json.load(f)\n",
    "\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "train_batch_size = 128  # The larger you select this, the better the results (usually). But it requires more GPU memory\n",
    "max_seq_length = 75\n",
    "num_epochs = 1\n",
    "\n",
    "model = SentenceTransformer(model_name)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "loss = losses.CosineSimilarityLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_id = 'hospital_1'\n",
    "train_subsamples = list(filter(lambda x: x.db_id == db_id, train_samples))\n",
    "dev_subsamples = list(filter(lambda x: x.db_id == db_id, dev_samples))\n",
    "\n",
    "s = get_schema_str(spider_tables[db_id].db_schema, col_fmt='', skip_type=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['department', 'physician'] 2\n",
      "['patient', 'appointment'] 4\n",
      "['appointment', 'physician'] 4\n",
      "['department', 'affiliated_with', 'physician'] 2\n",
      "['patient', 'appointment'] 2\n",
      "['patient', 'physician', 'prescribes'] 2\n",
      "['stay', 'patient', 'medication', 'prescribes'] 2\n",
      "['nurse', 'appointment'] 2\n",
      "['patient', 'physician'] 4\n",
      "['block', 'room'] 4\n",
      "['medication', 'prescribes', 'physician'] 4\n",
      "['medication', 'prescribes'] 2\n",
      "['stay', 'patient', 'undergoes'] 2\n",
      "['nurse', 'undergoes'] 2\n",
      "['prescribes', 'physician'] 2\n",
      "['department', 'affiliated_with'] 2\n",
      "['procedures', 'trained_in', 'physician'] 6\n",
      "\n",
      "['procedures', 'trained_in', 'physician'] 2\n",
      "['trained_in', 'procedures', 'physician'] 6\n",
      "['department', 'affiliated_with', 'physician'] 4\n",
      "['patient', 'medication', 'prescribes'] 4\n",
      "['nurse', 'on_call'] 2\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "for k, g in groupby(train_subsamples, key=lambda x: x.final.source_tables):\n",
    "    if len(k) > 1:\n",
    "        print(k, len(list(g)))\n",
    "\n",
    "\n",
    "print()\n",
    "for k, g in groupby(dev_subsamples, key=lambda x: x.final.source_tables):\n",
    "    if len(k) > 1:\n",
    "        print(k, len(list(g)))\n",
    "\n",
    "# TODO: remove alias from table names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss: \n",
    "\n",
    "* MultipleNegativesRankingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli_v3.py\n",
    "# https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#sentence_transformers.losses.MultipleNegativesRankingLoss\n",
    "import tqdm\n",
    "from itertools import groupby, combinations\n",
    "from src.spider_sparc_preprocess import SpiderSample, SparcSample\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, losses, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.similarity_functions import SimilarityFunction\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import BatchSamplers, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def curate_dataset(samples: list[SpiderSample|SparcSample]):\n",
    "    dataset = []\n",
    "    for db_id, group_samples in groupby(samples, key=lambda x: x.db_id):\n",
    "        schema_str = get_schema_str(spider_tables[db_id].db_schema, col_fmt='', skip_type=True, remove_meta=True)\n",
    "        # positive pairs\n",
    "        for tbls, samples in groupby(group_samples, key=lambda x: x.final.source_tables):\n",
    "            questions = [schema_str + '\\n' + s.final.question for s in samples]\n",
    "            pairs = list(combinations(questions, 2))\n",
    "            for p in pairs:\n",
    "                dataset.append(InputExample(texts=p, label=1))\n",
    "        # negative pairs\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_data = curate_dataset(train_samples)\n",
    "dev_data = curate_dataset(dev_samples)\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "train_batch_size = 128  # The larger you select this, the better the results (usually). But it requires more GPU memory\n",
    "max_seq_length = 75\n",
    "num_epochs = 1\n",
    "\n",
    "model = SentenceTransformer(model_name)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "# Use the denoising auto-encoder loss\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# model.fit(\n",
    "#     train_objectives=[(train_dataloader, train_loss)], epochs=1, show_progress_bar=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_acc_evaluator = BinaryClassificationEvaluator.from_input_examples(dev_data, name='dev')\n",
    "results = binary_acc_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "  T1.apt_number\n",
      "FROM Apartments AS T1\n",
      "JOIN View_Unit_Status AS T2\n",
      "  ON T1.apt_id = T2.apt_id\n",
      "WHERE\n",
      "  T2.available_yn = 0\n",
      "INTERSECT\n",
      "SELECT\n",
      "  T1.apt_number\n",
      "FROM Apartments AS T1\n",
      "JOIN View_Unit_Status AS T2\n",
      "  ON T1.apt_id = T2.apt_id\n",
      "WHERE\n",
      "  T2.available_yn = 1\n",
      "['apartments', 'view_unit_status', 'apartments', 'view_unit_status']\n",
      "FROM Apartments AS T1 JOIN View_Unit_Status AS T2 ON T1.apt_id = T2.apt_id WHERE T2.available_yn = 0 FROM Apartments AS T1 JOIN View_Unit_Status AS T2 ON T1.apt_id = T2.apt_id WHERE T2.available_yn = 1\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "import sqlglot.expressions as exp\n",
    "from sqlglot.optimizer import optimize\n",
    "from sqlglot.optimizer.annotate_types import annotate_types\n",
    "from sqlglot.optimizer.canonicalize import canonicalize\n",
    "from sqlglot.optimizer.eliminate_ctes import eliminate_ctes\n",
    "from sqlglot.optimizer.eliminate_joins import eliminate_joins\n",
    "from sqlglot.optimizer.eliminate_subqueries import eliminate_subqueries\n",
    "from sqlglot.optimizer.merge_subqueries import merge_subqueries\n",
    "from sqlglot.optimizer.normalize import normalize\n",
    "from sqlglot.optimizer.optimize_joins import optimize_joins\n",
    "from sqlglot.optimizer.pushdown_predicates import pushdown_predicates\n",
    "from sqlglot.optimizer.pushdown_projections import pushdown_projections\n",
    "from sqlglot.optimizer.qualify import qualify\n",
    "from sqlglot.optimizer.qualify_columns import quote_identifiers\n",
    "from sqlglot.optimizer.simplify import simplify\n",
    "from sqlglot.optimizer.unnest_subqueries import unnest_subqueries\n",
    "from sqlglot.schema import ensure_schema\n",
    "\n",
    "RULES = (\n",
    "    # qualify,\n",
    "    # pushdown_projections,\n",
    "    # normalize,\n",
    "    # unnest_subqueries,\n",
    "    # pushdown_predicates,\n",
    "    # optimize_joins,\n",
    "    # eliminate_subqueries,\n",
    "    # merge_subqueries,\n",
    "    # eliminate_joins,\n",
    "    # eliminate_ctes,\n",
    "    # quote_identifiers,\n",
    "    # annotate_types,\n",
    "    # canonicalize,\n",
    "    # simplify,\n",
    ")\n",
    "{\"sample_id\": 1272, \"db_id\": \"apartment_rentals\", \"final\": \n",
    " {\"question\": \"Show the apartment numbers of apartments with unit status availability of both 0 and 1.\", \n",
    "\"sql\": \"SELECT T1.apt_number FROM Apartments AS T1 JOIN View_Unit_Status AS T2 ON T1.apt_id  =  T2.apt_id WHERE T2.available_yn  =  0 INTERSECT SELECT T1.apt_number FROM Apartments AS T1 JOIN View_Unit_Status AS T2 ON T1.apt_id  =  T2.apt_id WHERE T2.available_yn  =  1\", \"source_tables\": [\"t1\", \"view_unit_status\", \"t1\", \"view_unit_status\", \"apartments\"]}}\n",
    "\n",
    "db_id = 'apartment_rentals'\n",
    "schema = spider_tables[db_id].db_schema\n",
    "sql = \"SELECT T1.apt_number FROM Apartments AS T1 JOIN View_Unit_Status AS T2 ON T1.apt_id  =  T2.apt_id WHERE T2.available_yn  =  0 INTERSECT SELECT T1.apt_number FROM Apartments AS T1 JOIN View_Unit_Status AS T2 ON T1.apt_id  =  T2.apt_id WHERE T2.available_yn  =  1\"\n",
    "sql = sqlglot.parse_one(sql, read='sqlite')\n",
    "print(sql.sql(pretty=True))\n",
    "tbls = [x.this.this.lower() for x in list(sql.find_all(exp.Table))]\n",
    "\n",
    "expression = ' '.join([x.sql() for x in sql.find_all(*[exp.From, exp.Join, exp.Where])])\n",
    "print(tbls)\n",
    "print(expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table and Columns]\n",
      "Physician: EmployeeID, Name, Position, SSN\n",
      "Department: DepartmentID, Name, Head\n",
      "Affiliated_With: Physician, Department, PrimaryAffiliation\n",
      "Procedures: Code, Name, Cost\n",
      "Trained_In: Physician, Treatment, CertificationDate, CertificationExpires\n",
      "Patient: SSN, Name, Address, Phone, InsuranceID, PCP\n",
      "Nurse: EmployeeID, Name, Position, Registered, SSN\n",
      "Appointment: AppointmentID, Patient, PrepNurse, Physician, Start, End, ExaminationRoom\n",
      "Medication: Code, Name, Brand, Description\n",
      "Prescribes: Physician, Patient, Medication, Date, Appointment, Dose\n",
      "Block: BlockFloor, BlockCode\n",
      "Room: RoomNumber, RoomType, BlockFloor, BlockCode, Unavailable\n",
      "On_Call: Nurse, BlockFloor, BlockCode, OnCallStart, OnCallEnd\n",
      "Stay: StayID, Patient, Room, StayStart, StayEnd\n",
      "Undergoes: Patient, Procedures, Stay, DateUndergoes, Physician, AssistingNurse\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-large')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-large')\n",
    "\n",
    "features = tokenizer(['A man is eating pizza', 'A black race car starts up in front of a crowd of people.'], ['A man eats something', 'A man is driving down a lonely road.'],  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    label_mapping = ['contradiction', 'entailment', 'neutral']\n",
    "    labels = [label_mapping[score_max] for score_max in scores.argmax(dim=1)]\n",
    "    print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatabaseModel(db_id='department_management', db_schema={'department': {'Department_ID': 'text', 'Name': 'text', 'Creation': 'text', 'Ranking': 'text', 'Budget_in_Billions': 'text', 'Num_Employees': 'text'}, 'head': {'head_ID': 'number', 'name': 'number', 'born_state': 'number', 'age': 'number'}, 'management': {'department_ID': 'text', 'head_ID': 'text', 'temporary_acting': 'text'}}, col_explanation={'department': {'Department_ID': 'Unique identifier for each department.', 'Name': 'Name of the department.', 'Creation': 'Date when the department was established.', 'Ranking': 'Ranking of the department based on performance.', 'Budget_in_Billions': 'Annual budget allocated to the department in billions.', 'Num_Employees': 'Total number of employees in the department.'}, 'head': {'head_ID': 'Unique identifier for each department head.', 'name': 'Name of the department head.', 'born_state': 'State where the department head was born.', 'age': 'Age of the department head.'}, 'management': {'department_ID': 'Identifier for the department managed.', 'head_ID': 'Identifier for the head of the department.', 'temporary_acting': 'Indicates if the head is temporarily acting in the position.'}}, foreign_keys=['management.head_ID = head.head_ID', 'management.department_ID = department.Department_ID'], primary_keys=['department.Department_ID', 'head.head_ID', 'management.department_ID'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spider_tables[train_samples[0].db_id].db_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "import sqlglot.expressions as exp\n",
    "from sqlglot.optimizer import optimize\n",
    "\n",
    "def extract_used_table(sql: str, schema: dict) -> list[str]:\n",
    "    sql = optimize(sqlglot.parse_one(sql, read='sqlite'), schema=schema)\n",
    "    tbls = [x.this.this for x in list(sql.find_all(exp.Table))]\n",
    "    return tbls\n",
    "\n",
    "extract_used_table(train_samples[0].final.sql, spider_tables[train_samples[0].db_id].db_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparc_path = proj_path / 'data' / 'sparc'\n",
    "\n",
    "tables, train_data, dev_data = load_spider_sparc_data(sparc_path)\n",
    "\n",
    "with (proj_path / 'db_data' / 'description.json').open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "\n",
    "sparc_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "# filter samples by count, must have at least 5 samples\n",
    "all_data = filter_samples_by_count_sparc(train_data+dev_data, n=5)\n",
    "# process samples -> {db_id: list of samples}\n",
    "sparc_samples = process_samples_sparc(all_data, sparc_tables)\n",
    "# change train/dev by sample\n",
    "train_samples, dev_samples = split_train_dev(sparc_samples, ratio=0.8)\n",
    "\n",
    "print(f'Number of train: {len(train_samples)} | Number of dev: {len(dev_samples)}')\n",
    "\n",
    "db_id = 'hospital_1'\n",
    "db_file = str(sparc_path / 'database' / db_id / f'{db_id}.sqlite')\n",
    "database = SqliteDatabase(db_file, foreign_keys=sparc_tables[db_id].foreign_keys)\n",
    "print(database.table_cols.keys())\n",
    "database.execute('SELECT * FROM Department LIMIT 5;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0-Question] How many employees does each department have?\n",
      "[0-SQL]: SELECT count(departmentID) FROM department GROUP BY departmentID\n",
      "[1-Question] Which department has the smallest number of employees?\n",
      "[1-SQL]: SELECT * FROM department GROUP BY departmentID ORDER BY count(departmentID) LIMIT 1;\n",
      "[2-Question] Tell me the name and position of the head of this department.\n",
      "[2-SQL]: SELECT T2.name ,  T2.position FROM department AS T1 JOIN physician AS T2 ON T1.head  =  T2.EmployeeID GROUP BY departmentID ORDER BY count(departmentID) LIMIT 1; \n",
      "\n",
      "[Final]\n",
      "Question: Find the name and position of the head of the department with the least employees.\n",
      "SQL: SELECT T2.name ,  T2.position FROM department AS T1 JOIN physician AS T2 ON T1.head  =  T2.EmployeeID GROUP BY departmentID ORDER BY count(departmentID) LIMIT 1;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.spider_sparc_preprocess import SparcSample, QuestionSQL\n",
    "\n",
    "def format_interactions(interactions: list[QuestionSQL]) -> str:\n",
    "    workload = ''\n",
    "    for i, interaction in enumerate(interactions):\n",
    "        workload += f'[{i}-Question] {interaction.question}\\n[{i}-SQL]: {interaction.sql}\\n'\n",
    "    return workload.strip()\n",
    "\n",
    "with (proj_path / 'db_data' / 'sparc_description.json').open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "\n",
    "db_id = 'hospital_1'\n",
    "train_subsamples = list(filter(lambda x: x.db_id == db_id, train_samples))\n",
    "dev_subsamples = list(filter(lambda x: x.db_id == db_id, dev_samples))\n",
    "\n",
    "table = sparc_tables[db_id]\n",
    "col_explanation = all_descriptions[db_id]\n",
    "# create schema string\n",
    "schema_str = get_schema_str(\n",
    "    schema=table.db_schema, \n",
    "    foreign_keys=table.foreign_keys,\n",
    "    primary_keys=table.primary_keys,\n",
    "    col_explanation=col_explanation\n",
    ")  \n",
    "database = SqliteDatabase(str(sparc_path / 'database' / db_id / f'{db_id}.sqlite'), foreign_keys=table.foreign_keys)\n",
    "\n",
    "data = train_samples[2]\n",
    "workload = format_interactions(data.interactions)\n",
    "print(workload, '\\n')\n",
    "print(f'[Final]\\nQuestion: {data.final.question}\\nSQL: {data.final.sql}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(departmentID)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(departmentID)\n",
       "0                    1\n",
       "1                    1\n",
       "2                    1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.execute(data.interactions[0].sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepartmentID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>General Medicine</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DepartmentID              Name  Head\n",
       "0             1  General Medicine     4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.execute(data.interactions[1].sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Percival Cox</td>\n",
       "      <td>Senior Attending Physician</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name                    Position\n",
       "0  Percival Cox  Senior Attending Physician"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.execute(data.interactions[2].sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Percival Cox</td>\n",
       "      <td>Senior Attending Physician</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name                    Position\n",
       "0  Percival Cox  Senior Attending Physician"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.execute(data.final.sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Common table extraction\n",
    "\n",
    "* find the common table used in the question-sql workloads: All joined tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train workloads: 25\n",
      "# of used tables: 22\n",
      "-----------------\n",
      "appointment: 9\n",
      "department: 7\n",
      "physician: 6\n",
      "stay: 6\n",
      "physician,patient: 6\n",
      "block,room: 4\n",
      "room: 4\n",
      "medication: 3\n",
      "appointment,patient: 2\n",
      "appointment,physician: 2\n",
      "physician,affiliated_with,department: 2\n",
      "nurse,appointment: 2\n",
      "prescribes,medication: 2\n",
      "physician,prescribes,medication: 2\n",
      "department,physician: 1\n",
      "physician,appointment,physician: 1\n",
      "patient,appointment: 1\n",
      "prescribes,physician: 1\n",
      "patient,prescribes,physician: 1\n",
      "stay,patient,prescribes: 1\n",
      "stay,patient,prescribes,medication: 1\n",
      "medication,prescribes: 1\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "import sqlglot.expressions as exp\n",
    "from sqlglot.diff import Keep\n",
    "from sqlglot.optimizer import optimize\n",
    "from collections import Counter\n",
    "\n",
    "def extract_table_expression(x: QuestionSQL, schema: dict) -> str:\n",
    "    sql = optimize(sqlglot.parse_one(x.sql, read='sqlite'), schema=schema)\n",
    "    tbls = [x.this.this for x in list(sql.find_all(exp.Table))]\n",
    "    expression = ' '.join([x.sql() for x in sql.find_all(*[exp.From, exp.Join])])\n",
    "    return ','.join(tbls), expression\n",
    "\n",
    "def get_sources(data: SparcSample, schema: dict) -> list[tuple[str, list[str]]]:\n",
    "    sources = []\n",
    "    for x in data.interactions:\n",
    "        tbls, expression = extract_table_expression(x, schema)\n",
    "        sources.append({'question': x.question, 'table': tbls, 'expression': expression})\n",
    "    return sources\n",
    "\n",
    "db_id = 'hospital_1'\n",
    "train_subsamples = list(filter(lambda x: x.db_id == db_id, train_samples))\n",
    "dev_subsamples = list(filter(lambda x: x.db_id == db_id, dev_samples))\n",
    "table = sparc_tables[db_id]\n",
    "database = SqliteDatabase(str(sparc_path / 'database' / db_id / f'{db_id}.sqlite'), foreign_keys=table.foreign_keys)\n",
    "\n",
    "used_tables = Counter()\n",
    "for data in train_subsamples:\n",
    "    sources = get_sources(data, table.db_schema)\n",
    "    used = [x['table'] for x in sources]\n",
    "    used_tables.update(used)\n",
    "\n",
    "print(f'# of train workloads: {len(train_subsamples)}')\n",
    "print(f'# of used tables: {len(used_tables)}\\n-----------------')\n",
    "for k, v in used_tables.most_common():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a nested query\n",
    "sql = \"\"\"\n",
    "SELECT * FROM (\n",
    "    SELECT * FROM Department\n",
    ") AS A\n",
    "WHERE A.department_id = 1;\n",
    "\"\"\"\n",
    "\n",
    "sql = sqlglot.parse_one(sql, read='sqlite')\n",
    "tbls = [x.this.this for x in list(sql.find_all(exp.Table))]\n",
    "expression = ' '.join([x.sql() for x in sql.find_all(*[exp.From, exp.Join])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract Term - Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:21<00:00,  3.27s/it]\n"
     ]
    }
   ],
   "source": [
    "class TermExpressions(BaseModel):\n",
    "    rationale: str = Field(description='The reasoning behind the decision.')\n",
    "    index: int = Field(description='Index of the question-sql pair.')\n",
    "    term: str = Field(description='A declarative form of the natural language term.')\n",
    "    expression: str = Field(description='SQL expression that refers to the term.')\n",
    "\n",
    "class Response(BaseModel):\n",
    "    output: list[TermExpressions]\n",
    "    \n",
    "template = '''### Task\n",
    "You are tasked with identifying the partial term - partial expression relationship to represent the common interest query.\n",
    "You will be proveded several pairs of question and SQL with index. Do not extract the FROM and JOIN clauses.\n",
    "There could be multiple terms and expressions in a single question-SQL pair.\n",
    "\n",
    "### Formatting\n",
    "Your output should be of the following list of JSON format:\n",
    "[\n",
    "    {{\n",
    "        \"rationale\": <str: the reasoning behind decision>,\n",
    "        \"index: <int: the index of the question-sql pair>,\n",
    "        \"term\": <str: a partial natural language term>,\n",
    "        \"expression\" : <str: a partial SQL expression that refer to the term>,\n",
    "    }}, ...\n",
    "]\n",
    "\n",
    "\n",
    "### Output\n",
    "<QUESTION-SQL>:\\n{workload}\n",
    "<OUTPUT>: \n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['workload']\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "model = model_openai.with_structured_output(Response)\n",
    "chain = (prompt | model)\n",
    "\n",
    "all_term_expression = defaultdict(list)\n",
    "for data in tqdm(train_subsamples, total=len(train_subsamples)):\n",
    "    workload = format_interactions(data.interactions)\n",
    "    term_expression = chain.invoke(input={'workload': workload}).output\n",
    "    all_term_expression[data.sample_id] = term_expression\n",
    "    # for x in data.interactions:\n",
    "    #     input_data = {'question': x.question, 'sql': x.sql}\n",
    "    #     term_expression = chain.invoke(input=input_data).output\n",
    "    #     tbls, _ = extract_table_expression(x, table.db_schema)\n",
    "    #     all_term_expression[tbls].append(term_expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sample 00-0]: employees - count(departmentID)\n",
      "[Sample 00-0]: department - GROUP BY departmentID\n",
      "[Sample 00-1]: most employees - ORDER BY count(departmentID) DESC LIMIT 1\n",
      "[Sample 00-1]: department name - name\n",
      "[Sample 00-1]: department - GROUP BY departmentID\n",
      "[Sample 01-0]: employees - count(departmentID)\n",
      "[Sample 01-0]: department - GROUP BY departmentID\n",
      "[Sample 01-1]: least employees - ORDER BY count(departmentID) LIMIT 1\n",
      "[Sample 01-1]: department - GROUP BY departmentID\n",
      "[Sample 01-2]: head - head\n",
      "[Sample 01-2]: department - GROUP BY departmentID\n",
      "[Sample 02-0]: employees - count(departmentID)\n",
      "[Sample 02-0]: department - GROUP BY departmentID\n",
      "[Sample 02-1]: smallest number of employees - ORDER BY count(departmentID) LIMIT 1\n",
      "[Sample 02-1]: department - GROUP BY departmentID\n",
      "[Sample 02-2]: name - T2.name\n",
      "[Sample 02-2]: position - T2.position\n",
      "[Sample 02-2]: department - GROUP BY departmentID\n",
      "[Sample 03-0]: patient id - patient\n",
      "[Sample 03-1]: names of patients - name\n",
      "[Sample 03-1]: appointments - appointment AS T1\n",
      "[Sample 04-0]: appointments - count(*)\n",
      "[Sample 04-0]: patient - GROUP BY patient\n",
      "[Sample 04-1]: more than one appointment - HAVING count(*) > 1\n",
      "[Sample 04-1]: patient - GROUP BY patient\n",
      "[Sample 04-2]: name - name\n",
      "[Sample 04-2]: phone number - phone\n",
      "[Sample 04-2]: patients - GROUP BY T1.patient\n",
      "[Sample 05-0]: start date - START\n",
      "[Sample 05-1]: appointments - ORDER BY START DESC\n",
      "[Sample 05-2]: most recent starting date - ORDER BY START DESC LIMIT 1\n",
      "[Sample 05-2]: appointment id - appointmentid\n",
      "[Sample 06-0]: ids of the physicians - Physician\n",
      "[Sample 06-1]: names - T2.name\n",
      "[Sample 06-1]: physicians - T2.EmployeeID\n",
      "[Sample 07-0]: physician names - SELECT name\n",
      "[Sample 07-1]: physicians who took appointments - SELECT T2.name FROM appointment AS T1 JOIN physician AS T2 ON T1.Physician = T2.EmployeeID\n",
      "[Sample 07-2]: physicians who never took any appointment - SELECT name FROM physician EXCEPT SELECT T2.name FROM appointment AS T1 JOIN physician AS T2 ON T1.Physician = T2.EmployeeID\n",
      "[Sample 08-0]: physician - SELECT name\n",
      "[Sample 08-1]: name - T3.name\n",
      "[Sample 08-1]: department - T2.department = T3.DepartmentID\n",
      "[Sample 08-2]: physician - T1.name\n",
      "[Sample 08-2]: department - T2.department = T3.DepartmentID\n",
      "[Sample 09-0]: appointments - SELECT * FROM appointment\n",
      "[Sample 09-0]: start date - ORDER BY START DESC\n",
      "[Sample 09-1]: most recent start date - ORDER BY START DESC LIMIT 1\n",
      "[Sample 09-1]: appointment - SELECT * FROM appointment\n",
      "[Sample 09-2]: name of the patient - T1.name\n",
      "[Sample 09-2]: appointment - JOIN appointment AS T2\n",
      "[Sample 09-2]: start - ORDER BY T2.start DESC\n",
      "[Sample 10-0]: stays - * FROM stay\n",
      "[Sample 10-0]: room 112 - room = 112\n",
      "[Sample 10-1]: patients - count(patient)\n",
      "[Sample 10-1]: room 112 - room = 112\n",
      "[Sample 11-0]: employee id - employeeid\n",
      "[Sample 11-0]: physician - physician\n",
      "[Sample 11-1]: prescriptions - prescribes\n",
      "[Sample 11-1]: physician - T2.employeeid\n",
      "[Sample 11-2]: patients - count(T1.SSN)\n",
      "[Sample 11-2]: prescriptions - T2.patient\n",
      "[Sample 11-2]: physician - T3.employeeid\n",
      "[Sample 12-0]: patient - patient\n",
      "[Sample 12-0]: room 111 - WHERE room = 111\n",
      "[Sample 12-1]: medication - T3.Medication\n",
      "[Sample 12-1]: patient - T2.SSN\n",
      "[Sample 12-2]: name of the medication - T4.name\n",
      "[Sample 12-2]: room 111 - WHERE room = 111\n",
      "[Sample 13-0]: patients - patient\n",
      "[Sample 13-0]: room 111 - room = 111\n",
      "[Sample 13-1]: stay start date - ORDER BY staystart DESC\n",
      "[Sample 13-1]: patients - patient\n",
      "[Sample 13-2]: most recently - ORDER BY staystart DESC LIMIT 1\n",
      "[Sample 13-2]: patients - patient\n",
      "[Sample 13-2]: room 111 - room = 111\n",
      "[Sample 14-0]: appointments - count(*)\n",
      "[Sample 14-0]: nurse - GROUP BY T1.employeeid\n",
      "[Sample 14-1]: most appointments - ORDER BY count(*) DESC LIMIT 1\n",
      "[Sample 14-1]: nurse - GROUP BY T1.employeeid\n",
      "[Sample 14-1]: name - T1.name\n",
      "[Sample 15-0]: patients - GROUP BY T1.employeeid\n",
      "[Sample 15-0]: physician - GROUP BY T1.employeeid\n",
      "[Sample 15-1]: patients - count(*)\n",
      "[Sample 15-1]: physician - GROUP BY T1.employeeid\n",
      "[Sample 15-2]: name - T1.name\n",
      "[Sample 15-2]: patients - count(*)\n",
      "[Sample 15-2]: physician - GROUP BY T1.employeeid\n",
      "[Sample 16-0]: patients - count(*)\n",
      "[Sample 16-0]: physician - GROUP BY T1.employeeid\n",
      "[Sample 16-1]: more than one - HAVING count(*) > 1\n",
      "[Sample 16-1]: physicians - GROUP BY T1.employeeid\n",
      "[Sample 16-2]: name - T1.name\n",
      "[Sample 16-2]: physicians - GROUP BY T1.employeeid\n",
      "[Sample 17-0]: block floor - T1.blockfloor\n",
      "[Sample 17-0]: room - T2.blockfloor\n",
      "[Sample 17-1]: rooms - count(*)\n",
      "[Sample 17-1]: block floor - T1.blockfloor\n",
      "[Sample 18-0]: room - T1.blockcode\n",
      "[Sample 18-0]: block code - T1.blockcode\n",
      "[Sample 18-1]: rooms - count(*)\n",
      "[Sample 18-1]: block code - T1.blockcode\n",
      "[Sample 19-0]: rooms - SELECT * FROM room\n",
      "[Sample 19-0]: available - WHERE unavailable = 0\n",
      "[Sample 19-1]: block codes - SELECT DISTINCT blockcode\n",
      "[Sample 19-1]: rooms available - WHERE unavailable = 0\n",
      "[Sample 20-0]: room types - roomtype\n",
      "[Sample 20-1]: distinct room types - count(DISTINCT roomtype)\n",
      "[Sample 21-0]: medication - WHERE name = 'Thesisin'\n",
      "[Sample 21-0]: code - SELECT code\n",
      "[Sample 21-1]: physicians - SELECT DISTINCT T1.physician\n",
      "[Sample 21-1]: medication - WHERE T2.name = 'Thesisin'\n",
      "[Sample 21-2]: names - SELECT DISTINCT T1.name\n",
      "[Sample 21-2]: medication - WHERE T3.name = 'Thesisin'\n",
      "[Sample 22-0]: medication - WHERE Brand = 'X'\n",
      "[Sample 22-0]: Brand X - WHERE Brand = 'X'\n",
      "[Sample 22-1]: physicians - SELECT DISTINCT T1.physician\n",
      "[Sample 22-1]: medication - WHERE T2.Brand = 'X'\n",
      "[Sample 22-2]: name - T1.name\n",
      "[Sample 22-2]: position - T1.position\n",
      "[Sample 22-2]: physician - JOIN physician AS T1\n",
      "[Sample 23-0]: medications - GROUP BY brand\n",
      "[Sample 23-0]: brand - GROUP BY brand\n",
      "[Sample 23-1]: medications - count(*)\n",
      "[Sample 23-1]: brand - GROUP BY T1.brand\n",
      "[Sample 24-0]: position title - POSITION\n",
      "[Sample 24-0]: physicians - FROM physician\n",
      "[Sample 24-1]: physicians - FROM physician\n",
      "[Sample 24-1]: senior - POSITION LIKE '%senior%'\n",
      "[Sample 24-2]: names - name\n",
      "[Sample 24-2]: physicians - FROM physician\n"
     ]
    }
   ],
   "source": [
    "for sample_id, term_exps in all_term_expression.items():\n",
    "    for x in term_exps:\n",
    "        print(f'[Sample {sample_id:02d}-{x.index}]: {x.term} - {x.expression}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table and Columns]\n",
      "Table Name: Physician\n",
      "  - 'EmployeeID'(text): Unique identifier for each physician.\n",
      "  - 'Name'(text): Full name of the physician.\n",
      "  - 'Position'(text): Job title or role of the physician.\n",
      "  - 'SSN'(text): Social Security Number of the physician.\n",
      "Table Name: Department\n",
      "  - 'Depart\n"
     ]
    }
   ],
   "source": [
    "with (proj_path / 'db_data' / 'sparc_description.json').open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "\n",
    "print(get_schema_str(\n",
    "    schema=sparc_tables['hospital_1'].db_schema, \n",
    "    col_explanation=all_descriptions['hospital_1'])[:300]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Access Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13216584, 100000001, 101, 1, '2008-04-24 10:00', '2008-04-24 11:00', 'A')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype_functions = {\n",
    "    'numeric': pd.to_numeric,\n",
    "    'datetime': pd.to_datetime\n",
    "}\n",
    "\n",
    "def null_percentage(s: pd.Series) -> float:\n",
    "    return s.isnull().sum() / len(s)\n",
    "\n",
    "column_info = {}\n",
    "for col in df.columns:\n",
    "    # dtype\n",
    "    null_index = df[col].isnull()\n",
    "    for logical_type in ['numeric', 'datetime', 'text']:\n",
    "        if logical_type in ['numeric', 'datetime']:\n",
    "            try:\n",
    "                df.loc[~null_index, col] = dtype_functions[logical_type](df.loc[~null_index, col], errors='raise')\n",
    "                attribute_type = 'ordinal'\n",
    "                break\n",
    "            except ValueError as e:\n",
    "                # print(f'-- {col}: {logical_type} {e}')\n",
    "                continue\n",
    "            except TypeError as e:\n",
    "                # print(f'-- {col}: {logical_type} {e}')\n",
    "                continue\n",
    "        else:\n",
    "            attribute_type = 'nominal'\n",
    "            break\n",
    "    print(f'{col}: {logical_type} {attribute_type}')\n",
    "    # unique values\n",
    "    unique_values = df[col].unique()\n",
    "    # min, max\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    # null percentage\n",
    "    null_percent = null_percentage(df[col])\n",
    "\n",
    "    column_info[col] = {\n",
    "        'logical_type': logical_type,\n",
    "        'attribute_type': attribute_type,\n",
    "        'unique_values': unique_values,\n",
    "        'min': min_val,\n",
    "        'max': max_val,\n",
    "        'null_percentage': null_percent\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
