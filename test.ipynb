{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from src.db_utils import get_schema_str\n",
    "from src.database import SqliteDatabase\n",
    "from src.spider_sparc_preprocess import (\n",
    "    load_spider_sparc_data,\n",
    "    process_all_tables, \n",
    "    load_samples_spider,\n",
    "    load_samples_sparc,\n",
    "    filter_samples_by_count_sparc,\n",
    "    filter_samples_by_count_spider, \n",
    "    process_samples_sparc,\n",
    "    process_samples_spider, \n",
    "    split_train_dev_test,\n",
    "    save_samples_spider\n",
    ")\n",
    "\n",
    "proj_path = Path('.').resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8023/8023 [00:01<00:00, 4236.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train: 6369 | Number of dev: 747 | Number of test: 907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # spider dataset\n",
    "# spider_path = proj_path / 'data' / 'spider'\n",
    "# tables, train_data, dev_data = load_spider_sparc_data(spider_path)\n",
    "\n",
    "# with (proj_path / 'data' / 'description.json').open() as f:\n",
    "#     all_descriptions = json.load(f)\n",
    "# spider_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "# all_data = filter_samples_by_count_spider(train_data+dev_data, n=10)\n",
    "# # process samples -> {db_id: list of samples}\n",
    "# # skip = [3146, 4690, 4691]\n",
    "# spider_samples = process_samples_spider(all_data, spider_tables, skip=[])\n",
    "# # change train/dev by sample\n",
    "# train_samples, dev_samples, test_samples = split_train_dev_test(spider_samples, train_ratio=0.8, dev_ratio=0.1)\n",
    "# print(f'Number of train: {len(train_samples)} | Number of dev: {len(dev_samples)} | Number of test: {len(test_samples)}')\n",
    "\n",
    "# save_samples_spider(train_samples, proj_path / 'data' / 'spider_train.json')\n",
    "# save_samples_spider(dev_samples, proj_path / 'data' / 'spider_dev.json')\n",
    "# save_samples_spider(test_samples, proj_path / 'data' / 'spider_test.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train: 6369 | Number of dev: 747 | Number of test: 907\n"
     ]
    }
   ],
   "source": [
    "with (proj_path / 'data' / 'spider' / f'tables.json').open() as f:\n",
    "    tables = json.load(f)\n",
    "\n",
    "with (proj_path / 'data' / 'description.json').open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "spider_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "\n",
    "train_samples = load_samples_spider(proj_path / 'data' / 'spider_train.json')\n",
    "dev_samples = load_samples_spider(proj_path / 'data' / 'spider_dev.json')\n",
    "test_samples = load_samples_spider(proj_path / 'data' / 'spider_test.json')\n",
    "print(f'Number of train: {len(train_samples)} | Number of dev: {len(dev_samples)} | Number of test: {len(test_samples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 32, 2: 30, 3: 16, 4: 2})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# how many multiple tables joined for each database?\n",
    "counter = defaultdict(Counter)\n",
    "for s in train_samples:\n",
    "    counter[s.db_id].update([len(s.final.source_tables)])\n",
    "\n",
    "counter['hospital_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Interest Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Dataset for training a cross-encoder\n",
    "\n",
    "* if two questions share the same source table, they are considered as a co-related pair\n",
    "* using jaccard similarity to label the common interest: \n",
    "    * e.g., $q_1$ has three tables $t_1, t_2, t_3$, $q_2$ has two tables $t_1, t_2$, then the jaccard similarity is $2/3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import groupby, combinations, product\n",
    "from src.spider_sparc_preprocess import SpiderSample, SparcSample\n",
    "\n",
    "def jaccard_similarity(i_tables: str|set, j_tables: str|set):\n",
    "    def preprocess(tables: str):\n",
    "        return set([t.strip() for t in tables.split(',')])\n",
    "    # Get the number of common tables\n",
    "    i_set = preprocess(i_tables) if isinstance(i_tables, str) else i_tables\n",
    "    j_set = preprocess(j_tables) if isinstance(j_tables, str) else j_tables\n",
    "\n",
    "    common_tables = i_set.intersection(j_set)\n",
    "    union_tables = i_set.union(j_set)\n",
    "    return len(common_tables) / len(union_tables)\n",
    "\n",
    "def curate_samples(samples: list) -> list[dict]:\n",
    "\n",
    "    dataset = []\n",
    "    for db_id, group_samples in groupby(samples, key=lambda x: x.db_id):\n",
    "        # schema_str = get_schema_str(spider_tables[db_id].db_schema, col_fmt='', skip_type=True, remove_meta=True)\n",
    "        data_dict = defaultdict(list)\n",
    "        for tbls, samples in groupby(group_samples, key=lambda x: x.final.source_tables):\n",
    "            tbls = ', '.join(tbls)\n",
    "            for s in samples:\n",
    "                data_dict[tbls].append(s.final.question)\n",
    "        \n",
    "        for i_tables, j_tables in combinations(data_dict.keys(), 2):\n",
    "            similarity = jaccard_similarity(i_tables, j_tables)\n",
    "            i_data = data_dict[i_tables]\n",
    "            j_data = data_dict[j_tables]\n",
    "            for i, j in product(i_data, j_data):\n",
    "                dataset.append(\n",
    "                    {\n",
    "                        'db_id': db_id,\n",
    "                        'sentence1': i,\n",
    "                        'sentence2': j,\n",
    "                        'label': similarity,\n",
    "                        'tables1': i_tables,\n",
    "                        'tables2': j_tables\n",
    "                    }\n",
    "                )\n",
    "    return dataset\n",
    "\n",
    "train_dataset = curate_samples(train_samples)\n",
    "dev_dataset = curate_samples(dev_samples)\n",
    "test_dataset = curate_samples(test_samples)\n",
    "\n",
    "with (proj_path / 'data' / 'spider_common_interest_train.json').open('w') as f:\n",
    "    json.dump(train_dataset, f, indent=4)\n",
    "\n",
    "with (proj_path / 'data' / 'spider_common_interest_dev.json').open('w') as f:\n",
    "    json.dump(dev_dataset, f, indent=4)\n",
    "\n",
    "with (proj_path / 'data' / 'spider_common_interest_test.json').open('w') as f:\n",
    "    json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137797, 1278, 1800)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(dev_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b477341266ca49debae7b3cd21a9bfeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/datasets/packaged_modules/json/json.py:137\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mpaj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReadOptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/pyarrow/_json.pyx:308\u001b[0m, in \u001b[0;36mpyarrow._json.read_json\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: JSON parse error: Column() changed from object to array in row 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/datasets/builder.py:1853\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1852\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1853\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/datasets/packaged_modules/json/json.py:160\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    158\u001b[0m         file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoding_errors\n\u001b[1;32m    159\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 160\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_read_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/datasets/packaged_modules/json/json.py:38\u001b[0m, in \u001b[0;36mpandas_read_json\u001b[0;34m(path_or_buf, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:1027\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m-> 1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtypes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/pandas/core/generic.py:7031\u001b[0m, in \u001b[0;36mNDFrame.convert_dtypes\u001b[0;34m(self, infer_objects, convert_string, convert_integer, convert_boolean, convert_floating, dtype_backend)\u001b[0m\n\u001b[1;32m   7030\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m-> 7031\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[union-attr]\u001b[39;49;00m\n\u001b[1;32m   7032\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_integer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_integer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_boolean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_boolean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_floating\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_floating\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7038\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7039\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_mgr, axes\u001b[38;5;241m=\u001b[39mnew_mgr\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:455\u001b[0m, in \u001b[0;36mBaseBlockManager.convert_dtypes\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconvert_dtypes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m     applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:694\u001b[0m, in \u001b[0;36mBlock.convert_dtypes\u001b[0;34m(self, copy, using_cow, infer_objects, convert_string, convert_integer, convert_boolean, convert_floating, dtype_backend)\u001b[0m\n\u001b[1;32m    693\u001b[0m sub_blks \u001b[38;5;241m=\u001b[39m [blk] \u001b[38;5;28;01mif\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_split()\n\u001b[0;32m--> 694\u001b[0m dtypes \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dtypes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_integer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_boolean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_floating\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msub_blks\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mfor\u001b[39;00m dtype \u001b[38;5;129;01min\u001b[39;00m dtypes):\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;66;03m# Avoid block splitting if no dtype changes\u001b[39;00m\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:695\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    693\u001b[0m sub_blks \u001b[38;5;241m=\u001b[39m [blk] \u001b[38;5;28;01mif\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_split()\n\u001b[1;32m    694\u001b[0m dtypes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 695\u001b[0m     \u001b[43mconvert_dtypes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_integer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_boolean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_floating\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m sub_blks\n\u001b[1;32m    705\u001b[0m ]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mfor\u001b[39;00m dtype \u001b[38;5;129;01min\u001b[39;00m dtypes):\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;66;03m# Avoid block splitting if no dtype changes\u001b[39;00m\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:1145\u001b[0m, in \u001b[0;36mconvert_dtypes\u001b[0;34m(input_array, convert_string, convert_integer, convert_boolean, convert_floating, infer_objects, dtype_backend)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1145\u001b[0m     pa_type \u001b[38;5;241m=\u001b[39m \u001b[43mto_pyarrow_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py:221\u001b[0m, in \u001b[0;36mto_pyarrow_type\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03mConvert dtype to a pyarrow type instance.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[43mArrowDtype\u001b[49m):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mpyarrow_dtype\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ArrowDtype' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mproj_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspider_common_interest_train.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mproj_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspider_common_interest_dev.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mproj_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspider_common_interest_test.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/datasets/load.py:2096\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2096\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2106\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2107\u001b[0m )\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/datasets/builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 924\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/datasets/builder.py:999\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    995\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1006\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/datasets/builder.py:1740\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1738\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1740\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "File \u001b[0;32m~/code/BusinessLake/.venv/lib/python3.11/site-packages/datasets/builder.py:1896\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1894\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[1;32m   1895\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1896\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('json', \n",
    "    data_files={'train': str(proj_path / 'data' / 'spider_common_interest_train.json'), \n",
    "                'validation': str(proj_path / 'data' / 'spider_common_interest_dev.json'),\n",
    "                'test': str(proj_path / 'data' / 'spider_common_interest_test.json')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, losses, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.similarity_functions import SimilarityFunction\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import BatchSamplers, SentenceTransformerTrainingArguments\n",
    "\n",
    "\n",
    "def load_dataset(path: Path):\n",
    "    with path.open('r') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "with (proj_path / 'data' / 'spider_common_interest_train.json').open() as f:\n",
    "    train_dataset = json.load(f)\n",
    "\n",
    "with (proj_path / 'data' / 'spider_common_interest_dev.json').open() as f:\n",
    "    dev_dataset = json.load(f)\n",
    "\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "train_batch_size = 128  # The larger you select this, the better the results (usually). But it requires more GPU memory\n",
    "max_seq_length = 75\n",
    "num_epochs = 1\n",
    "\n",
    "model = SentenceTransformer(model_name)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "loss = losses.CosineSimilarityLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_id = 'hospital_1'\n",
    "train_subsamples = list(filter(lambda x: x.db_id == db_id, train_samples))\n",
    "dev_subsamples = list(filter(lambda x: x.db_id == db_id, dev_samples))\n",
    "\n",
    "s = get_schema_str(spider_tables[db_id].db_schema, col_fmt='', skip_type=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['department', 'physician'] 2\n",
      "['patient', 'appointment'] 4\n",
      "['appointment', 'physician'] 4\n",
      "['department', 'affiliated_with', 'physician'] 2\n",
      "['patient', 'appointment'] 2\n",
      "['patient', 'physician', 'prescribes'] 2\n",
      "['stay', 'patient', 'medication', 'prescribes'] 2\n",
      "['nurse', 'appointment'] 2\n",
      "['patient', 'physician'] 4\n",
      "['block', 'room'] 4\n",
      "['medication', 'prescribes', 'physician'] 4\n",
      "['medication', 'prescribes'] 2\n",
      "['stay', 'patient', 'undergoes'] 2\n",
      "['nurse', 'undergoes'] 2\n",
      "['prescribes', 'physician'] 2\n",
      "['department', 'affiliated_with'] 2\n",
      "['procedures', 'trained_in', 'physician'] 6\n",
      "\n",
      "['procedures', 'trained_in', 'physician'] 2\n",
      "['trained_in', 'procedures', 'physician'] 6\n",
      "['department', 'affiliated_with', 'physician'] 4\n",
      "['patient', 'medication', 'prescribes'] 4\n",
      "['nurse', 'on_call'] 2\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "for k, g in groupby(train_subsamples, key=lambda x: x.final.source_tables):\n",
    "    if len(k) > 1:\n",
    "        print(k, len(list(g)))\n",
    "\n",
    "\n",
    "print()\n",
    "for k, g in groupby(dev_subsamples, key=lambda x: x.final.source_tables):\n",
    "    if len(k) > 1:\n",
    "        print(k, len(list(g)))\n",
    "\n",
    "# TODO: remove alias from table names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss: \n",
    "\n",
    "* MultipleNegativesRankingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli_v3.py\n",
    "# https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#sentence_transformers.losses.MultipleNegativesRankingLoss\n",
    "import tqdm\n",
    "from itertools import groupby, combinations\n",
    "from src.spider_sparc_preprocess import SpiderSample, SparcSample\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, losses, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.similarity_functions import SimilarityFunction\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import BatchSamplers, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def curate_dataset(samples: list[SpiderSample|SparcSample]):\n",
    "    dataset = []\n",
    "    for db_id, group_samples in groupby(samples, key=lambda x: x.db_id):\n",
    "        schema_str = get_schema_str(spider_tables[db_id].db_schema, col_fmt='', skip_type=True, remove_meta=True)\n",
    "        # positive pairs\n",
    "        for tbls, samples in groupby(group_samples, key=lambda x: x.final.source_tables):\n",
    "            questions = [schema_str + '\\n' + s.final.question for s in samples]\n",
    "            pairs = list(combinations(questions, 2))\n",
    "            for p in pairs:\n",
    "                dataset.append(InputExample(texts=p, label=1))\n",
    "        # negative pairs\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_data = curate_dataset(train_samples)\n",
    "dev_data = curate_dataset(dev_samples)\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "train_batch_size = 128  # The larger you select this, the better the results (usually). But it requires more GPU memory\n",
    "max_seq_length = 75\n",
    "num_epochs = 1\n",
    "\n",
    "model = SentenceTransformer(model_name)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "# Use the denoising auto-encoder loss\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# model.fit(\n",
    "#     train_objectives=[(train_dataloader, train_loss)], epochs=1, show_progress_bar=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'db_id': 'department_management',\n",
       "  'sentence1': 'How many heads of the departments are older than 56 ?',\n",
       "  'sentence2': 'List the creation year, name and budget of each department.',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'How many heads of the departments are older than 56 ?',\n",
       "  'sentence2': 'What are the maximum and minimum budget of the departments?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'How many heads of the departments are older than 56 ?',\n",
       "  'sentence2': 'What is the average number of employees of the departments whose rank is between 10 and 15?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'How many heads of the departments are older than 56 ?',\n",
       "  'sentence2': 'In which year were most departments established?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'List the name, born state and age of the heads of departments ordered by age.',\n",
       "  'sentence2': 'List the creation year, name and budget of each department.',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'List the name, born state and age of the heads of departments ordered by age.',\n",
       "  'sentence2': 'What are the maximum and minimum budget of the departments?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'List the name, born state and age of the heads of departments ordered by age.',\n",
       "  'sentence2': 'What is the average number of employees of the departments whose rank is between 10 and 15?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'List the name, born state and age of the heads of departments ordered by age.',\n",
       "  'sentence2': 'In which year were most departments established?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the heads who are born outside the California state?',\n",
       "  'sentence2': 'List the creation year, name and budget of each department.',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the heads who are born outside the California state?',\n",
       "  'sentence2': 'What are the maximum and minimum budget of the departments?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the heads who are born outside the California state?',\n",
       "  'sentence2': 'What is the average number of employees of the departments whose rank is between 10 and 15?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the heads who are born outside the California state?',\n",
       "  'sentence2': 'In which year were most departments established?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the states where at least 3 heads were born?',\n",
       "  'sentence2': 'List the creation year, name and budget of each department.',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the states where at least 3 heads were born?',\n",
       "  'sentence2': 'What are the maximum and minimum budget of the departments?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the states where at least 3 heads were born?',\n",
       "  'sentence2': 'What is the average number of employees of the departments whose rank is between 10 and 15?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the states where at least 3 heads were born?',\n",
       "  'sentence2': 'In which year were most departments established?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'How many heads of the departments are older than 56 ?',\n",
       "  'sentence2': \"What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\",\n",
       "  'label': 0.3333333333333333,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department, management, head'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'List the name, born state and age of the heads of departments ordered by age.',\n",
       "  'sentence2': \"What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\",\n",
       "  'label': 0.3333333333333333,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department, management, head'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the heads who are born outside the California state?',\n",
       "  'sentence2': \"What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\",\n",
       "  'label': 0.3333333333333333,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department, management, head'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the states where at least 3 heads were born?',\n",
       "  'sentence2': \"What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\",\n",
       "  'label': 0.3333333333333333,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department, management, head'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'How many heads of the departments are older than 56 ?',\n",
       "  'sentence2': \"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\",\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'How many heads of the departments are older than 56 ?',\n",
       "  'sentence2': 'How many departments are led by heads who are not mentioned?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'List the name, born state and age of the heads of departments ordered by age.',\n",
       "  'sentence2': \"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\",\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'List the name, born state and age of the heads of departments ordered by age.',\n",
       "  'sentence2': 'How many departments are led by heads who are not mentioned?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the heads who are born outside the California state?',\n",
       "  'sentence2': \"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\",\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the heads who are born outside the California state?',\n",
       "  'sentence2': 'How many departments are led by heads who are not mentioned?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the states where at least 3 heads were born?',\n",
       "  'sentence2': \"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\",\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the states where at least 3 heads were born?',\n",
       "  'sentence2': 'How many departments are led by heads who are not mentioned?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'How many heads of the departments are older than 56 ?',\n",
       "  'sentence2': 'How many acting statuses are there?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'List the name, born state and age of the heads of departments ordered by age.',\n",
       "  'sentence2': 'How many acting statuses are there?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the heads who are born outside the California state?',\n",
       "  'sentence2': 'How many acting statuses are there?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the names of the states where at least 3 heads were born?',\n",
       "  'sentence2': 'How many acting statuses are there?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'head',\n",
       "  'tables2': 'management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'List the creation year, name and budget of each department.',\n",
       "  'sentence2': \"What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\",\n",
       "  'label': 0.3333333333333333,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'department, management, head'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the maximum and minimum budget of the departments?',\n",
       "  'sentence2': \"What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\",\n",
       "  'label': 0.3333333333333333,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'department, management, head'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What is the average number of employees of the departments whose rank is between 10 and 15?',\n",
       "  'sentence2': \"What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\",\n",
       "  'label': 0.3333333333333333,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'department, management, head'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'In which year were most departments established?',\n",
       "  'sentence2': \"What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\",\n",
       "  'label': 0.3333333333333333,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'department, management, head'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'List the creation year, name and budget of each department.',\n",
       "  'sentence2': \"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\",\n",
       "  'label': 0.5,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'List the creation year, name and budget of each department.',\n",
       "  'sentence2': 'How many departments are led by heads who are not mentioned?',\n",
       "  'label': 0.5,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the maximum and minimum budget of the departments?',\n",
       "  'sentence2': \"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\",\n",
       "  'label': 0.5,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the maximum and minimum budget of the departments?',\n",
       "  'sentence2': 'How many departments are led by heads who are not mentioned?',\n",
       "  'label': 0.5,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What is the average number of employees of the departments whose rank is between 10 and 15?',\n",
       "  'sentence2': \"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\",\n",
       "  'label': 0.5,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What is the average number of employees of the departments whose rank is between 10 and 15?',\n",
       "  'sentence2': 'How many departments are led by heads who are not mentioned?',\n",
       "  'label': 0.5,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'In which year were most departments established?',\n",
       "  'sentence2': \"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\",\n",
       "  'label': 0.5,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'In which year were most departments established?',\n",
       "  'sentence2': 'How many departments are led by heads who are not mentioned?',\n",
       "  'label': 0.5,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'List the creation year, name and budget of each department.',\n",
       "  'sentence2': 'How many acting statuses are there?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What are the maximum and minimum budget of the departments?',\n",
       "  'sentence2': 'How many acting statuses are there?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'What is the average number of employees of the departments whose rank is between 10 and 15?',\n",
       "  'sentence2': 'How many acting statuses are there?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'In which year were most departments established?',\n",
       "  'sentence2': 'How many acting statuses are there?',\n",
       "  'label': 0.0,\n",
       "  'tables1': 'department',\n",
       "  'tables2': 'management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': \"What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\",\n",
       "  'sentence2': \"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\",\n",
       "  'label': 0.6666666666666666,\n",
       "  'tables1': 'department, management, head',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': \"What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\",\n",
       "  'sentence2': 'How many departments are led by heads who are not mentioned?',\n",
       "  'label': 0.6666666666666666,\n",
       "  'tables1': 'department, management, head',\n",
       "  'tables2': 'department, management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': \"What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\",\n",
       "  'sentence2': 'How many acting statuses are there?',\n",
       "  'label': 0.3333333333333333,\n",
       "  'tables1': 'department, management, head',\n",
       "  'tables2': 'management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': \"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\",\n",
       "  'sentence2': 'How many acting statuses are there?',\n",
       "  'label': 0.5,\n",
       "  'tables1': 'department, management',\n",
       "  'tables2': 'management'},\n",
       " {'db_id': 'department_management',\n",
       "  'sentence1': 'How many departments are led by heads who are not mentioned?',\n",
       "  'sentence2': 'How many acting statuses are there?',\n",
       "  'label': 0.5,\n",
       "  'tables1': 'department, management',\n",
       "  'tables2': 'management'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How many acting statuses are there?']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\",\n",
       "  'How many acting statuses are there?'),\n",
       " ('How many departments are led by heads who are not mentioned?',\n",
       "  'How many acting statuses are there?')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(product(i_data, j_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How many heads of the departments are older than 56 ?',\n",
       " 'List the name, born state and age of the heads of departments ordered by age.',\n",
       " 'What are the names of the heads who are born outside the California state?',\n",
       " 'What are the names of the states where at least 3 heads were born?']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# algorithm\n",
    "# if table in in the joined tables, they are related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('head', 'department', 0.0), ('head', 'department, management, head', 0.3333333333333333), ('head', 'department, management', 0.0), ('head', 'management', 0.0), ('department', 'department, management, head', 0.3333333333333333), ('department', 'department, management', 0.5), ('department', 'management', 0.0), ('department, management, head', 'department, management', 0.6666666666666666), ('department, management, head', 'management', 0.3333333333333333), ('department, management', 'management', 0.5)]\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(i_tables: str|set, j_tables: str|set):\n",
    "    def preprocess(tables: str):\n",
    "        return set([t.strip() for t in tables.split(',')])\n",
    "    # Get the number of common tables\n",
    "    i_set = preprocess(i_tables) if isinstance(i_tables, str) else i_tables\n",
    "    j_set = preprocess(j_tables) if isinstance(j_tables, str) else j_tables\n",
    "\n",
    "    common_tables = i_set.intersection(j_set)\n",
    "    union_tables = i_set.union(j_set)\n",
    "    return len(common_tables) / len(union_tables)\n",
    "\n",
    "# Calculate similarity between all pairs of tables\n",
    "table_pairs = []\n",
    "for i_tables, j_tables in combinations(data_dict.keys(), 2):\n",
    "    similarity = jaccard_similarity(i_tables, j_tables)\n",
    "    table_pairs.append((i_tables, j_tables, similarity))\n",
    "\n",
    "print(table_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_acc_evaluator = BinaryClassificationEvaluator.from_input_examples(dev_data, name='dev')\n",
    "results = binary_acc_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dev_cosine_accuracy': 0.9995553579368608,\n",
       " 'dev_cosine_accuracy_threshold': 0.5259714126586914,\n",
       " 'dev_cosine_f1': 0.9997776295307983,\n",
       " 'dev_cosine_f1_threshold': 0.5259714126586914,\n",
       " 'dev_cosine_precision': 1.0,\n",
       " 'dev_cosine_recall': 0.9995553579368608,\n",
       " 'dev_cosine_ap': 1.0,\n",
       " 'dev_dot_accuracy': 0.9995553579368608,\n",
       " 'dev_dot_accuracy_threshold': 0.5259714722633362,\n",
       " 'dev_dot_f1': 0.9997776295307983,\n",
       " 'dev_dot_f1_threshold': 0.5259714722633362,\n",
       " 'dev_dot_precision': 1.0,\n",
       " 'dev_dot_recall': 0.9995553579368608,\n",
       " 'dev_dot_ap': 1.0,\n",
       " 'dev_manhattan_accuracy': 0.9995553579368608,\n",
       " 'dev_manhattan_accuracy_threshold': 15.076079368591309,\n",
       " 'dev_manhattan_f1': 0.9997776295307983,\n",
       " 'dev_manhattan_f1_threshold': 15.076079368591309,\n",
       " 'dev_manhattan_precision': 1.0,\n",
       " 'dev_manhattan_recall': 0.9995553579368608,\n",
       " 'dev_manhattan_ap': 1.0,\n",
       " 'dev_euclidean_accuracy': 0.9995553579368608,\n",
       " 'dev_euclidean_accuracy_threshold': 0.9736785888671875,\n",
       " 'dev_euclidean_f1': 0.9997776295307983,\n",
       " 'dev_euclidean_f1_threshold': 0.9736785888671875,\n",
       " 'dev_euclidean_precision': 1.0,\n",
       " 'dev_euclidean_recall': 0.9995553579368608,\n",
       " 'dev_euclidean_ap': 1.0,\n",
       " 'dev_max_accuracy': 0.9995553579368608,\n",
       " 'dev_max_accuracy_threshold': 15.076079368591309,\n",
       " 'dev_max_f1': 0.9997776295307983,\n",
       " 'dev_max_f1_threshold': 15.076079368591309,\n",
       " 'dev_max_precision': 1.0,\n",
       " 'dev_max_recall': 0.9995553579368608,\n",
       " 'dev_max_ap': 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "  T1.apt_number\n",
      "FROM Apartments AS T1\n",
      "JOIN View_Unit_Status AS T2\n",
      "  ON T1.apt_id = T2.apt_id\n",
      "WHERE\n",
      "  T2.available_yn = 0\n",
      "INTERSECT\n",
      "SELECT\n",
      "  T1.apt_number\n",
      "FROM Apartments AS T1\n",
      "JOIN View_Unit_Status AS T2\n",
      "  ON T1.apt_id = T2.apt_id\n",
      "WHERE\n",
      "  T2.available_yn = 1\n",
      "['apartments', 'view_unit_status', 'apartments', 'view_unit_status']\n",
      "FROM Apartments AS T1 JOIN View_Unit_Status AS T2 ON T1.apt_id = T2.apt_id WHERE T2.available_yn = 0 FROM Apartments AS T1 JOIN View_Unit_Status AS T2 ON T1.apt_id = T2.apt_id WHERE T2.available_yn = 1\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "import sqlglot.expressions as exp\n",
    "from sqlglot.optimizer import optimize\n",
    "from sqlglot.optimizer.annotate_types import annotate_types\n",
    "from sqlglot.optimizer.canonicalize import canonicalize\n",
    "from sqlglot.optimizer.eliminate_ctes import eliminate_ctes\n",
    "from sqlglot.optimizer.eliminate_joins import eliminate_joins\n",
    "from sqlglot.optimizer.eliminate_subqueries import eliminate_subqueries\n",
    "from sqlglot.optimizer.merge_subqueries import merge_subqueries\n",
    "from sqlglot.optimizer.normalize import normalize\n",
    "from sqlglot.optimizer.optimize_joins import optimize_joins\n",
    "from sqlglot.optimizer.pushdown_predicates import pushdown_predicates\n",
    "from sqlglot.optimizer.pushdown_projections import pushdown_projections\n",
    "from sqlglot.optimizer.qualify import qualify\n",
    "from sqlglot.optimizer.qualify_columns import quote_identifiers\n",
    "from sqlglot.optimizer.simplify import simplify\n",
    "from sqlglot.optimizer.unnest_subqueries import unnest_subqueries\n",
    "from sqlglot.schema import ensure_schema\n",
    "\n",
    "RULES = (\n",
    "    # qualify,\n",
    "    # pushdown_projections,\n",
    "    # normalize,\n",
    "    # unnest_subqueries,\n",
    "    # pushdown_predicates,\n",
    "    # optimize_joins,\n",
    "    # eliminate_subqueries,\n",
    "    # merge_subqueries,\n",
    "    # eliminate_joins,\n",
    "    # eliminate_ctes,\n",
    "    # quote_identifiers,\n",
    "    # annotate_types,\n",
    "    # canonicalize,\n",
    "    # simplify,\n",
    ")\n",
    "{\"sample_id\": 1272, \"db_id\": \"apartment_rentals\", \"final\": \n",
    " {\"question\": \"Show the apartment numbers of apartments with unit status availability of both 0 and 1.\", \n",
    "\"sql\": \"SELECT T1.apt_number FROM Apartments AS T1 JOIN View_Unit_Status AS T2 ON T1.apt_id  =  T2.apt_id WHERE T2.available_yn  =  0 INTERSECT SELECT T1.apt_number FROM Apartments AS T1 JOIN View_Unit_Status AS T2 ON T1.apt_id  =  T2.apt_id WHERE T2.available_yn  =  1\", \"source_tables\": [\"t1\", \"view_unit_status\", \"t1\", \"view_unit_status\", \"apartments\"]}}\n",
    "\n",
    "db_id = 'apartment_rentals'\n",
    "schema = spider_tables[db_id].db_schema\n",
    "sql = \"SELECT T1.apt_number FROM Apartments AS T1 JOIN View_Unit_Status AS T2 ON T1.apt_id  =  T2.apt_id WHERE T2.available_yn  =  0 INTERSECT SELECT T1.apt_number FROM Apartments AS T1 JOIN View_Unit_Status AS T2 ON T1.apt_id  =  T2.apt_id WHERE T2.available_yn  =  1\"\n",
    "sql = sqlglot.parse_one(sql, read='sqlite')\n",
    "print(sql.sql(pretty=True))\n",
    "tbls = [x.this.this.lower() for x in list(sql.find_all(exp.Table))]\n",
    "\n",
    "expression = ' '.join([x.sql() for x in sql.find_all(*[exp.From, exp.Join, exp.Where])])\n",
    "print(tbls)\n",
    "print(expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table and Columns]\n",
      "Physician: EmployeeID, Name, Position, SSN\n",
      "Department: DepartmentID, Name, Head\n",
      "Affiliated_With: Physician, Department, PrimaryAffiliation\n",
      "Procedures: Code, Name, Cost\n",
      "Trained_In: Physician, Treatment, CertificationDate, CertificationExpires\n",
      "Patient: SSN, Name, Address, Phone, InsuranceID, PCP\n",
      "Nurse: EmployeeID, Name, Position, Registered, SSN\n",
      "Appointment: AppointmentID, Patient, PrepNurse, Physician, Start, End, ExaminationRoom\n",
      "Medication: Code, Name, Brand, Description\n",
      "Prescribes: Physician, Patient, Medication, Date, Appointment, Dose\n",
      "Block: BlockFloor, BlockCode\n",
      "Room: RoomNumber, RoomType, BlockFloor, BlockCode, Unavailable\n",
      "On_Call: Nurse, BlockFloor, BlockCode, OnCallStart, OnCallEnd\n",
      "Stay: StayID, Patient, Room, StayStart, StayEnd\n",
      "Undergoes: Patient, Procedures, Stay, DateUndergoes, Physician, AssistingNurse\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-large')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-large')\n",
    "\n",
    "features = tokenizer(['A man is eating pizza', 'A black race car starts up in front of a crowd of people.'], ['A man eats something', 'A man is driving down a lonely road.'],  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    label_mapping = ['contradiction', 'entailment', 'neutral']\n",
    "    labels = [label_mapping[score_max] for score_max in scores.argmax(dim=1)]\n",
    "    print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatabaseModel(db_id='department_management', db_schema={'department': {'Department_ID': 'text', 'Name': 'text', 'Creation': 'text', 'Ranking': 'text', 'Budget_in_Billions': 'text', 'Num_Employees': 'text'}, 'head': {'head_ID': 'number', 'name': 'number', 'born_state': 'number', 'age': 'number'}, 'management': {'department_ID': 'text', 'head_ID': 'text', 'temporary_acting': 'text'}}, col_explanation={'department': {'Department_ID': 'Unique identifier for each department.', 'Name': 'Name of the department.', 'Creation': 'Date when the department was established.', 'Ranking': 'Ranking of the department based on performance.', 'Budget_in_Billions': 'Annual budget allocated to the department in billions.', 'Num_Employees': 'Total number of employees in the department.'}, 'head': {'head_ID': 'Unique identifier for each department head.', 'name': 'Name of the department head.', 'born_state': 'State where the department head was born.', 'age': 'Age of the department head.'}, 'management': {'department_ID': 'Identifier for the department managed.', 'head_ID': 'Identifier for the head of the department.', 'temporary_acting': 'Indicates if the head is temporarily acting in the position.'}}, foreign_keys=['management.head_ID = head.head_ID', 'management.department_ID = department.Department_ID'], primary_keys=['department.Department_ID', 'head.head_ID', 'management.department_ID'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spider_tables[train_samples[0].db_id].db_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['head']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlglot\n",
    "import sqlglot.expressions as exp\n",
    "from sqlglot.optimizer import optimize\n",
    "\n",
    "def extract_used_table(sql: str, schema: dict) -> list[str]:\n",
    "    sql = optimize(sqlglot.parse_one(sql, read='sqlite'), schema=schema)\n",
    "    tbls = [x.this.this for x in list(sql.find_all(exp.Table))]\n",
    "    return tbls\n",
    "\n",
    "extract_used_table(train_samples[0].final.sql, spider_tables[train_samples[0].db_id].db_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparc_path = proj_path / 'data' / 'sparc'\n",
    "\n",
    "tables, train_data, dev_data = load_spider_sparc_data(sparc_path)\n",
    "\n",
    "with (proj_path / 'db_data' / 'description.json').open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "\n",
    "sparc_tables = process_all_tables(tables, descriptions=all_descriptions)\n",
    "# filter samples by count, must have at least 5 samples\n",
    "all_data = filter_samples_by_count_sparc(train_data+dev_data, n=5)\n",
    "# process samples -> {db_id: list of samples}\n",
    "sparc_samples = process_samples_sparc(all_data, sparc_tables)\n",
    "# change train/dev by sample\n",
    "train_samples, dev_samples = split_train_dev(sparc_samples, ratio=0.8)\n",
    "\n",
    "print(f'Number of train: {len(train_samples)} | Number of dev: {len(dev_samples)}')\n",
    "\n",
    "db_id = 'hospital_1'\n",
    "db_file = str(sparc_path / 'database' / db_id / f'{db_id}.sqlite')\n",
    "database = SqliteDatabase(db_file, foreign_keys=sparc_tables[db_id].foreign_keys)\n",
    "print(database.table_cols.keys())\n",
    "database.execute('SELECT * FROM Department LIMIT 5;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0-Question] How many employees does each department have?\n",
      "[0-SQL]: SELECT count(departmentID) FROM department GROUP BY departmentID\n",
      "[1-Question] Which department has the smallest number of employees?\n",
      "[1-SQL]: SELECT * FROM department GROUP BY departmentID ORDER BY count(departmentID) LIMIT 1;\n",
      "[2-Question] Tell me the name and position of the head of this department.\n",
      "[2-SQL]: SELECT T2.name ,  T2.position FROM department AS T1 JOIN physician AS T2 ON T1.head  =  T2.EmployeeID GROUP BY departmentID ORDER BY count(departmentID) LIMIT 1; \n",
      "\n",
      "[Final]\n",
      "Question: Find the name and position of the head of the department with the least employees.\n",
      "SQL: SELECT T2.name ,  T2.position FROM department AS T1 JOIN physician AS T2 ON T1.head  =  T2.EmployeeID GROUP BY departmentID ORDER BY count(departmentID) LIMIT 1;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.spider_sparc_preprocess import SparcSample, QuestionSQL\n",
    "\n",
    "def format_interactions(interactions: list[QuestionSQL]) -> str:\n",
    "    workload = ''\n",
    "    for i, interaction in enumerate(interactions):\n",
    "        workload += f'[{i}-Question] {interaction.question}\\n[{i}-SQL]: {interaction.sql}\\n'\n",
    "    return workload.strip()\n",
    "\n",
    "with (proj_path / 'db_data' / 'sparc_description.json').open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "\n",
    "db_id = 'hospital_1'\n",
    "train_subsamples = list(filter(lambda x: x.db_id == db_id, train_samples))\n",
    "dev_subsamples = list(filter(lambda x: x.db_id == db_id, dev_samples))\n",
    "\n",
    "table = sparc_tables[db_id]\n",
    "col_explanation = all_descriptions[db_id]\n",
    "# create schema string\n",
    "schema_str = get_schema_str(\n",
    "    schema=table.db_schema, \n",
    "    foreign_keys=table.foreign_keys,\n",
    "    primary_keys=table.primary_keys,\n",
    "    col_explanation=col_explanation\n",
    ")  \n",
    "database = SqliteDatabase(str(sparc_path / 'database' / db_id / f'{db_id}.sqlite'), foreign_keys=table.foreign_keys)\n",
    "\n",
    "data = train_samples[2]\n",
    "workload = format_interactions(data.interactions)\n",
    "print(workload, '\\n')\n",
    "print(f'[Final]\\nQuestion: {data.final.question}\\nSQL: {data.final.sql}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(departmentID)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(departmentID)\n",
       "0                    1\n",
       "1                    1\n",
       "2                    1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.execute(data.interactions[0].sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepartmentID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>General Medicine</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DepartmentID              Name  Head\n",
       "0             1  General Medicine     4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.execute(data.interactions[1].sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Percival Cox</td>\n",
       "      <td>Senior Attending Physician</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name                    Position\n",
       "0  Percival Cox  Senior Attending Physician"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.execute(data.interactions[2].sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Percival Cox</td>\n",
       "      <td>Senior Attending Physician</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name                    Position\n",
       "0  Percival Cox  Senior Attending Physician"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.execute(data.final.sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Common table extraction\n",
    "\n",
    "* find the common table used in the question-sql workloads: All joined tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train workloads: 25\n",
      "# of used tables: 22\n",
      "-----------------\n",
      "appointment: 9\n",
      "department: 7\n",
      "physician: 6\n",
      "stay: 6\n",
      "physician,patient: 6\n",
      "block,room: 4\n",
      "room: 4\n",
      "medication: 3\n",
      "appointment,patient: 2\n",
      "appointment,physician: 2\n",
      "physician,affiliated_with,department: 2\n",
      "nurse,appointment: 2\n",
      "prescribes,medication: 2\n",
      "physician,prescribes,medication: 2\n",
      "department,physician: 1\n",
      "physician,appointment,physician: 1\n",
      "patient,appointment: 1\n",
      "prescribes,physician: 1\n",
      "patient,prescribes,physician: 1\n",
      "stay,patient,prescribes: 1\n",
      "stay,patient,prescribes,medication: 1\n",
      "medication,prescribes: 1\n"
     ]
    }
   ],
   "source": [
    "import sqlglot\n",
    "import sqlglot.expressions as exp\n",
    "from sqlglot.diff import Keep\n",
    "from sqlglot.optimizer import optimize\n",
    "from collections import Counter\n",
    "\n",
    "def extract_table_expression(x: QuestionSQL, schema: dict) -> str:\n",
    "    sql = optimize(sqlglot.parse_one(x.sql, read='sqlite'), schema=schema)\n",
    "    tbls = [x.this.this for x in list(sql.find_all(exp.Table))]\n",
    "    expression = ' '.join([x.sql() for x in sql.find_all(*[exp.From, exp.Join])])\n",
    "    return ','.join(tbls), expression\n",
    "\n",
    "def get_sources(data: SparcSample, schema: dict) -> list[tuple[str, list[str]]]:\n",
    "    sources = []\n",
    "    for x in data.interactions:\n",
    "        tbls, expression = extract_table_expression(x, schema)\n",
    "        sources.append({'question': x.question, 'table': tbls, 'expression': expression})\n",
    "    return sources\n",
    "\n",
    "db_id = 'hospital_1'\n",
    "train_subsamples = list(filter(lambda x: x.db_id == db_id, train_samples))\n",
    "dev_subsamples = list(filter(lambda x: x.db_id == db_id, dev_samples))\n",
    "table = sparc_tables[db_id]\n",
    "database = SqliteDatabase(str(sparc_path / 'database' / db_id / f'{db_id}.sqlite'), foreign_keys=table.foreign_keys)\n",
    "\n",
    "used_tables = Counter()\n",
    "for data in train_subsamples:\n",
    "    sources = get_sources(data, table.db_schema)\n",
    "    used = [x['table'] for x in sources]\n",
    "    used_tables.update(used)\n",
    "\n",
    "print(f'# of train workloads: {len(train_subsamples)}')\n",
    "print(f'# of used tables: {len(used_tables)}\\n-----------------')\n",
    "for k, v in used_tables.most_common():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a nested query\n",
    "sql = \"\"\"\n",
    "SELECT * FROM (\n",
    "    SELECT * FROM Department\n",
    ") AS A\n",
    "WHERE A.department_id = 1;\n",
    "\"\"\"\n",
    "\n",
    "sql = sqlglot.parse_one(sql, read='sqlite')\n",
    "tbls = [x.this.this for x in list(sql.find_all(exp.Table))]\n",
    "expression = ' '.join([x.sql() for x in sql.find_all(*[exp.From, exp.Join])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Department']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00 - Question] Find the department with the most employees.\n",
      "[00 - SQL] SELECT name FROM department GROUP BY departmentID ORDER BY count(departmentID) DESC LIMIT 1;\n",
      "[01 - Question] Tell me the employee id of the head of the department with the least employees.\n",
      "[01 - SQL] SELECT head FROM department GROUP BY departmentID ORDER BY count(departmentID) LIMIT 1;\n",
      "[02 - Question] Find the name and position of the head of the department with the least employees.\n",
      "[02 - SQL] SELECT T2.name ,  T2.position FROM department AS T1 JOIN physician AS T2 ON T1.head  =  T2.EmployeeID GROUP BY departmentID ORDER BY count(departmentID) LIMIT 1;\n",
      "[03 - Question] List the names of patients who have made appointments.\n",
      "[03 - SQL] SELECT name FROM appointment AS T1 JOIN patient AS T2 ON T1.patient  =  T2.ssn\n",
      "[04 - Question] Which patients made more than one appointment? Tell me the name and phone number of these patients.\n",
      "[04 - SQL] SELECT name ,  phone FROM appointment AS T1 JOIN patient AS T2 ON T1.patient  =  T2.ssn GROUP BY T1.patient HAVING count(*)  >  1\n",
      "[05 - Question] What is the id of the appointment that started most recently?\n",
      "[05 - SQL] SELECT appointmentid FROM appointment ORDER BY START DESC LIMIT 1\n",
      "[06 - Question] What are the names of all the physicians who took appointments.\n",
      "[06 - SQL] SELECT T2.name FROM appointment AS T1 JOIN physician AS T2 ON T1.Physician  =  T2.EmployeeID\n",
      "[07 - Question] Which physicians have never taken any appointment? Find their names.\n",
      "[07 - SQL] SELECT name FROM physician EXCEPT SELECT T2.name FROM appointment AS T1 JOIN physician AS T2 ON T1.Physician  =  T2.EmployeeID\n",
      "[08 - Question] What are the name and primarily affiliated department name of each physician?\n",
      "[08 - SQL] SELECT T1.name ,  T3.name FROM physician AS T1 JOIN affiliated_with AS T2 ON T1.EmployeeID  =  T2.physician JOIN department AS T3 ON T2.department  =  T3.DepartmentID WHERE T2.PrimaryAffiliation  =  1\n",
      "[09 - Question] Find the name of the patient who made the appointment with the most recent start date.\n",
      "[09 - SQL] SELECT T1.name FROM patient AS T1 JOIN appointment AS T2 WHERE T1.ssn = T2.patient ORDER BY T2.start DESC LIMIT 1\n",
      "[10 - Question] Count the number of patients who stayed in room 112.\n",
      "[10 - SQL] SELECT count(patient) FROM stay WHERE room  =  112\n",
      "[11 - Question] Find the number of patients' prescriptions physician John Dorian made.\n",
      "[11 - SQL] SELECT count(T1.SSN) FROM patient AS T1 JOIN prescribes AS T2 ON T1.SSN  =  T2.patient JOIN physician AS T3 ON T2.physician  =  T3.employeeid WHERE T3.name = 'John Dorian'\n",
      "[12 - Question] What is the name of the medication used for the patient staying in room 111?\n",
      "[12 - SQL] SELECT T4.name FROM stay AS T1 JOIN patient AS T2 ON T1.Patient  =  T2.SSN JOIN Prescribes AS T3 ON T3.Patient  =  T2.SSN JOIN Medication AS T4 ON T3.Medication  =  T4.Code WHERE room  =  111\n",
      "[13 - Question] What is the id of the patient who stayed in room 111 most recently?\n",
      "[13 - SQL] SELECT patient FROM stay WHERE room  =  111 ORDER BY staystart DESC LIMIT 1\n",
      "[14 - Question] Find the name of the nurse who has the largest number of appointments.\n",
      "[14 - SQL] SELECT T1.name FROM nurse AS T1 JOIN appointment AS T2 ON T1.employeeid  =  T2.prepnurse GROUP BY T1.employeeid ORDER BY count(*) DESC LIMIT 1\n",
      "[15 - Question] Return the name of each physician and the number of patients he or she treats.\n",
      "[15 - SQL] SELECT T1.name ,  count(*) FROM physician AS T1 JOIN patient AS T2 ON T1.employeeid  =  T2.PCP GROUP BY T1.employeeid\n",
      "[16 - Question] Which physicians are in charge of more than one patient? Give me their names.\n",
      "[16 - SQL] SELECT T1.name FROM physician AS T1 JOIN patient AS T2 ON T1.employeeid  =  T2.PCP GROUP BY T1.employeeid HAVING count(*)  >  1\n",
      "[17 - Question] How many rooms does each block floor have?\n",
      "[17 - SQL] SELECT count(*) ,  T1.blockfloor FROM BLOCK AS T1 JOIN room AS T2 ON T1.blockfloor  =  T2.blockfloor AND T1.blockcode  =  T2.blockcode GROUP BY T1.blockfloor\n",
      "[18 - Question] How many rooms are located for each block code?\n",
      "[18 - SQL] SELECT count(*) ,  T1.blockcode FROM BLOCK AS T1 JOIN room AS T2 ON T1.blockfloor  =  T2.blockfloor AND T1.blockcode  =  T2.blockcode GROUP BY T1.blockcode\n",
      "[19 - Question] Tell me the distinct block codes where some rooms are available.\n",
      "[19 - SQL] SELECT DISTINCT blockcode FROM room WHERE unavailable  =  0\n",
      "[20 - Question] Find the number of distinct room types available.\n",
      "[20 - SQL] SELECT count(DISTINCT roomtype) FROM room\n",
      "[21 - Question] List the names of all the physicians who prescribe Thesisin as medication.\n",
      "[21 - SQL] SELECT DISTINCT T1.name FROM physician AS T1 JOIN prescribes AS T2 ON T1.employeeid = T2.physician JOIN medication AS T3 ON T3.code = T2.medication WHERE T3.name  =  'Thesisin'\n",
      "[22 - Question] Which physicians prescribe a medication of brand X? Tell me the name and position of those physicians.\n",
      "[22 - SQL] SELECT DISTINCT T1.name ,  T1.position FROM physician AS T1 JOIN prescribes AS T2 ON T1.employeeid = T2.physician JOIN medication AS T3 ON T3.code = T2.medication WHERE T3.Brand  =  'X'\n",
      "[23 - Question] How many medications are prescribed for each brand?\n",
      "[23 - SQL] SELECT count(*) ,  T1.name FROM medication AS T1 JOIN prescribes AS T2 ON T1.code = T2.medication GROUP BY T1.brand\n",
      "[24 - Question] What are the names of the physicians who have 'senior' in their titles.\n",
      "[24 - SQL] SELECT name FROM physician WHERE POSITION LIKE '%senior%'\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_subsamples):\n",
    "    print(f'[{i:02d} - Question] {data.final.question}')\n",
    "    print(f'[{i:02d} - SQL] {data.final.sql}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "  T2.name,\n",
      "  T2.position\n",
      "FROM department AS T1\n",
      "JOIN physician AS T2\n",
      "  ON T1.head = T2.EmployeeID\n",
      "GROUP BY\n",
      "  departmentID\n",
      "ORDER BY\n",
      "  COUNT(departmentID)\n",
      "LIMIT 1\n"
     ]
    }
   ],
   "source": [
    "data = train_samples[2]\n",
    "print(sqlglot.transpile(data.interactions[2].sql, write=\"sqlite\", identify=False, pretty=True)[0])\n",
    "sql = optimize(sqlglot.parse_one(data.interactions[2].sql, read='sqlite'), schema=table.db_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract Term - Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:21<00:00,  3.27s/it]\n"
     ]
    }
   ],
   "source": [
    "class TermExpressions(BaseModel):\n",
    "    rationale: str = Field(description='The reasoning behind the decision.')\n",
    "    index: int = Field(description='Index of the question-sql pair.')\n",
    "    term: str = Field(description='A declarative form of the natural language term.')\n",
    "    expression: str = Field(description='SQL expression that refers to the term.')\n",
    "\n",
    "class Response(BaseModel):\n",
    "    output: list[TermExpressions]\n",
    "    \n",
    "template = '''### Task\n",
    "You are tasked with identifying the partial term - partial expression relationship to represent the common interest query.\n",
    "You will be proveded several pairs of question and SQL with index. Do not extract the FROM and JOIN clauses.\n",
    "There could be multiple terms and expressions in a single question-SQL pair.\n",
    "\n",
    "### Formatting\n",
    "Your output should be of the following list of JSON format:\n",
    "[\n",
    "    {{\n",
    "        \"rationale\": <str: the reasoning behind decision>,\n",
    "        \"index: <int: the index of the question-sql pair>,\n",
    "        \"term\": <str: a partial natural language term>,\n",
    "        \"expression\" : <str: a partial SQL expression that refer to the term>,\n",
    "    }}, ...\n",
    "]\n",
    "\n",
    "\n",
    "### Output\n",
    "<QUESTION-SQL>:\\n{workload}\n",
    "<OUTPUT>: \n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['workload']\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "model = model_openai.with_structured_output(Response)\n",
    "chain = (prompt | model)\n",
    "\n",
    "all_term_expression = defaultdict(list)\n",
    "for data in tqdm(train_subsamples, total=len(train_subsamples)):\n",
    "    workload = format_interactions(data.interactions)\n",
    "    term_expression = chain.invoke(input={'workload': workload}).output\n",
    "    all_term_expression[data.sample_id] = term_expression\n",
    "    # for x in data.interactions:\n",
    "    #     input_data = {'question': x.question, 'sql': x.sql}\n",
    "    #     term_expression = chain.invoke(input=input_data).output\n",
    "    #     tbls, _ = extract_table_expression(x, table.db_schema)\n",
    "    #     all_term_expression[tbls].append(term_expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sample 00-0]: employees - count(departmentID)\n",
      "[Sample 00-0]: department - GROUP BY departmentID\n",
      "[Sample 00-1]: most employees - ORDER BY count(departmentID) DESC LIMIT 1\n",
      "[Sample 00-1]: department name - name\n",
      "[Sample 00-1]: department - GROUP BY departmentID\n",
      "[Sample 01-0]: employees - count(departmentID)\n",
      "[Sample 01-0]: department - GROUP BY departmentID\n",
      "[Sample 01-1]: least employees - ORDER BY count(departmentID) LIMIT 1\n",
      "[Sample 01-1]: department - GROUP BY departmentID\n",
      "[Sample 01-2]: head - head\n",
      "[Sample 01-2]: department - GROUP BY departmentID\n",
      "[Sample 02-0]: employees - count(departmentID)\n",
      "[Sample 02-0]: department - GROUP BY departmentID\n",
      "[Sample 02-1]: smallest number of employees - ORDER BY count(departmentID) LIMIT 1\n",
      "[Sample 02-1]: department - GROUP BY departmentID\n",
      "[Sample 02-2]: name - T2.name\n",
      "[Sample 02-2]: position - T2.position\n",
      "[Sample 02-2]: department - GROUP BY departmentID\n",
      "[Sample 03-0]: patient id - patient\n",
      "[Sample 03-1]: names of patients - name\n",
      "[Sample 03-1]: appointments - appointment AS T1\n",
      "[Sample 04-0]: appointments - count(*)\n",
      "[Sample 04-0]: patient - GROUP BY patient\n",
      "[Sample 04-1]: more than one appointment - HAVING count(*) > 1\n",
      "[Sample 04-1]: patient - GROUP BY patient\n",
      "[Sample 04-2]: name - name\n",
      "[Sample 04-2]: phone number - phone\n",
      "[Sample 04-2]: patients - GROUP BY T1.patient\n",
      "[Sample 05-0]: start date - START\n",
      "[Sample 05-1]: appointments - ORDER BY START DESC\n",
      "[Sample 05-2]: most recent starting date - ORDER BY START DESC LIMIT 1\n",
      "[Sample 05-2]: appointment id - appointmentid\n",
      "[Sample 06-0]: ids of the physicians - Physician\n",
      "[Sample 06-1]: names - T2.name\n",
      "[Sample 06-1]: physicians - T2.EmployeeID\n",
      "[Sample 07-0]: physician names - SELECT name\n",
      "[Sample 07-1]: physicians who took appointments - SELECT T2.name FROM appointment AS T1 JOIN physician AS T2 ON T1.Physician = T2.EmployeeID\n",
      "[Sample 07-2]: physicians who never took any appointment - SELECT name FROM physician EXCEPT SELECT T2.name FROM appointment AS T1 JOIN physician AS T2 ON T1.Physician = T2.EmployeeID\n",
      "[Sample 08-0]: physician - SELECT name\n",
      "[Sample 08-1]: name - T3.name\n",
      "[Sample 08-1]: department - T2.department = T3.DepartmentID\n",
      "[Sample 08-2]: physician - T1.name\n",
      "[Sample 08-2]: department - T2.department = T3.DepartmentID\n",
      "[Sample 09-0]: appointments - SELECT * FROM appointment\n",
      "[Sample 09-0]: start date - ORDER BY START DESC\n",
      "[Sample 09-1]: most recent start date - ORDER BY START DESC LIMIT 1\n",
      "[Sample 09-1]: appointment - SELECT * FROM appointment\n",
      "[Sample 09-2]: name of the patient - T1.name\n",
      "[Sample 09-2]: appointment - JOIN appointment AS T2\n",
      "[Sample 09-2]: start - ORDER BY T2.start DESC\n",
      "[Sample 10-0]: stays - * FROM stay\n",
      "[Sample 10-0]: room 112 - room = 112\n",
      "[Sample 10-1]: patients - count(patient)\n",
      "[Sample 10-1]: room 112 - room = 112\n",
      "[Sample 11-0]: employee id - employeeid\n",
      "[Sample 11-0]: physician - physician\n",
      "[Sample 11-1]: prescriptions - prescribes\n",
      "[Sample 11-1]: physician - T2.employeeid\n",
      "[Sample 11-2]: patients - count(T1.SSN)\n",
      "[Sample 11-2]: prescriptions - T2.patient\n",
      "[Sample 11-2]: physician - T3.employeeid\n",
      "[Sample 12-0]: patient - patient\n",
      "[Sample 12-0]: room 111 - WHERE room = 111\n",
      "[Sample 12-1]: medication - T3.Medication\n",
      "[Sample 12-1]: patient - T2.SSN\n",
      "[Sample 12-2]: name of the medication - T4.name\n",
      "[Sample 12-2]: room 111 - WHERE room = 111\n",
      "[Sample 13-0]: patients - patient\n",
      "[Sample 13-0]: room 111 - room = 111\n",
      "[Sample 13-1]: stay start date - ORDER BY staystart DESC\n",
      "[Sample 13-1]: patients - patient\n",
      "[Sample 13-2]: most recently - ORDER BY staystart DESC LIMIT 1\n",
      "[Sample 13-2]: patients - patient\n",
      "[Sample 13-2]: room 111 - room = 111\n",
      "[Sample 14-0]: appointments - count(*)\n",
      "[Sample 14-0]: nurse - GROUP BY T1.employeeid\n",
      "[Sample 14-1]: most appointments - ORDER BY count(*) DESC LIMIT 1\n",
      "[Sample 14-1]: nurse - GROUP BY T1.employeeid\n",
      "[Sample 14-1]: name - T1.name\n",
      "[Sample 15-0]: patients - GROUP BY T1.employeeid\n",
      "[Sample 15-0]: physician - GROUP BY T1.employeeid\n",
      "[Sample 15-1]: patients - count(*)\n",
      "[Sample 15-1]: physician - GROUP BY T1.employeeid\n",
      "[Sample 15-2]: name - T1.name\n",
      "[Sample 15-2]: patients - count(*)\n",
      "[Sample 15-2]: physician - GROUP BY T1.employeeid\n",
      "[Sample 16-0]: patients - count(*)\n",
      "[Sample 16-0]: physician - GROUP BY T1.employeeid\n",
      "[Sample 16-1]: more than one - HAVING count(*) > 1\n",
      "[Sample 16-1]: physicians - GROUP BY T1.employeeid\n",
      "[Sample 16-2]: name - T1.name\n",
      "[Sample 16-2]: physicians - GROUP BY T1.employeeid\n",
      "[Sample 17-0]: block floor - T1.blockfloor\n",
      "[Sample 17-0]: room - T2.blockfloor\n",
      "[Sample 17-1]: rooms - count(*)\n",
      "[Sample 17-1]: block floor - T1.blockfloor\n",
      "[Sample 18-0]: room - T1.blockcode\n",
      "[Sample 18-0]: block code - T1.blockcode\n",
      "[Sample 18-1]: rooms - count(*)\n",
      "[Sample 18-1]: block code - T1.blockcode\n",
      "[Sample 19-0]: rooms - SELECT * FROM room\n",
      "[Sample 19-0]: available - WHERE unavailable = 0\n",
      "[Sample 19-1]: block codes - SELECT DISTINCT blockcode\n",
      "[Sample 19-1]: rooms available - WHERE unavailable = 0\n",
      "[Sample 20-0]: room types - roomtype\n",
      "[Sample 20-1]: distinct room types - count(DISTINCT roomtype)\n",
      "[Sample 21-0]: medication - WHERE name = 'Thesisin'\n",
      "[Sample 21-0]: code - SELECT code\n",
      "[Sample 21-1]: physicians - SELECT DISTINCT T1.physician\n",
      "[Sample 21-1]: medication - WHERE T2.name = 'Thesisin'\n",
      "[Sample 21-2]: names - SELECT DISTINCT T1.name\n",
      "[Sample 21-2]: medication - WHERE T3.name = 'Thesisin'\n",
      "[Sample 22-0]: medication - WHERE Brand = 'X'\n",
      "[Sample 22-0]: Brand X - WHERE Brand = 'X'\n",
      "[Sample 22-1]: physicians - SELECT DISTINCT T1.physician\n",
      "[Sample 22-1]: medication - WHERE T2.Brand = 'X'\n",
      "[Sample 22-2]: name - T1.name\n",
      "[Sample 22-2]: position - T1.position\n",
      "[Sample 22-2]: physician - JOIN physician AS T1\n",
      "[Sample 23-0]: medications - GROUP BY brand\n",
      "[Sample 23-0]: brand - GROUP BY brand\n",
      "[Sample 23-1]: medications - count(*)\n",
      "[Sample 23-1]: brand - GROUP BY T1.brand\n",
      "[Sample 24-0]: position title - POSITION\n",
      "[Sample 24-0]: physicians - FROM physician\n",
      "[Sample 24-1]: physicians - FROM physician\n",
      "[Sample 24-1]: senior - POSITION LIKE '%senior%'\n",
      "[Sample 24-2]: names - name\n",
      "[Sample 24-2]: physicians - FROM physician\n"
     ]
    }
   ],
   "source": [
    "for sample_id, term_exps in all_term_expression.items():\n",
    "    for x in term_exps:\n",
    "        print(f'[Sample {sample_id:02d}-{x.index}]: {x.term} - {x.expression}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table and Columns]\n",
      "Table Name: Physician\n",
      "  - 'EmployeeID'(text): Unique identifier for each physician.\n",
      "  - 'Name'(text): Full name of the physician.\n",
      "  - 'Position'(text): Job title or role of the physician.\n",
      "  - 'SSN'(text): Social Security Number of the physician.\n",
      "Table Name: Department\n",
      "  - 'Depart\n"
     ]
    }
   ],
   "source": [
    "with (proj_path / 'db_data' / 'sparc_description.json').open() as f:\n",
    "    all_descriptions = json.load(f)\n",
    "\n",
    "print(get_schema_str(\n",
    "    schema=sparc_tables['hospital_1'].db_schema, \n",
    "    col_explanation=all_descriptions['hospital_1'])[:300]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Access Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13216584, 100000001, 101, 1, '2008-04-24 10:00', '2008-04-24 11:00', 'A')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype_functions = {\n",
    "    'numeric': pd.to_numeric,\n",
    "    'datetime': pd.to_datetime\n",
    "}\n",
    "\n",
    "def null_percentage(s: pd.Series) -> float:\n",
    "    return s.isnull().sum() / len(s)\n",
    "\n",
    "column_info = {}\n",
    "for col in df.columns:\n",
    "    # dtype\n",
    "    null_index = df[col].isnull()\n",
    "    for logical_type in ['numeric', 'datetime', 'text']:\n",
    "        if logical_type in ['numeric', 'datetime']:\n",
    "            try:\n",
    "                df.loc[~null_index, col] = dtype_functions[logical_type](df.loc[~null_index, col], errors='raise')\n",
    "                attribute_type = 'ordinal'\n",
    "                break\n",
    "            except ValueError as e:\n",
    "                # print(f'-- {col}: {logical_type} {e}')\n",
    "                continue\n",
    "            except TypeError as e:\n",
    "                # print(f'-- {col}: {logical_type} {e}')\n",
    "                continue\n",
    "        else:\n",
    "            attribute_type = 'nominal'\n",
    "            break\n",
    "    print(f'{col}: {logical_type} {attribute_type}')\n",
    "    # unique values\n",
    "    unique_values = df[col].unique()\n",
    "    # min, max\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    # null percentage\n",
    "    null_percent = null_percentage(df[col])\n",
    "\n",
    "    column_info[col] = {\n",
    "        'logical_type': logical_type,\n",
    "        'attribute_type': attribute_type,\n",
    "        'unique_values': unique_values,\n",
    "        'min': min_val,\n",
    "        'max': max_val,\n",
    "        'null_percentage': null_percent\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
